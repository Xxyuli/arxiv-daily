# arXiv论文监控报告 - 2025年10月28日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2025年10月28日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 5篇

---

## 1. Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos

### 基本信息
- **作者**: Qixiu Li, Yu Deng, Yaobo Liang, Lin Luo, Lei Zhou, Chengtang Yao, Lingqi Zeng, Zhiyuan Feng, Huizhi Liang, Sicheng Xu, Yizhong Zhang, Xi Chen, Hao Chen, Lily Sun, Dong Chen, Jiaolong Yang, Baining Guo
- **arXiv ID**: [oai:arXiv.org:2510.21571v1](https://arxiv.org/abs/2510.21571)
- **发布日期**: Mon, 27 Oct 2025 00:00:00 -0400
- **分类**: cs.RO, cs.AI, cs.CV, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.21571)

            ### 原文摘要
            arXiv:2510.21571v1 Announce Type: cross  Abstract: This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that "in-the-wild" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.


            
### AI分析（基于论文正文）
以下是针对论文《Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos》的详细总结：

---

### 1. **论文概要**
本论文提出了一种基于真实人类活动视频的大规模视觉-语言-动作（VLA）模型预训练方法，用于机器人灵巧手操作任务。该方法通过全自动的人类活动分析框架，将无标注、非结构化的第一人称视角人类手部活动视频转化为与机器人VLA数据格式对齐的结构化数据，包括原子级动作片段、语言描述、帧级3D手部运动和相机运动。作者构建了一个包含100万片段、2600万帧的手部VLA数据集，并设计了基于因果注意力机制的扩散变换器动作专家模型。实验表明，该预训练模型在零样本场景下表现出强大的动作预测能力，并在少量真实机器人数据微调后显著提升了任务成功率和泛化性能。

---

### 2. **研究动机**
当前机器人VLA模型的预训练数据主要依赖于实验室环境下的人工遥操作采集（如Open X-Embodiment数据集），存在数据规模有限、任务多样性不足、场景覆盖狭窄等问题（第1节，第2-5行）。尽管已有研究尝试利用人类视频进行机器人学习（第2节，第8-10行），但现有方法大多依赖于受控环境下的标注视频或潜在动作表示，未能充分利用大规模无标注真实人类视频进行VLA模型的显式3D动作预训练（第2节，第15-18行）。作者指出，现有方法在任务粒度和标签对齐方面存在明显不足：缺乏有效的原子级动作分割方法，且无法从单目非标定视频中恢复精确的3D手部运动（第1节，第20-25行）。本文首次系统性地解决了如何将非结构化人类视频转化为与机器人VLA数据完全对齐的格式这一关键问题（第1节，第26-30行）。

---

### 3. **核心贡献与创新点**
1. **全自动人类活动分析框架**（第3节，图2）：提出了一种端到端的处理流程，将任意长度的非结构化人类手部视频转化为结构化VLA轨迹。该框架包含三个核心阶段：3D运动标注、原子动作分割和指令标注，全程无需人工干预（第3节，第1-4行）。
2. **基于3D手部运动速度的原子动作分割算法**（第3.2节）：创新性地利用世界坐标系下腕部运动速度的局部极小值作为动作分割点，实现了高效且可扩展的原子级动作片段提取（第3.2节，第5-8行）。该方法无需额外模型推理或预标注文本标签，显著优于现有的时序动作分割方法（第3.2节，第9-12行）。
3. **大规模手部VLA数据集构建**（第3.4节）：通过处理Ego4D、Epic-Kitchen等现有数据集中的原始视频，构建了包含100万片段、2600万帧的手部VLA数据集，覆盖烹饪、清洁、维修等真实场景中的多样化对象、属性和技能（第3.4节，第13-18行）。
4. **统一单双手动作预测的VLA模型架构**（第4.1节）：设计了基于PaliGemma-2 VLM骨干和扩散变换器动作专家的模型结构，引入动作掩码机制和因果注意力去噪，实现了对单双手动作的统一处理（第4.1.3节，第4.1.4节）。

---

### 4. **方法概述**
**整体流程**：如第3节图2所示，方法分为三个阶段：
- **3D运动标注**（第3.1节）：首先通过背景光流判断相机运动状态，使用DroidCalib、MoGe-2等方法估计相机内参并进行去畸变处理（第3.1节，第5-8行）。随后应用HaWoR进行每帧相机空间3D手部重建，结合改进的MegaSAM进行相机姿态跟踪，最终通过样条平滑获得世界坐标系下的3D手部运动序列（第3.1节，第9-14行）。
- **原子动作分割**（第3.2节）：基于世界坐标系下腕部运动速度的局部极小值检测，独立分割左右手的原子级动作片段。该方法在固定窗口内检测局部速度最小值作为分割点，确保每个片段捕获单一原子动作（第3.2节，第5-8行）。
- **指令标注**（第3.3节）：从每个片段中均匀采样8帧，叠加从当前帧到片段结束的手掌轨迹可视化，使用GPT-4.1生成命令式语言描述。若片段无语义意义则标注为"N/A"（第3.3节，第1-4行）。

**模型架构**（第4.1节，图3）：
- **VLM骨干**：采用PaliGemma-2（3B参数），包含SigLIP视觉编码器和Gemma-2语言模型。额外输入相机视场角（FoV）信息和可学习的"认知"令牌，其输出特征$f_c$作为动作专家的条件（第4.1.1节，第5-8行）。
- **动作专家**：采用DiT-Base扩散变换器，输入为认知特征$f_c$、手部状态$s_t$和带噪声的动作块$(a^i_t,...,a^i_{t+N})$。通过AdaLN注入认知特征，使用因果注意力确保每个动作步骤仅关注先前动作（第4.1.1节，第9-14行；第4.1.4节，第1-4行）。
- **动作空间**：定义如公式(3)，包含左右手的相对腕部平移$\Delta t$、旋转$\Delta r$和15个关节的欧拉角$\theta_h$（第4.1.2节）。
- **训练策略**：预训练阶段应用轨迹感知的数据增强，包括随机裁剪、透视变换和图像翻转，同时相应调整动作序列（第4.2节，第1-4行）。微调阶段将机器人动作空间与人类手部动作空间对齐，使用简单的关节拓扑映射策略（第4.3节，第1-4行）。

---

### 5. **实验说明**
**评估指标**：
- 抓取任务：使用预测手指轨迹与目标物体点云之间的最小距离（$d_{hand-obj}$）评估运动合理性（第5.2.1节，第5-6行）。
- 视觉多样性：基于DINOv2特征与OpenImages数据集的余弦相似度计算（第5.1.1节，第1-4行）。
- 语言多样性：通过名词、动词、形容词的分布统计以及h-index和i100-index评估（第5.1.2节，第1-4行）。

**数据集**：
- 预训练数据：构建的手部VLA数据集（1M片段，26M帧），来源包括Ego4D（77%）、Epic-Kitchen（12%）、EgoExo4D（6%）、SSV2（5%）（第3.4节，第13-15行）。
- 对比基线：EgoDex（实验室环境人类手部VLA数据）、Open X-Embodiment（OXE）、DROID、AgiBot World beta等机器人VLA数据集（第5.1.1节，第8-10行）。

**实验条件**：
- 预训练：8×NVIDIA H100 GPU，2天，批量大小512，学习率（动作专家1e-4，VLM 1e-5）（第5节，第1-3行）。
- 微调：8×NVIDIA H100 GPU，8小时，批量大小256，学习率1e-5（第5节，第4-5行）。

---

### 6. **改进建议和未来研究方向**
**已承认的局限性**：
- 3D运动标注存在噪声和误差，特别是对于快速运动或遮挡情况（第3.1节脚注1）。
- 动作分割算法可能导致过度分割（如来回擦拭动作），需依赖后续指令标注进行合并（第3.2节，第13-14行）。
- 人类与机器人手部动作空间的映射仍较为简化，可能影响微调效果（第4.3节，第7-8行）。

**潜在改进方向**：
1. **多模态融合增强**：可引入触觉或音频模态信息，提升在遮挡或低光照条件下的动作预测鲁棒性。结合多传感器融合技术，可行性较高。
2. **动态动作空间自适应**：当前动作空间固定为102维，可探索基于任务复杂度的动态维度调整，减少冗余计算。需解决训练稳定性问题。
3. **跨域迁移优化**：针对人类-机器人动作域差异，可引入对抗训练或元学习策略，提升跨域泛化能力。该方法与现有框架兼容，实施难度中等。
4. **实时推理加速**：扩散模型的迭代去噪过程导致推理延迟，可研究蒸馏技术或非自回归变体，适用于实时机器人控制场景。

---

本总结严格遵循论文内容，所有陈述均可在原文中找到对应依据。

---

## 2. RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation

### 基本信息
- **作者**: Yuquan Xue, Guanxing Lu, Zhenyu Wu, Chuanrui Zhang, Bofang Jia, Zhengyi Gu, Yansong Tang, Ziwei Wang
- **arXiv ID**: [oai:arXiv.org:2510.17640v2](https://arxiv.org/abs/2510.17640)
- **发布日期**: Mon, 27 Oct 2025 00:00:00 -0400
- **分类**: cs.RO, cs.AI, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.17640)

            ### 原文摘要
            arXiv:2510.17640v2 Announce Type: replace-cross  Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance on complex robotic manipulation tasks through imitation learning. However, existing imitation learning datasets contain only successful trajectories and lack failure or recovery data, especially for out-of-distribution (OOD) states where the robot deviates from the main policy due to minor perturbations or errors, leading VLA models to struggle with states deviating from the training distribution. To this end, we propose an automated OOD data augmentation framework named RESample through exploratory sampling. Specifically, we first leverage offline reinforcement learning to obtain an action-value network that accurately identifies sub-optimal actions under the current manipulation policy. We further sample potential OOD states from trajectories via rollout, and design an exploratory sampling mechanism that adaptively incorporates these action proxies into the training dataset to ensure efficiency. Subsequently, our framework explicitly encourages the VLAs to recover from OOD states and enhances their robustness against distributional shifts. We conduct extensive experiments on the LIBERO benchmark as well as real-world robotic manipulation tasks, demonstrating that RESample consistently improves the stability and generalization ability of VLA models.


            
### AI分析（基于论文正文）
# RESample论文总结

## 1. 论文概要
本论文提出了一种名为RESample的鲁棒数据增强框架，旨在解决机器人模仿学习中分布外泛化问题。该框架通过探索性采样机制，利用离线强化学习获取动作价值网络来识别潜在次优动作，并策略性地将分布外状态样本纳入训练数据集。研究在LIBERO基准测试和真实世界机器人操作任务中验证了该框架的有效性，显著提升了视觉语言动作模型在分布外状态下的稳定性和泛化能力。

## 2. 研究动机
当前基于视觉语言动作模型的机器人模仿学习面临严重的分布外泛化挑战。论文在第I节和第II节中详细分析了现有方法的局限性：传统模仿学习数据集仅包含成功轨迹，缺乏失败或恢复数据，特别是在机器人因微小扰动或错误偏离主要策略时的分布外状态。这种数据缺失导致VLA模型在面对训练分布之外的状态时表现不佳。

现有解决方案存在明显不足：大规模演示数据集收集成本高昂且缺乏任务多样性（见参考文献[3]）；合成数据或仿真数据存在明显的仿真到现实差距（见参考文献[8,9]）；传统数据增强方法依赖简单变换或启发式规则，无法捕捉真实操作场景的复杂变异性（见参考文献[10,11]）；而强化学习方法虽然能探索未见状态，但在物理系统中耗时、不安全且不稳定（见参考文献[13-15]）。

论文在第I节图1中通过瓶颈状态示意图进一步说明了问题的严重性：传统VLA训练策略在面对分布外状态时无法恢复，导致任务失败。这种局限性在长视野、接触丰富的操作任务中尤为突出，因为预测误差会在 rollout 过程中累积，将智能体带入无监督的分布外状态。

## 3. 核心贡献与创新点

**创新点1：探索性采样机制设计**
论文在第III-D节提出了基于策略-评论家分歧的探索性采样机制。该机制的核心创新在于将策略πθ(a|s)的高置信度动作与评论家Qφ(s,a)的低价值评估相结合，识别"自信错误"。具体实现如公式(7)所示：Aexp ≜{a ∈AC | Qφ(st,a) < τQ}，其中τQ表示可接受价值阈值。这种设计使得框架能够有针对性地暴露策略的潜在缺陷，而非进行无目标的随机探索。

**创新点2：演员锚定似然感知评论家**
在第III-C节中，论文提出了专门针对VLA模型定制的演员锚定似然感知评论家。与Cal-QL[31]相比，该方法在轻量级代理演员πψ上进行校准，该代理演员通过行为克隆在D上训练。正则化器R(φ)包含三个组件：均匀惩罚Runi（公式4）抑制动作空间未支持区域的Q值；演员锚定校准Ract（公式5）防止近策略动作的系统性低估；数据内保持Rdata（公式6）保持对演示对经验值的保真度。

**创新点3：离线、基于学习的探索性增强范式**
论文在第II-B节中强调了其与现有方法的区别：传统启发式方法无法捕捉真实世界变化的复杂性，而学习型方法又面临样本效率和安全风险。RESample引入了一种新的离线、基于学习的探索性增强范式，利用离线训练的动作评论家来识别和增强具有物理基础的分布外动作，这些动作专门针对策略的失败模式进行定制。

**创新点4：跨任务增强能力**
在第IV-B节的实验结果中，论文展示了框架的跨任务增强能力（见图5）。从一个任务生成的增强数据可以有效地转移到同一类别中的其他任务，产生额外的5-10%平均性能提升。这表明RESample不仅过度拟合任务特定扰动，而是诱导捕获可泛化失败模式的数据。

## 4. 方法概述

RESample框架的整体流程如图2所示，包含以下关键组件和技术细节：

**价值函数估计与Q学习**
框架依赖于显式动作价值网络（评论家）来指导探索性采样。评论家设计基于Cal-QL范式[31]，但进行了重要改进。具体而言，评论家Qφ(s,a)通过时间差分损失和校准保守项进行优化（公式2）：
min φ LTD(φ)+λR(φ)

正则化器R(φ)（公式3）包含三个互补组件：
- 均匀惩罚Runi(s) = Ea∼Uni(A)[Qφ(s,a)]（公式4），抑制均匀采样动作的Q值
- 演员锚定校准Ract(s) = Ea∼πψ(·|s)[ˆQφ(s,a)]（公式5），其中ˆQφ(s,a) = max(Qφ(s,a),VMC(s))
- 数据内保持Rdata(s) = E(s,a)∼D[Qφ(s,a)]（公式6），保持对演示对的保真度

**探索性采样机制**
探索性采样是框架的核心操作机制，具体流程如下：
1. 在每个状态st，策略生成候选动作集AC ∼πθ(·|st)
2. 评论家过滤此集合以识别探索子集：Aexp ≜{a ∈AC | Qφ(st,a) < τQ}（公式7）
3. 干预规则选择执行动作a∗t：如果子集非空，则选择具有最大策略似然的元素；否则默认使用策略的名义最大化器

**双重精化循环**
框架建立了一个自增强的精化课程：
- 策略在丰富的数据集上重新训练，以获得显式的分布外恢复技能
- 评论家同时在成功和失败上更新，以提高其评估保真度
- 迭代此循环产生类似课程的进展，逐渐发现和纠正越来越细微的漏洞

**训练流程细节**
根据第IV-A节，训练过程分为两个阶段：
1. 基础策略训练：DiT Policy在每个LIBERO任务套件上训练50个周期，批大小64；π0基础策略在整个LIBERO基准上微调30k步，批大小256
2. 数据增强后重新训练：使用探索性采样框架进行数据增强后，使用相同超参数重新训练策略

动作评论家使用SAC算法[32]实现，批大小256，折扣因子γ=0.99，软更新率τ=0.005，温度参数α=5.0。

## 5. 实验说明

**评估指标**
主要评估指标为任务成功率，在LIBERO基准的四个任务类别（空间、对象、目标和长视野）上进行测量。每个任务执行多次试验并记录成功次数。

**数据集**
- 主要评估使用LIBERO基准[16]，包含4个不同任务类别：空间任务、对象任务、目标任务和长视野任务，每个类别包含10个任务
- 真实世界实验使用自定义数据集，包含四个操作任务：拾取积木、堆叠杯子、排列立方体和堆叠2个杯子

**对比基线方法**
论文与以下最先进的VLA模型进行对比：
- Diffusion Policy [21]：基于扩散的连续动作模型
- Octo [2]：开源通用机器人策略
- OpenVLA [3]：开源视觉语言动作模型
- DiT Policy [19]：扩散Transformer策略
- π0 [5]：基于视觉-语言-动作流模型的通用机器人控制

**实验条件**
训练配置：
- DiT Policy：50个周期，批大小64，Adam-W优化器，余弦退火学习率从1e-4开始
- π0：30k步微调，批大小256，Adam-W优化器，余弦退火学习率从2e-5开始
- 动作评论家：SAC算法，批大小256，固定学习率1e-4，50k步收敛

硬件配置：论文中未明确说明训练、微调、推理使用的GPU数量和具体配置。

## 6. 改进建议和未来研究方向

**已识别的局限性**
作者在第V节中明确承认的主要限制包括对动作评论家准确性的依赖。离线训练可能导致估计偏差，从而影响采样轨迹的质量。此外，框架的有效性依赖于策略和评论家之间有意义的分歧，如果评论家过度保守或校准不当，可能会限制探索性采样的效果。

**从方法/结果中推断的局限性**
从实验结果表明，框架在较简单任务（如空间和对象任务）中的改进相对有限，特别是对于基线性能已经很高的π0模型。这表明当策略已经接近最优时，探索性采样的边际收益会减少。此外，表I显示在对象任务中甚至出现了轻微的性能下降（-1.2%），表明强制探索可能在某些情况下引入不必要的扰动。

**结合多领域知识的改进点**

**在线评论家更新机制**
可行性评估：高。结合在线强化学习技术，可以设计混合离线-在线训练方案，其中评论家在部署过程中继续更新。这可以减轻离线训练导致的估计偏差问题，同时通过精心设计的安全约束来管理在线探索的风险。

**课程启发式采样策略**
可行性评估：中。借鉴课程学习理念，可以设计逐步增加挑战性的采样策略。初始阶段关注明显的失败模式，随着策略改进，逐渐转向更细微的分布外场景。这需要开发难度度量标准来排序分布外状态。

**多模态分布外状态识别**
可行性评估：中。结合异常检测和不确定性量化技术，可以增强框架识别多样化分布外状态的能力。除了基于Q值的识别，可以集成基于不确定性的度量，为探索

---

## 3. SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing

### 基本信息
- **作者**: Jesse Haworth, Juo-Tung Chen, Nigel Nelson, Ji Woong Kim, Masoud Moghani, Chelsea Finn, Axel Krieger
- **arXiv ID**: [oai:arXiv.org:2510.20965v1](https://arxiv.org/abs/2510.20965)
- **发布日期**: Mon, 27 Oct 2025 00:00:00 -0400
- **分类**: cs.RO, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.20965)

            ### 原文摘要
            arXiv:2510.20965v1 Announce Type: cross  Abstract: Robotic suturing is a prototypical long-horizon dexterous manipulation task, requiring coordinated needle grasping, precise tissue penetration, and secure knot tying. Despite numerous efforts toward end-to-end autonomy, a fully autonomous suturing pipeline has yet to be demonstrated on physical hardware. We introduce SutureBot: an autonomous suturing benchmark on the da Vinci Research Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying. To ensure repeatability, we release a high-fidelity dataset comprising 1,890 suturing demonstrations. Furthermore, we propose a goal-conditioned framework that explicitly optimizes insertion-point precision, improving targeting accuracy by 59\%-74\% over a task-only baseline. To establish this task as a benchmark for dexterous imitation learning, we evaluate state-of-the-art vision-language-action (VLA) models, including $\pi_0$, GR00T N1, OpenVLA-OFT, and multitask ACT, each augmented with a high-level task-prediction policy. Autonomous suturing is a key milestone toward achieving robotic autonomy in surgery. These contributions support reproducible evaluation and development of precision-focused, long-horizon dexterous manipulation policies necessary for end-to-end suturing. Dataset is available at: https://huggingface.co/datasets/jchen396/suturebot


            
### AI分析（基于论文正文）
**论文概要**  
本论文针对机器人缝合这一典型的长时程灵巧操作任务，提出了一种基于目标条件化模仿学习的自主缝合框架SutureBot，并在达芬奇研究套件（dVRK）平台上构建了包含1,890条真实世界演示数据的公开基准。该方法通过引入目标点条件化机制，显著提升了缝合针插入点的定位精度（平均误差降低59%-74%）。论文系统评估了四种视觉-语言-动作模型（π0、GR00T N1、OpenVLA-OFT、ACT）在分层策略架构下的性能，并分析了预训练、目标表示形式对策略精度与泛化能力的影响。

---

**研究动机**  
当前机器人缝合领域存在三个核心问题：首先，现有方法多集中于单一子任务（如针拾取、打结），缺乏端到端缝合的系统性解决方案（第1节指出STAR、SNAP等方法未实现完整流程）。其次，传统方法如模型预测控制（MPC）虽能实现高精度缝合（第1节引用[19]），但依赖精确建模且难以适应组织形变，而模仿学习（IL）策略虽具备鲁棒性，却受限于数据规模——现有缝合数据集不足200条轨迹（第1节引用[8,33]），远低于通用任务所需的百万级数据量（如π0.5的预训练数据）。此外，手术机器人领域缺乏标准化基准与精度评估指标（第1节指出当前仅依赖粗粒度任务完成率），难以量化策略的临床级精度需求。  

通过分析全文可推断，作者未明确陈述但隐含的动机包括：1）解决长时程任务中子任务序列的协调问题（第2.1节任务分解设计）；2）探索小规模领域专用数据下预训练VLA模型的有效迁移（第3.2节显示π0在dVRK平台的性能受限）；3）建立可复现的评估体系以推动手术自主化研究（第2节强调数据与基准的公开性）。

---

**核心贡献与创新点**  
1. **首个端到端自主缝合基准与大规模数据集**  
   - 贡献描述：构建了包含1,890条高保真演示的dVRK缝合数据集，覆盖针拾取、组织穿刺、打结三个子任务及恢复演示（第2.1节）。数据集提供同步视觉-动力学数据及手动标注的插入/退出点坐标（CSV格式），并引入器械配置、光照、初始位姿等多样性因素。  
   - 依据：第2.1节详细说明数据组成（628条针拾取、310条穿刺、952条打结）与标注流程（GUI标注工具）。  
   - 创新性：相较此前最大缝合数据集（<200条），数据规模提升近10倍，且首次公开包含恢复轨迹的缝合数据。

2. **目标条件化模仿学习框架**  
   - 贡献描述：提出通过目标点条件化提升缝合精度的机制，设计三种目标表示形式（点标签、二值掩码、距离图），并验证点标签形式能显著降低插入点误差（第3.2节）。  
   - 依据：表1显示点标签使ACT和π0的插入误差分别降至1.3mm与1.0mm，统计检验结果（p<0.0167）证实其优于其他表示。  
   - 创新性：首次将目标条件化应用于手术策略的精度控制，突破传统IL仅优化任务完成率的局限。

3. **分层策略架构与多任务统一训练**  
   - 贡献描述：采用高层语言策略（Swin Transformer编码）与低层VLA策略的级联结构，支持基于图像观测的子任务自动切换（图1）。首次实现单一低层策略（如ACT）对全部缝合子任务的统一控制。  
   - 依据：第2.2节说明高层策略的88.73%任务预测准确率，表3显示Oracle实验验证其与人工指令的等效性。  
   - 创新性：克服了SRT等工作中需为每个任务训练独立模型的限制（第1节对比），通过语言条件化实现子任务间平滑过渡。

4. **系统性评估与泛化分析**  
   - 贡献描述：在6种伤口几何、光照与器械变更条件下测试策略泛化能力（表4），揭示VLA模型（π0）在几何变化下的稳定性与数据多样性的必要性。  
   - 依据：第3.3节显示π0在未见伤口上保持相近精度（插入误差2.0mm vs 1.9mm），而ACT性能显著下降。  
   - 创新性：首次在真实手术环境中量化VLA模型对多维度环境变化的适应性。

---

**方法概述**  
1. **分层策略架构**  
   - 高层策略：采用Swin Transformer编码立体内窥镜图像，通过Transformer解码器生成语言指令（如“执行针穿刺”），实现基于视觉的子任务分类（第2.2节，图1）。  
   - 低层策略：接收语言指令、腕部相机图像、内窥镜图像及目标条件，输出连续机器人动作。评估四种模型：  
     - **ACT**：基于Transformer的动作分块预测，通过L1回归损失训练（第2.2节）。  
     - **π0与GR00T N1**：采用流匹配动作头与VLM骨干网络，使用MSE损失（第2.2节）。  
     - **OpenVLA-OFT**：基于并行解码与FiLM条件化（第2.2节引用[22]）。  

2. **目标条件化机制**  
   - 点标签：在内窥镜图像上叠加蓝/绿像素标记插入/退出点（图4）。  
   - 二值掩码：三通道图像，通道2-3分别编码插入/退出区域（第2.3节）。  
   - 距离图：前两通道编码归一化像素偏移向量(dx,dy)，第三通道为插入点热图（第2.3节）。  
   - 训练时，目标条件与观测共同输入低层策略；推理时通过GUI指定目标点生成条件（第2.1节）。

3. **训练流程**  
   - 数据划分：保留每任务2条演示及对应恢复数据作为评估集（共12条）。  
   - 优化配置：ACT与OpenVLA使用L1损失训练≥10,000步；π0与GR00T使用MSE损失，早停防止过拟合（第2.2节）。  
   - 硬件平台：NVIDIA DGX A100（8×A100 80GB GPU），推理使用双RTX 4090工作站（第3.2节）。

4. **精度评估机制**  
   - 采用紫外标记法：术前用UV笔标注目标点，术中关闭UV灯隐藏标记，术后通过ImageJ测量实际缝合点与标记的欧氏距离（第3.1节，图5）。  
   - 插入/退出误差作为核心精度指标，与任务成功率、端到端完成率共同评估。

---

**实验说明**  
**评估指标**  
- 任务成功率：针拾取（120s内完成抓取与传递）、组织穿刺（120s内穿透前后壁）、线拉穿（60s内抽针返回）、打结（120s内完成绕线与收紧）（第3.1节）。  
- 精度指标：插入点误差与退出点误差（毫米级欧氏距离）。  
- 效率指标：单任务耗时与端到端流程总时间。  

**数据集**  
- 3-D Med软组织缝合垫，绿色编织聚酯缝线（3-0 Ethibond）。  
- 数据模态：同步记录的机器人动力学（6-DOF笛卡尔位姿、关节角、RCM帧位姿）与视觉数据（腕部相机640×480@30Hz，立体内窥镜960×540@30Hz）。  

**基线方法**  
- **通用VLA模型**：π0（流匹配专家）、GR00T N1（人形机器人预训练）、OpenVLA-OFT（并行解码）。  
- **专用IL模型**：多任务ACT（动作分块Transformer）。  
- **对照方法**：无目标条件化基线、Oracle人工指令变体（表3）。  

**实验条件**  
- 训练硬件：NVIDIA DGX A100（8×A100 80GB GPU），论文明确说明（第2.2节）。  
- 推理硬件：双NVIDIA RTX 4090工作站，论文明确说明（第3.2节）。  
- 超参数：早停策略基于评估损失，具体数值未在正文中给出（提及见附录）。

---

**改进建议和未来研究方向**  
**已承认的局限性**  
1. **数据规模与多样性不足**：实验仅基于单一缝合垫材质，光照与器械变更导致性能显著下降（表4），作者指出需扩展至生物组织、血液干扰等场景（第5节）。  
2. **目标点选择依赖人工**：当前需通过GUI手动指定目标点，阻碍全流程自动化（第5节对比[26]的自动目标检测）。  
3. **评估样本量有限**：每组实验仅

---

## 4. LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models

### 基本信息
- **作者**: Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu
- **arXiv ID**: [oai:arXiv.org:2510.13626v2](https://arxiv.org/abs/2510.13626)
- **发布日期**: Mon, 27 Oct 2025 00:00:00 -0400
- **分类**: cs.RO, cs.CL, cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.13626)

            ### 原文摘要
            arXiv:2510.13626v2 Announce Type: replace  Abstract: Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.


            
### AI分析（基于论文正文）
### 论文概要
本文针对视觉-语言-动作模型在标准化基准测试中表现出的表面高成功率与实际鲁棒性不足之间的矛盾，通过系统化扰动分析揭示了VLA模型的内在脆弱性。研究基于LIBERO基准构建了涵盖物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声等七个维度的扰动测试框架，对10个代表性VLA模型进行了全面评估。实验结果表明，即使轻微扰动也会导致性能急剧下降（如相机视角变化使性能从95%降至30%以下），且模型普遍存在忽略语言指令、依赖位置记忆而非语义理解等问题。研究进一步提出了LIBERO-Plus基准和统计框架量化组合扰动效应，为VLA模型的鲁棒性评估提供了系统方法论。

### 研究动机
当前VLA模型在标准化基准测试中展现出接近完美的成功率（如LIBERO基准），但这些表面数据掩盖了模型在现实环境变化下的根本性弱点。论文在第1章指出，现有评估方法（如Liu et al. 2023; Li et al. 2024c）主要关注静态理想条件下的聚合成功率，未能捕捉模型在现实变化下的稳定性和可靠性。这种评估方式掩盖了模型处理模拟环境中固有细微变化的能力不足，即使这些任务仍处于仿真范畴。

通过分析第2章和第4章的实验设计，可推断作者动机源于以下观察：在固定相机角度或恒定光照下训练的模型，当面临视角轻微偏移或机器人初始配置微小变化时往往无法泛化。这种缺陷对VLA模型尤为严重，因为此类模型必须整合多模态信息并在任何输入通道受到扰动时保持连贯行为。论文第2.3节通过表1数据显示，现有模型在基准测试中的高成功率实际反映了对训练期间特定狭窄定义线索的过拟合，而非真正的多模态理解能力。

此外，第4章的语言指令实验进一步揭示了深层问题：模型对语言输入的利用严重不足，甚至完全忽略指令内容。这种多模态交互的表面能力与实际理解之间的不匹配，促使作者开展系统性脆弱性分析，以揭示当前VLA模型评估范式的局限性。

### 核心贡献与创新点
1. **系统性脆弱性分析框架**  
   论文构建了覆盖七个关键维度的扰动测试体系（第2.1节），包括物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声。每个维度均设计了具体扰动规范（附录A），如物体布局扰动分为添加干扰物体和改变目标物体位置两个子类（图1）。这种多维度分析首次全面量化了VLA模型对各种环境变化的敏感度，揭示了模型在相机视角和机器人初始状态变化下的极端脆弱性（表1显示性能下降最高达87.6%）。

2. **语言指令利用不足的实证发现**  
   通过第4章的空白指令实验（图3a）和目标替换实验（图3b），论文首次证实当前VLA模型实质上忽略了语言模态。具体而言，当语言输入被替换为空值时，OpenVLA-OFT在物体套件上的性能基本不变；当任务目标被替换为场景内其他物体时，成功率骤降至近零。这表明模型的“语言鲁棒性”实际源于对语言输入的忽视，而非真正的语言理解能力（发现7）。

3. **组合泛化间隙的统计定义**  
   第5章提出了组合泛化间隙的统计框架，通过定义随机变量Di和Dj表示扰动应用情况（公式1），成功指标Y（公式2），以及条件概率s(Di=d_i, Dj=d_j)（公式3），最终推导出组合间隙Δij作为给定成功条件下两个扰动的协方差（公式8）。该框架首次量化了多扰动组合下的非线性效应，发现一致的负组合间隙（图4）表明扰动间存在耦合噪声源，暴露了学习表征的纠缠问题（发现9）。

4. **LIBERO-Plus基准构建**  
   基于分析结果，第6章提出了包含10,030个任务的LIBERO-Plus基准，涵盖七个扰动维度和21个底层组件（图6）。该基准通过自动化任务生成和基于四代表现模型的精度分布将任务分层为五个难度级别（L1-L5），提供了细粒度评估能力（图5）。与现有基准相比（表3），LIBERO-Plus在自动化程度、扰动维度和细粒度分析方面均有显著提升。

### 方法概述
论文采用分层实验设计，从单维度扰动分析逐步扩展到多维度组合评估。技术方案核心包括：

**单维度扰动测试框架**（第2章）：  
对每个扰动维度设计具体操作流程。物体布局扰动通过添加干扰物体和改变目标物体位置实现（图1）；相机视角扰动改变第三人称相机的视角姿态和视野范围；机器人初始状态扰动调整机械臂初始姿态；语言指令扰动通过重写任务指令增加语言丰富性和复杂性；光照条件扰动改变光照强度、方向、颜色和阴影模式；背景纹理扰动修改场景纹理和材质；传感器噪声扰动向输入图像注入光度失真（如抖动、高斯模糊）。每个模型在原始条件和各扰动条件下的成功率通过大量独立实验计算（表1）。

**语言指令分析模块**（第4章）：  
采用三重假设验证策略：假设1（语言泛化能力）通过常识子类关键词重写测试；假设2（关键词提取）通过空白指令实验直接验证；假设3（语言模态忽略）通过目标替换任务证实。具体地，空白指令实验将语言输入完全替换为空值，观察性能变化（图3a）；目标替换任务将指令中的目标物体替换为场景内其他物体，如“拿起字母汤”替换为“拿起番茄酱”，并构建新目标指令序列（图3b）。

**组合泛化统计模型**（第5章）：  
基于概率论构建评估框架。首先定义扰动应用随机变量Di和成功指标Y，然后计算条件概率s(Di=d_i, Dj=d_j) = P(Y=1|Di=d_i, Dj=d_j)。通过2000次独立重复实验估计联合概率p(Di=d_i, Dj=d_j|Y=1)（公式4）和边际概率p(Di=1|Y=1)（公式5）、p(Dj=1|Y=1)（公式6）。最终计算组合间隙Δij = Cov(Di, Dj|Y=1) = p(Di=1, Dj=1|Y=1) - p(Di=1|Y=1)p(Dj=1|Y=1)（公式8），并通过卡方检验验证统计显著性（附录F）。

**基准构建与验证流程**（第6章）：  
LIBERO-Plus构建分为两个阶段：首先系统扩展原始LIBERO基准，应用七种扰动因素后基于第2章发现过滤和平衡任务类别；然后使用四代表现模型评估生成任务，根据精度分布将任务分层为五个难度级别。最终基准包含10,030个任务，通过自动化流程确保大规模多样本评估。基于该基准的增强训练实验使用超过20,000条成功轨迹数据集，从官方OpenVLA-OFT权重开始进行混合微调，验证广义训练对模型鲁棒性的提升效果（表2）。

### 实验说明
**评估指标**：  
主要使用任务成功率（%），定义为成功执行任务的测试案例比例。对于组合泛化分析，额外使用条件概率和组合间隙Δij作为量化指标。

**数据集**：  
基于LIBERO基准扩展构建的LIBERO-Plus数据集，包含10,030个任务，覆盖七个扰动维度：物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声。数据集通过自动化流程生成，包含五个难度级别（L1-L5）的任务分层。

**对比基线方法**：  
论文评估了10个代表性VLA模型，按架构和训练范式分类：  
- 自回归模型：OpenVLA (Kim et al., 2024)及其变体（OpenVLA-OFT、OpenVLA-OFT_w、OpenVLA-OFT_m）、π0 (Black et al.)、π0-fast (Pertsch et al., 2025)、Nora (Hung et al., 2025)  
- 扩散模型：WorldVLA (Cen et al., 2025)  
- 其他范式：UniVLA (Bu et al., 2025)、RIPT-VLA (Brohan et al., 2022)  
详细模型描述见附录B。

**实验条件**：  
论文中未明确说明训练、微调、推理阶段使用的GPU具体数量和配置。实验在LIBERO模拟环境中进行，使用2000次独立重复实验确保统计显著性。对于增强训练实验，使用超过20,000条成功轨迹的数据集，从官方OpenVLA-OFT权重开始进行混合微调。

### 改进建议和未来研究方向
**已识别的局限性**：  
1. **位置偏差而非语义理解**：发现5表明当前VLA模型依赖记忆的位置线索而非不变的对象语义，当目标物体位置改变时性能显著下降。  
2. **语言模态利用不足**：发现7证实模型实质上忽略语言指令，仅依赖视觉-动作映射完成任务。  
3. **负组合泛化间隙**：发现9显示扰动组合引入超出独立效应的额外难度，表明模型缺乏捕捉高阶依赖的机制。  
4. **手腕相机依赖性**：发现6揭示对手腕相机输入的过度

---

## 5. Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation

### 基本信息
- **作者**: Dogyun Park, Taehoon Lee, Minseok Joo, Hyunwoo J. Kim
- **arXiv ID**: [oai:arXiv.org:2510.21167v1](https://arxiv.org/abs/2510.21167)
- **发布日期**: Mon, 27 Oct 2025 00:00:00 -0400
- **分类**: cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.21167)
- **源码地址**: [查看源码](https://github.com/mlvlab/bfm.)

            ### 原文摘要
            arXiv:2510.21167v1 Announce Type: new  Abstract: Recently, Flow Matching models have pushed the boundaries of high-fidelity data generation across a wide range of domains. It typically employs a single large network to learn the entire generative trajectory from noise to data. Despite their effectiveness, this design struggles to capture distinct signal characteristics across timesteps simultaneously and incurs substantial inference costs due to the iterative evaluation of the entire model. To address these limitations, we propose Blockwise Flow Matching (BFM), a novel framework that partitions the generative trajectory into multiple temporal segments, each modeled by smaller but specialized velocity blocks. This blockwise design enables each block to specialize effectively in its designated interval, improving inference efficiency and sample quality. To further enhance generation fidelity, we introduce a Semantic Feature Guidance module that explicitly conditions velocity blocks on semantically rich features aligned with pretrained representations. Additionally, we propose a lightweight Feature Residual Approximation strategy that preserves semantic quality while significantly reducing inference cost. Extensive experiments on ImageNet 256x256 demonstrate that BFM establishes a substantially improved Pareto frontier over existing Flow Matching methods, achieving 2.1x to 4.9x accelerations in inference complexity at comparable generation performance. Code is available at https://github.com/mlvlab/BFM.


            
### AI分析（基于论文正文）
### 论文概要

本文提出Blockwise Flow Matching (BFM)，一种改进流匹配模型的高效高质量生成框架。BFM将生成轨迹划分为多个时间片段，每个片段由小型专用速度块建模，解决了传统单一模型难以同时捕捉不同时间步信号特征的问题。通过引入语义特征引导模块和特征残差近似策略，BFM在ImageNet 256×256上实现了2.1×至4.9×的推理加速，同时保持或提升生成质量（FID从基线2.06降至1.75）。研究范围涵盖生成模型架构优化、语义特征对齐及高效推理策略。

---

### 研究动机

传统流匹配模型（如SiT、FiT）采用单一大型网络学习从噪声到数据的整个生成轨迹（第1节），面临两个核心问题：  
1. **时序异质性冲突**：生成轨迹不同阶段的信号特征存在显著差异。如图3所示，早期时间步以不规则低频模式为主，后期需建模精细高频细节。单一模型需同时处理这些冲突需求，导致对全局结构和局部细节的捕捉能力受限（第1节，图3）。  
2. **推理效率低下**：单一模型需在每个求解器步骤中全量计算，总复杂度为O(SK)（S为步数，K为模型FLOPs），限制了实际部署（第1节）。  

现有工作如REPA[26]通过语义特征对齐提升生成质量，但未解决时序特征差异问题，且计算效率未优化（第2.1节）。动态推理方法（如DyDiT[50]）虽调整计算图，但仍依赖单一共享模型（第2.2节）。BFM的动机源于通过时序分块设计，使每个模块专注于局部动态，同时引入语义条件化以弥补分块导致的语义信息缺失（第4.1-4.2节）。

---

### 核心贡献与创新点

1. **Blockwise Flow Matching (BFM)框架**  
   - **创新点**：将连续生成轨迹划分为M个非重叠时间片段{[t_{m-1}, t_m)}，每个片段由专用速度块v_θ^(m)建模（第4.1节）。  
   - **依据**：公式(4)-(8)定义了分块训练目标，其中每个块仅处理局部时间窗内的速度场（见第4.1节，公式(7)-(8)）。  
   - **区别**：与传统FM（如SiT[22]）使用单一模型相比，BFM通过分块 specialization 提升模型表达能力，且推理时仅激活对应块，降低计算量（表1：BFM-S比SiT-S FLOPs降低1.5×，FID提升1.1）。  

2. **语义特征引导模块**  
   - **创新点**：设计共享特征对齐网络f_ϕ，从噪声状态x_t提取语义特征f_t，并通过对齐损失（公式(9)）使其与预训练编码器（如DINOv2[25]）特征一致。  
   - **依据**：f_ϕ输出作为条件输入至速度块v_θ^(m)（公式(10)），增强块对高层语义的感知（第4.2节，图2a）。  
   - **区别**：与REPA[26]直接对齐隐状态不同，SemFeat采用模块化设计，分离特征学习与生成建模，提升特征一致性（图8：PCA可视化显示BFM特征更稳定；表6：BFM+SemFeat比FM+REPA FID降低6.0）。  

3. **特征残差近似策略**  
   - **创新点**：引入轻量特征残差网络f_η，通过残差连接f_t ≈ f_{t_{m-1}} + b_m(t)·f_η(x_t, c)近似语义特征，其中b_m(t)为归一化时间偏移（公式(12)）。  
   - **依据**：图5显示片段内特征差异随t增大而增加，残差设计可有效建模该动态（第4.3节）。  
   - **区别**：相比直接近似，残差连接加速收敛并降低近似误差（图7：L_FRN损失下降更快；表1：BFM-SSF-RA比BFM-SSF FLOPs降低41%，FID仅增1.4）。  

---

### 方法概述

BFM框架包含三个核心组件，其工作流程如下：  
1. **分块速度场建模**：  
   - 将时间域[0,1]划分为M个片段，每个片段m对应速度块v_θ^(m)。训练时，对t ∈ [t_{m-1}, t_m)，按公式(6)采样x_t，并计算真实速度v_t^(m)（公式(7)）。块参数通过最小化分块流匹配损失（公式(8)）学习。  
   - 推理时，根据当前t选择对应块预测速度，仅需计算单个块（第4.4节，算法3）。  

2. **语义特征引导集成**：  
   - 特征对齐网络f_ϕ接收x_t和条件c，输出f_t。其训练目标为最小化对齐损失L_align（公式(9)），其中h_ψ为投影层，d为余弦距离。  
   - f_t与时间步嵌入求和后，通过AdaLN-Zero调制注入v_θ^(m)的注意力层（第5节）。总损失为L_BFM + λL_align（公式(11)），λ平衡两项权重（第4.2节，算法1）。  

3. **特征残差近似推理**：  
   - 训练完成后冻结f_ϕ和v_θ，训练FRN f_η以最小化特征回归损失L_FRN（公式(12)）。  
   - 推理时，每片段起始点计算f_{t_{m-1}} = f_ϕ(x_{t_{m-1}}, c)，后续时间步通过f_η近似f_t，避免重复计算f_ϕ（第4.3节，图2b-c，算法2）。  

**关键公式与组件**：  
- 分块插值：x_t = (1 - a_m(t))x_{t_{m-1}} + a_m(t)x_{t_m}（公式(6)）  
- 速度目标：v_t^(m) = (x_{t_m} - x_{t_{m-1}})/(t_m - t_{m-1})（公式(7)）  
- 残差近似：f_t = f_{t_{m-1}} + b_m(t)·f_η(x_t, c)（第4.3节）  

---

### 实验说明

**评估指标**：  
FID（↓）、sFID（↓）、Inception Score（IS↑）、Precision（↑）、Recall（↑）（第5节）。  

**数据集**：  
ImageNet 256×256（第5节），使用Stable Diffusion VAE进行8×下采样（第5节）。  

**对比基线**：  
- **传统扩散/流匹配模型**：ADM、CDM、LDM-4、VDM++、U-ViT-H  
- **Transformer基模型**：MDTv2-XL、MaskDiT、FlowDCN、SimpleDiffusion、SD-DiT  
- **高效生成模型**：FiTv2、DiT-XL、SiT-XL、REPA-XL、DiffMoE-L、DyDiT-XL（第5.1节，表2）  

**实验条件**：  
- **训练**：BFM-XLSF使用400轮，BFM-XLSF-RA使用600轮；VAE为Stable Diffusion预训练模型；优化器及学习率未明确说明。  
- **硬件配置**：论文中未明确说明GPU数量与配置。  
- **推理**：使用250步ODE Euler求解器（表2），BFM步数为246；分类器引导权重w=4.0（图4）。  

---

### 改进建议和未来研究方向

**已提及局限性**：  
1. **分块语义暴露不足**：时间分块限制各块对干净信号的接触，可能影响高层特征学习（第4.2节）。  
2. **残差近似误差累积**：FRN的近似误差可能随片段内时间步增加而累积（图5，第4.3节）。  

**未明确潜在局限**：  
1. **块间协调性**：分块独立训练可能忽略跨块动态连续性，导致生成轨迹不平滑。  
2. **扩展性约束**：BFM总参数量随块数线性增长（表1：BFM-S总参量99M vs SiT-S 33M），可能限制进一步分段。  

**改进建议**：  
1. **动态块间连接**：引入跨块注意力机制（如交叉块门控），增强全局语义传递，可行性高（可借鉴Transformer-XL等序列模型）。  
2. **自适应分块策略**：根据频谱分析（图3）动态调整片段边界，使分块更契合信号变化点，需设计轻量边界决策网络

---

