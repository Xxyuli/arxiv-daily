# arXiv论文监控报告 - 2025年12月09日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2025年12月09日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 6篇

---

## 1. HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies

### 基本信息
- **作者**: Zhiying Du, Bei Liu, Yaobo Liang, Yichao Shen, Haidong Cao, Xiangyu Zheng, Zhiyuan Feng, Zuxuan Wu, Jiaolong Yang, Yu-Gang Jiang
- **arXiv ID**: [oai:arXiv.org:2512.05693v1](https://arxiv.org/abs/2512.05693)
- **发布日期**: Mon, 08 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.05693)
- **源码地址**: [查看源码](https://github.com/zhiyingdu/himoe-vla.)

            ### 原文摘要
            arXiv:2512.05693v1 Announce Type: cross  Abstract: The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.


            
### AI分析（基于论文正文）
### **论文概要**

本文提出了一种名为HiMoE-VLA的新型视觉-语言-动作（VLA）框架，旨在解决从大规模异构机器人演示数据中学习通用策略的核心挑战。该框架的核心创新在于其动作模块采用了一种分层混合专家（Hierarchical Mixture-of-Experts, HiMoE）架构。该架构通过位于浅层的动作空间MoE（AS-MoE）和位于相邻层的异构平衡MoE（HB-MoE），分别处理动作空间差异（如关节空间与末端执行器空间控制）和更广泛的异构性来源（如本体差异、传感器配置），并逐步将其抽象为共享的知识表示。实验表明，在CALVIN、LIBERO等模拟基准以及xArm、ALOHA等真实机器人平台上，HiMoE-VLA在成功率和跨域泛化能力上均超越了现有VLA基线方法。

### **研究动机**

构建具身智能基础模型的关键在于利用大规模、高质量的机器人演示数据。尽管近期研究（如Li等人，2024；Qu等人，2025）尝试在Open X-Embodiment（OXE）等大规模异构数据集上进行预训练，但现有VLA模型（如RT-2、OpenVLA）在处理机器人数据固有的、多方面的异构性时仍面临根本性挑战（见第1节）。这种异构性主要体现在：1）**本体与动作空间**：不同机器人的自由度、关节构型、动作表示（如关节角度、末端执行器位姿）存在显著差异；2）**感知配置**：传感器数量、视角（如第三人称、腕部摄像头）各不相同；3）**控制频率与操作风格**：数据采集时的控制频率和人类遥操作风格引入额外变化（见第1节）。

现有方法缺乏处理这种异构性的原则性设计。例如，RDT-1B（Liu等人，2024）为双手操作引入了统一动作空间，但缺乏处理同一动作空间内异构性的架构机制。HPT（Wang等人，2024a）采用数据集特定的“茎干”和“头部”来对齐不同输入，但这限制了跨数据集的知识迁移（见第2节“Vision-Language-Action Models”部分）。直接混合这些异构数据源（如表6(b)所示）会导致模型难以有效整合知识，从而限制其泛化能力，并在迁移到新设置时性能下降。因此，一个核心且紧迫的科学问题是：**如何从高度异构的机器人数据中学习一个可泛化的机器人基础模型？** 本文的研究动机即源于此，旨在通过一种新颖的架构设计，显式地解耦并处理不同来源的异构性，实现更有效的跨域知识迁移。

### **核心贡献与创新点**

本文的核心贡献在于提出了一种专门为处理异构机器人数据而设计的分层混合专家VLA框架，具体创新点如下：

1.  **面向异构机器人数据的层级化MoE架构**：本文提出了一个新颖的**分层混合专家（HiMoE）** 架构，作为VLA框架中的动作模块（见第3.2.2节，图2）。该架构并非使用单一或同质的MoE层，而是进行了**层级化组织**：
    *   **动作空间MoE（AS-MoE）**：置于网络**浅层边界**。其核心功能是专门处理**动作空间**的差异，例如关节角度控制与末端执行器位姿控制之间的根本性不匹配。通过让浅层专家专注于此类细粒度、非传递性的差异，避免了不同动作空间信号在早期混合导致的干扰（见表6(b)和表7的对比实验）。
    *   **异构平衡MoE（HB-MoE）**：置于**AS-MoE的相邻层**。其职责是自适应地处理更广泛的异构性来源，包括机器人本体特定的运动学、传感器配置、场景变化等。HB-MoE旨在将这些多样性逐步抽象和平衡，为后续的共享表示学习做准备。
    *   **中间Transformer块**：位于AS-MoE和HB-MoE之间，作为**共享层**，负责将经过专家初步处理的异构信号整合为统一的、可跨域泛化的知识表示（见第1节及图2说明）。

    这种“AS-MoE → Transformer → HB-MoE”的层级结构是本文与先前MoE工作在概念上的关键区别（见第2节“Mixture of Experts”部分），它实现了从特定差异处理到通用知识抽象的渐进式流程。

2.  **针对层级化架构的专用正则化方法**：为了引导上述层级化架构实现预期功能，本文设计了两种互补的正则化目标，与流匹配（Flow-Matching）损失共同构成训练目标（公式1）：
    *   **动作空间正则化（AS-Reg）**：一种基于对比学习的目标（公式4，5），应用于AS-MoE。它鼓励被路由到**同一动作空间令牌**的专家们产生相似的表示（正样本对），而与路由到其他动作空间令牌的专家表示相异（负样本对）。这**强化了AS-MoE专家在动作空间维度上的专业化**（见第3.3节）。
    *   **异构平衡正则化（HB-Reg）**：一种旨在平衡专家负载的目标（公式6，7），应用于HB-MoE。它通过最小化每个专家的实际路由频率（`fi`）与其期望路由概率（`Pi`）之间的差异，确保来自不同异构来源的输入能够**更均匀地分布到各个HB-MoE专家**中。这防止了专家利用不足，并**促进了在更深层对广泛异构性进行平衡的抽象**（见第3.3节）。

3.  **在模拟与真实机器人任务上验证的卓越性能与泛化能力**：通过在大规模异构数据（OXE + ALOHA，共2410万帧）上预训练，并在多个具有挑战性的基准上微调评估，HiMoE-VLA取得了当前最优（SOTA）性能。在CALVIN的D→D设置中，其连续任务完成总数（3.967）超越了π0、MDT等强基线（见表1）。在LIBERO的四个任务套件上，平均成功率（97.8%）也超过了UniVLA、OpenVLA-OFT等方法（见表2）。更重要的是，在xArm和ALOHA真实机器人上的实验表明，该模型不仅在已知任务上成功率更高（见表3，4），在包含**未见干扰物**和**全新物体**的泛化测试中也表现出更强的鲁棒性（见表5），验证了其处理异构性并实现有效知识迁移的设计初衷。

### **方法概述**

HiMoE-VLA的整体架构如图1所示，包含一个预训练的视觉-语言模块（VLM）和一个新提出的基于HiMoE的动作模块。

**1. 视觉-语言模块**：采用PaliGemma模型（Beyer等人，2024）作为骨干网络，与π0（Black等人，2024）相同。该模块将语言指令和RGB图像（通常包括第三人称和腕部视角）编码为多模态表示。关键设计是**从语言模型中间层提取键值（KV）表示**，而非仅使用最终层输出，以提供更强的条件信号（见第3.2.1节及附录B）。这些KV缓存将在动作模块的每一层用于交叉注意力。

**2. 动作模块与分层MoE（HiMoE）**：这是方法的核心。其输入是机器人的本体感知状态 `qt` 和加噪的动作序列 `Aτ_t`（公式2）。首先，将不同动作空间（如7维关节角、6维末端位姿）的数据映射到一个**固定的24维统一向量**中，并进行归一化（见第4节“Implementation Details”）。随后，向量通过轻量级MLP后输入HiMoE。

**HiMoE的详细工作流程**（见图2）：
*   **层级结构**：网络由多个块堆叠而成。每个块遵循“**AS-MoE层 → Transformer块 → HB-MoE层**”的顺序。AS-MoE位于最浅层，HB-MoE位于相邻深层，中间是标准的Transformer编码器块。
*   **MoE路由机制**：AS-MoE和HB-MoE均包含N个专家（默认N=32）。对于每个输入令牌，一个门控网络（线性投影+Softmax）计算其对于所有专家的权重。采用**Top-K路由**（默认K=4），即只激活权重最高的K个专家，其输出加权求和作为该MoE层的输出。这种设计保持了模型的稀疏性。
*   **层间融合**：**每一层**的专家或Transformer块的输出，都会与从VLM骨干提取的对应层的**KV表示进行交叉注意力计算**（见第3.2.2节）。这种逐层的融合使得低级视觉线索和高级语义信息能够贯穿整个层次结构，实现浅层有效专业化、深层强泛化的效果。
*   **输出**：经过多个HiMoE块处理后，最终的融合表示通过一个预测头，输出**去噪后的未来动作序列** `At`。

**3. 训练目标**：总损失函数如公式1所示，包含三部分：
*   **流匹配损失（L_flow）**：采用流匹配目标（公式3）来建模动作序列的条件分布

---

## 2. Training-Time Action Conditioning for Efficient Real-Time Chunking

### 基本信息
- **作者**: Kevin Black, Allen Z. Ren, Michael Equi, Sergey Levine
- **arXiv ID**: [oai:arXiv.org:2512.05964v1](https://arxiv.org/abs/2512.05964)
- **发布日期**: Mon, 08 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.05964)

            ### 原文摘要
            arXiv:2512.05964v1 Announce Type: cross  Abstract: Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $\pi_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Training-Time Action Conditioning for Efficient Real-Time Chunking》及其所有约束条件，生成一份详尽的论文总结。

***

### **论文总结报告**

**1. 论文概要**
本文针对实时分块（RTC）框架中，为保证动作块间连续性而采用的推理时修复（inpainting）方法所带来的计算开销和延迟问题，提出了一种名为“训练时动作条件化”的替代方案。该方法通过在训练阶段模拟推理延迟，直接对动作前缀进行条件化，从而完全消除了推理时的额外计算。研究在模拟环境和真实机器人任务（搭建纸盒、制作意式浓缩咖啡）上验证了该方法的有效性，结果表明，在高延迟场景下，训练时RTC性能优于推理时RTC，且在真实任务中能以更低的计算成本维持相同的任务完成率和速度。

**2. 研究动机**
在具身智能领域，视觉-语言-动作模型（VLAs）因其强大的泛化能力被用于高频机器人控制。然而，大模型固有的推理延迟（数十至数百毫秒）与实时、反应式控制的需求存在根本矛盾。实时分块（RTC）[5] 通过异步预测动作块并利用推理时修复技术对已执行的动作前缀进行条件化，以生成平滑的轨迹，是解决该矛盾的有效框架。

然而，作者指出（见第I节），RTC所依赖的推理时修复方法（基于伪逆引导[18, 21]）本身引入了额外的计算开销和延迟，这在一定程度上削弱了实时执行框架的初衷。具体而言，该方法需要在每个去噪步骤中计算向量-雅可比乘积（即反向传播），增加了单次推理的耗时（见第IV节）。此外，作者通过实验发现（见第V.A节及图3），推理时修复方法在处理高推理延迟时存在根本性局限：随着需要修复的动作前缀长度增加，修复算法的负担加重，导致性能下降。

因此，本文的研究动机是：**寻找一种能够保持RTC框架异步执行和动作连续性优势，同时完全避免推理时计算开销的替代方案**。作者旨在提出一种简单、无需修改模型架构或运行时系统、且能作为“即插即用”替代品的方法。

**3. 核心贡献与创新点**
本文的核心贡献是提出并系统验证了 **“训练时动作条件化”** 方法，以替代RTC中的推理时修复。其创新点具体体现在以下三个方面：

1.  **概念创新：将延迟补偿从推理时转移至训练时。** 这是本文最核心的概念性突破。与RTC[5]在推理时利用复杂的引导算法（伪逆引导）来“修补”动作不同，本文提出直接在训练数据中模拟推理延迟。通过从真实动作块中随机截取一部分作为“动作前缀”，并让模型学习在给定该前缀的条件下预测剩余部分（“动作后缀”），模型在推理时便天然具备了处理延迟的能力。这从根本上移除了推理时的引导计算（见第IV节及算法1）。

2.  **架构与训练流程的轻量级修改。** 该方法实现了“即插即用”的特性，其创新性在于对标准条件流匹配训练流程极简的修改（见第IV节及图2）：
    *   **差异化流匹配时间步：** 允许模型为每个动作时间步分配不同的流匹配时间步 `τ`。对于使用adaLN-zero进行条件化的扩散变换器架构（如π0.6的动作专家模块），这仅需允许尺度、偏移和门控参数在token间不同，且不增加可学习参数量（见第IV节）。
    *   **前缀条件化机制：** 在训练时，对于动作前缀部分，直接使用真实（无噪声）动作，并将其对应的 `τ` 设为1（完全去噪状态）。对于动作后缀部分，则按标准流匹配添加噪声并学习去噪。这相当于在去噪过程中将前缀“固定”为真实值，从而实现对前缀的条件化（见图2）。
    *   **损失掩码：** 仅在模型对动作后缀的预测上计算流匹配损失，忽略前缀部分的输出（见算法1）。

3.  **实证验证了其在延迟与效率上的优势。** 本文通过系统的实验证明了该方法的实用价值：
    *   **模拟实验（第V.A节，图3）：** 在动态Kinetix基准测试中，当推理延迟 `d ≥ 2` 时，训练时RTC的性能显著优于推理时RTC，且差距随延迟增大而扩大。这验证了其在处理高延迟场景下的鲁棒性优势。
    *   **真实世界实验（第V.B节，图5）：** 在π0.6 VLA模型上微调后，应用于复杂的纸盒搭建和意式浓缩咖啡制作任务。结果显示，训练时RTC在保持与推理时RTC相同任务成功率和执行速度的同时，将端到端推理延迟从135ms降低至108ms。这直接证实了其消除计算开销、提升效率的核心主张。

**4. 方法概述**
本文方法建立在RTC[5]的问题表述之上：策略预测一个长度为 `H` 的动作块 `A_t`，并每 `s` 步执行一次（滚动执行）。定义推理延迟为 `d` 个控制步长。核心思想是训练一个条件策略 `p(A_{t+d:H} | o_t, A_{t:t+d})`，即在给定当前观测 `o_t` 和来自上一动作块的前 `d` 个真实动作（前缀）的条件下，预测当前块中剩余的动作（后缀）。

**具体实现流程如下（结合算法1与图2）：**

1.  **数据准备与延迟采样：** 从训练数据集中取一个完整的真实动作块 `A_t`（长度 `H`）。在每次训练迭代中，从一个预设的分布（如均匀分布 `Unif[0, max_delay]`）中随机采样一个延迟值 `d`。将 `A_t` 的前 `d` 个动作标记为前缀 `A_{t:t+d}`，剩余动作标记为后缀 `A_{t+d:H}`。

2.  **噪声注入与时间步设置：** 对**整个动作块** `A_t` 按标准流匹配添加噪声：`A_t^τ = τ * A_t + (1-τ) * ϵ`，其中 `ϵ ~ N(0, I)`，`τ` 在(0,1]间均匀采样。**关键修改在于**：为每个时间步构造一个时间步向量 `time`。对于前缀对应的索引，将 `time` 强制设为1.0；对于后缀索引，则使用采样到的 `τ`。即 `time[i] = 1.0 if i < d else τ`。

3.  **模型前向传播：** 将观测 `o_t`、带噪声的整个动作块 `A_t^τ` 以及上述构造的差异化 `time` 向量输入模型 `v_θ`。模型需要基于不同的 `time` 条件为每个时间步预测去噪方向 `v_θ`。由于前缀的 `time=1.0`，模型被隐式地告知这部分输入已是“干净”的真实数据。

4.  **损失计算：** 计算模型预测 `v_θ` 与流匹配目标 `(ϵ - A_t)` 之间的均方误差。**关键修改在于**：使用一个掩码 `postfix_mask`，该掩码仅在后缀位置为1，在前缀位置为0。最终损失是掩码后误差的归一化和。这意味着梯度仅通过后缀部分的预测回传，模型学习的是“在给定干净前缀的条件下，如何为噪声后缀去噪”。

5.  **推理过程：** 在推理时，给定一个来自上一块的真实动作前缀 `A_{t:t+d}` 和当前延迟 `d`，从标准高斯噪声初始化整个动作块。在每一步去噪迭代中，将前缀部分的值直接“粘贴”到对应位置（`x_t = jnp.where(prefix_mask, action_prefix, x_t)`），并为前缀设置 `time=1.0`。模型基于此条件对后缀进行去噪。经过 `num_steps` 步后，即得到当前块的动作预测（见算法1 `sample_actions` 函数）。

该方法的核心在于，通过训练时对延迟的随机模拟和精巧的差异化条件化，使模型内化了处理异步执行和动作块间过渡的能力，从而在推理时无需任何额外计算即可实现平滑衔接。

**5. 实验说明**
*   **评估指标：**
    *   **模拟实验：** 二元成功率（Solve Rate），即在给定环境下成功完成任务的轨迹比例。
    *   **真实世界实验：** 任务成功率（Success Rate）和任务完成总时长（Duration）。
*   **数据集：**
    *   **模拟实验：** 使用动态Kinetix基准测试[15]的数据，该数据由混合专家策略生成。
    *   **真实世界实验：** 基于π0.6模型[24]及其任务特定数据，聚焦于两个任务：纸盒搭建（Box Building）和意式浓缩咖啡制作（Espresso Making），任务细节来自π*

---

## 3. Real-Time Execution of Action Chunking Flow Policies

### 基本信息
- **作者**: Kevin Black, Manuel Y. Galliker, Sergey Levine
- **arXiv ID**: [oai:arXiv.org:2506.07339v2](https://arxiv.org/abs/2506.07339)
- **发布日期**: Mon, 08 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2506.07339)

            ### 原文摘要
            arXiv:2506.07339v2 Announce Type: replace-cross  Abstract: Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, "freezing" actions guaranteed to execute and "inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.


            
### AI分析（基于论文正文）
好的，作为一名熟悉顶级机器学习/人工智能会议风格的资深论文总结者，我将根据您提供的论文《Real-Time Execution of Action Chunking Flow Policies》和严格的格式要求，生成一份详实、客观的论文总结。

***

### **论文概要**

本文旨在解决基于扩散或流匹配的大规模视觉-语言-动作模型在实时机器人控制中面临的高延迟问题。现有动作分块策略虽能保证时间一致性，但同步推理模式会在分块边界处引入停顿或动作不连续。为此，论文提出了一种名为“实时分块”的推理时算法。该方法将异步动作分块构建为一个修复问题，在并行执行当前动作块的同时，生成与已执行动作兼容的下一动作块。通过在Kinetix模拟器中的12个动态任务和真实世界的6个复杂双手操作任务上进行评估，结果表明该方法能显著提升任务吞吐量，并在存在显著推理延迟时保持高性能。

### **研究动机**

现代大规模AI模型，特别是用于物理世界交互的视觉-语言-动作模型，通常具有数十亿参数，导致单次推理延迟（δ）远高于机器人控制所需的采样周期（Δt）（见第2节）。例如，论文指出，在RTX 4090 GPU上，30亿参数的π0 VLA仅KV缓存预填充就需要46ms，而其控制频率目标为50Hz（Δt=20ms）。在远程推理场景下，网络延迟会进一步加剧此问题。

动作分块策略（输出并执行一个包含多个未来动作的序列）是应对高延迟、保证动作时间一致性的常用方法（见第1节）。然而，标准的**同步推理**策略（在完成当前块的执行后，等待并生成下一个块）在δ > Δt时，会在分块间引入可见的停顿，这不仅降低了执行速度，还改变了机器人动力学，引入了训练与评估间的分布偏移（见第2节）。

一种直接的改进是**异步推理**，即提前启动下一个块的推理，使其与当前块的执行并行。但这带来了新的挑战：当新块可用时，旧块已部分执行。由于生成新块时无法预知执行期间的环境变化，新旧块在交接点（transition point）可能产生严重的不连续或“模式跳跃”行为，导致机器人动作突变（见图2）。现有平滑方法（如时间集成）对多模态动作分布进行平均，可能产生无效动作，反而恶化性能（见第1节及图2）。

因此，论文的核心动机是设计一个**实时、异步**的执行框架，该框架必须：1) 满足实时约束（始终在需要时提供动作）；2) 保证分块间的**连续性**，避免动作突变；3) 能够**整合最新观测**，保持系统的反应能力。现有工作在解决延迟与连续性、反应性的矛盾方面存在不足，这构成了本研究的出发点。

### **核心贡献与创新点**

1.  **提出了“实时分块”推理时算法**：这是论文最核心的贡献。RTC首次将异步动作分块的连续性保证问题形式化为一个**修复问题**（见第3节）。其核心创新在于：在生成新动作块时，将那些因推理延迟而**必定会被执行**的旧块尾部动作“冻结”为目标值，然后基于流匹配/扩散模型的修复能力，生成与这些冻结动作兼容的剩余动作。这种方法概念新颖，将计算机视觉中的修复思想创造性地应用于实时序列决策控制。

2.  **引入了“软掩码”机制以增强分块间连续性**：论文发现，仅修复与旧块重叠的前`d`个动作（硬掩码）在延迟较小时，提供的引导信号可能过弱，仍可能导致策略切换和不连续（见图4）。为此，作者提出了**软掩码**（见第3.2节，公式5）。该机制不仅冻结前`d`个动作（权重为1），还对旧块中所有重叠但尚未执行的动作（共`H-s`个）赋予一个从1到0**指数衰减**的权重。这使模型在生成新块时，能“关注”旧块更长期的策略，从而获得更好的跨分块连续性，这是对基础修复方法的关键改进。

3.  **设计并开源了一个用于评估实时性能的高动态模拟基准**：作者指出，现有的模仿学习基准多为准静态任务，标准分块执行已能取得近乎完美的成功率，无法有效评估实时方法的优劣（见第4.1节）。因此，他们基于Kinetix模拟器创建了一个包含**12个高动态、随机性强的操作与运动任务**的新基准（见图5左上），涉及投掷、接取、平衡等，并添加了动作噪声以强调闭环修正的重要性。这个基准为未来相关研究提供了重要的评估工具。

4.  **在真实世界复杂任务上进行了系统性的实证验证**：论文不仅进行了模拟实验，还使用π0.5 VLA在真实的双手机器人上评估了RTC在**6项挑战性操作任务**上的表现，包括点燃蜡烛、插网线、叠衣服等（见第4.2节）。实验涵盖了不同注入延迟（+0ms, +100ms, +200ms）的场景，并引入了**任务吞吐量**作为综合衡量速度与性能的指标。结果表明，RTC在存在显著延迟时，性能几乎不下降，且在所有延迟水平下均取得了最佳的平均吞吐量（见图6右），验证了其在实际系统中的有效性与鲁棒性。

### **方法概述**

RTC是一个纯推理时的算法，适用于任何基于流匹配或扩散的VLA。其系统架构如算法1及图3所示，核心是一个运行在后台线程的推理循环（`INFERENCELOOP`）和一个每Δt被调用一次的前台动作获取函数（`GETACTION`）。

**核心流程**：
1.  **状态共享与触发**：共享变量（当前动作块`A_cur`、最新观测`o_cur`等）由互斥锁保护。当控制器调用`GETACTION`提供新观测`o_next`时，会通知后台推理线程。
2.  **推理启动时机**：后台线程等待直到已执行的动作数`t`达到最小执行视界`s_min`，然后开始生成下一个块。实际执行视界`s`被设定为`max(估计的延迟d, s_min)`，以确保实时性（见算法1第14行）。
3.  **引导修复生成**：新块通过`GUIDEDINFERENCE`函数生成。该函数是方法的技术核心：
    *   **输入**：策略π、当前观测`o`、旧块剩余动作`A_prev`（长度为`H-s`）、延迟`d`、执行视界`s`。
    *   **软掩码构建**：根据公式5计算权重向量`W`。`W`的长度为`H`，其中前`d`维为1，最后`s`维为0，中间`H-d-s`维按指数从1衰减至0。
    *   **迭代去噪与引导**：从噪声`A^0`开始，进行`n`步迭代去噪（对应流匹配从τ=0积分到1）。在每一步τ，执行以下操作（对应算法1第25-29行）：
        a.  **计算去噪估计**：根据公式3，`c(A^1) = A^τ + (1-τ) * v_π(A^τ, o, τ)`，这是对最终去噪动作块`A^1`的估计。
        b.  **计算加权误差**：`e = (A_prev - c(A^1))^T * diag(W)`。这里`A_prev`被右填充至长度`H`以匹配维度。误差衡量了当前去噪估计与目标（旧块剩余动作）在掩码加权下的差异。
        c.  **计算引导梯度**：`g = e · ∂c(A^1)/∂A^τ`，这是一个向量-雅可比积，可通过自动微分高效计算（见第3.1节）。
        d.  **更新动作块**：根据公式1和2，`A^{τ+1/n} = A^τ + 1/n * [ v_π(A^τ, o, τ) + min(β, (1-τ)/(τ·r_τ^2)) * g ]`。其中，`min(β, ...)`项是作者引入的**引导权重裁剪**，用于在去噪步数较少时稳定训练（见第3.1节及附录A.2）。
4.  **块切换**：新块`A_new`生成后，立即替换共享变量中的`A_cur`，并重置已执行动作计数器`t`，使其指向新块的开始。

整个方法通过“冻结-修复”机制，确保了新旧块在重叠区域的强连续性；通过软掩码，利用了旧块更长期的策略信息；通过后台异步推理，满足了实时性要求。

### **实验说明**

**评估指标**：
*   **模拟实验**：二元成功率（Solve Rate），每个数据点基于2048次试验计算，并给出95% Wilson分数区间。
*   **真实世界实验**：
    *   **任务分数**：整数分，对应成功完成的子步骤数量。
    *   **累积进度图**：展示控制器步数（

---

## 4. SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control

### 基本信息
- **作者**: Zhengyi Luo, Ye Yuan, Tingwu Wang, Chenran Li, Sirui Chen, Fernando Casta\~neda, Zi-Ang Cao, Jiefeng Li, David Minor, Qingwei Ben, Xingye Da, Runyu Ding, Cyrus Hogg, Lina Song, Edy Lim, Eugene Jeong, Tairan He, Haoru Xue, Wenli Xiao, Zi Wang, Simon Yuen, Jan Kautz, Yan Chang, Umar Iqbal, Linxi "Jim" Fan, Yuke Zhu
- **arXiv ID**: [oai:arXiv.org:2511.07820v2](https://arxiv.org/abs/2511.07820)
- **发布日期**: Mon, 08 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI, cs.CV, cs.GR, cs.SY, eess.SY
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2511.07820)

            ### 原文摘要
            arXiv:2511.07820v2 Announce Type: replace-cross  Abstract: Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited set of behaviors, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leveraging dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.


            
### AI分析（基于论文正文）
好的，作为一名资深论文总结者，我将为您提供一篇关于《SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control》的详细、结构化的总结。

***

### **论文概要**

本文提出SONIC框架，旨在解决人形机器人控制领域缺乏规模化基础模型的问题。作者认为，运动跟踪是一个具有可扩展性的基础任务，能够利用大规模运动捕捉数据提供密集监督，从而学习到通用的人体运动先验。通过沿三个维度（模型容量、数据量和计算量）进行规模化，SONIC训练出一个能够生成自然、鲁棒全身运动的通用控制器。该框架的核心创新在于引入了一个实时的运动学规划器和一个支持多模态输入的统一令牌空间，从而将规模化运动跟踪的能力桥接到下游的交互控制、遥操作、多模态控制等实际应用中，并展示了与视觉-语言-动作基础模型的无缝集成。

### **研究动机**

当前，尽管在自然语言处理和图像生成等领域，通过扩大模型规模、数据量和计算量来构建基础模型已成为主流范式，并展现出涌现能力和泛化性，但这一范式在人形机器人控制领域尚未实现类似的规模化收益（见第1节）。现有的人形机器人控制策略通常规模较小（如仅数百万参数的多层感知机），针对有限的行为集进行训练，且依赖于大量人工设计的奖励函数。这种为每个任务单独设计奖励的方式不仅繁琐，还可能导致训练不稳定甚至性能下降（Peng et al., 2018, 2021）。这构成了人形控制规模化发展的根本障碍。

此外，一个理想的人形控制器需要支持多样化的现实应用，如遥操作、目标导向任务、导航以及视觉-语言指令等。然而，构建一个既能规模化训练，又能灵活适应不同任务规范的统一系统极具挑战性（见第1节）。现有基于运动跟踪的研究（如Chen et al., 2025; Liao et al., 2025）大多局限于在训练数据上展示跟踪效果，未能充分证明其在众多下游任务中的通用性。

因此，本文的研究动机是双重的：1）**寻找一个可规模化的基础任务**，以摆脱对人工奖励工程的依赖，并利用大规模数据学习通用运动技能；2）**构建一个实用的系统框架**，使得学习到的通用控制器能够灵活、便捷地应用于各种现实场景和交互模式。作者提出将运动跟踪作为这一基础任务，因为它能直接利用现有的大规模人体运动捕捉数据（如AMASS），提供无需奖励的逐帧密集监督。

### **核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下三个方面，每一项都超越了现有工作的局限：

1.  **首次系统性地论证并实现了人形控制任务的规模化，确立了大规模运动跟踪作为人形基础任务的可行性。** 与现有工作（如Any2Track, BeyondMimic, GMT）通常在数百万帧数据、单GPU上训练不同，SONIC将训练规模提升至前所未有的水平：使用超过1亿帧（700小时）的高质量运动数据，在128块GPU上进行了总计9,000 GPU小时的训练，模型参数规模从120万扩展到4200万（见第2.1节，图2）。实验结果表明，沿数据、模型、计算三个维度的扩展均能带来运动模仿性能的持续、稳定提升（图2a-c），这为人形控制领域提供了明确的规模化定律证据。更重要的是，由此训练出的单一策略在未见过的、大规模运动序列上（如AMASS测试集）实现了接近100%的成功率和极低的跟踪误差，显著优于所有基线方法（图2d-g），证明了规模化带来的强大泛化能力。

2.  **设计并实现了一个统一的令牌空间，支持多模态、跨具身的控制接口。** 这是本文的关键概念创新。如图8所示，SONIC构建了一个共享的潜在命令空间（即“统一令牌空间”），将来自不同来源和模态的输入（如VR全身姿态、VR三点命令、人体视频估计姿态、文本/音乐生成的运动、VLA模型输出命令）通过各自的编码器映射到同一空间。随后，一个统一的机器人控制解码器将这些令牌解码为低层的关节控制信号（见第3.2节）。这一设计使得**同一个训练好的跟踪策略**能够直接处理所有上述输入，无需为每种模态重新训练或进行蒸馏，实现了“一次训练，多处使用”。这解决了现有系统通常为不同输入模式设计独立策略或需要复杂集成的问题。

3.  **开发了一个实时的运动学生成规划器，实现了自然、交互式的全身控制。** 为了将通用的运动跟踪能力转化为实用的交互控制，本文引入了一个基于自回归框架的实时运动学规划器（见第3.3节）。该规划器以前一状态和用户命令（如目标速度、方向、风格）为条件，持续生成未来0.8至2.4秒的运动片段。其推理速度极快（标准笔记本上<5ms），并支持高频重规划（每100ms或命令更新时）。这个规划器充当了高层用户意图与底层跟踪策略之间的桥梁，使得机器人能够执行复杂的交互任务，如任意速度和方向的导航、不同风格的行走、拳击组合动作等（见第2.2节，图3）。与依赖有限动作片段库切换的现有方案（Starke et al., 2021; Unitree, 2025）相比，SONIC能生成连续、流畅且自然的过渡运动。

### **方法概述**

SONIC的整体框架如图8所示，其核心是一个统一的控制策略，由编码器-解码器架构构成，并辅以一个前端的运动学规划器。

**1. 统一控制策略的架构与训练：**
策略的核心是一个**Transformer解码器**。其输入包括：
*   **历史观测令牌**：过去若干帧的机器人本体感觉信息（关节位置、速度等）经过MLP编码后的序列。
*   **命令令牌**：来自“统一令牌空间”的输入。这是方法的关键：不同类型的输入（`I_cmd`）通过各自的**编码器**（`E_cmd`）被投影到同一个`d`维的潜在空间，生成命令令牌。例如，VR全身姿态有对应的编码器，文本生成的运动有另一个编码器，但它们输出的令牌维度相同。
*   **历史动作令牌**：过去执行的动作序列。

这些令牌被拼接后输入给Transformer解码器，其输出经过一个MLP头，预测当前时刻的关节位置目标（PD控制的目标），进而生成扭矩（见第3.2节，公式1）。整个策略通过**行为克隆**进行训练，损失函数是预测动作与专家动作（来自重定向后运动数据）之间的均方误差。训练数据是超大规模的（100M帧），涵盖了极其多样的人类行为。

**2. 统一令牌空间与多模态编码器：**
这是实现多模态控制的核心机制。对于每种输入模态`m`，都有一个对应的编码器`E_m`。例如：
*   **VR全身编码器**：将SMPL格式的人体姿态序列编码为令牌。
*   **混合编码器**：用于处理VR三点遥操作命令（头、双手腕的SE(3)姿态等）和VLA输出。它将这些稀疏的上半身命令与一个可学习的“风格嵌入”结合，生成令牌（见第3.2节）。
*   **运动令牌化器**：对于来自视频、文本或音乐生成模型（如GENMO）的完整人体运动序列，使用一个预训练的VQ-VAE将其离散化为运动令牌，这些令牌可直接作为命令令牌输入策略。
所有编码器与策略主体进行**端到端联合训练**，确保不同模态的语义在令牌空间中对齐。

**3. 实时运动学规划器：**
这是一个独立于跟踪策略的、轻量级的自回归生成模型（见第3.3节）。它以机器人的当前根状态和用户命令为条件，在运动空间（如关节角轨迹）中生成短期（0.8-2.4秒）的未来运动片段`M_{1:K}`。规划器本身在**与策略相同的大规模运动数据集**上进行训练，学习人体运动的分布和动态。生成的参考运动`M_{1:K}`随后被送入跟踪策略（通过运动令牌化器或直接作为目标姿态），由策略在物理仿真中稳健地执行。这种“规划器+跟踪器”的分层设计，既保证了高层交互的灵活性和创造性，又依赖底层策略的物理鲁棒性。

### **实验说明**

**评估指标：**
1.  **成功率**：跟踪过程中，若人形机器人根高度与参考运动偏差超过0.25m或根朝向偏差超过1弧度，则视为失败。
2.  **MPJPE**：根关节相对的平均每关节位置误差（毫米），衡量局部姿态精度。
3.  **速度误差**：与参考运动之间的速度差异（毫米/帧）。
4.  **加速度误差**：与参考运动之间的加速度差异（毫米/帧²）。

**数据集：**
*   **训练集**：内部收集的大规模运动捕捉数据集，包含170名受试者，总计超过1亿帧（700小时）的多样化人类行为数据（见第3.1节）。
*   **测试集**：从AMASS数据集中随机选取的重定向后数据，包含1,602条轨迹（9小时），用于评估泛化能力。**SONIC未在AMASS上训练**。

**对比基线方法：**
1. 

---

## 5. BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language

### 基本信息
- **作者**: Nitin Priyadarshini Shankar, Vaibhav Singh, Sheetal Kalyani, Christian Maciocco
- **arXiv ID**: [oai:arXiv.org:2512.05721v1](https://arxiv.org/abs/2512.05721)
- **发布日期**: Mon, 08 Dec 2025 00:00:00 -0500
- **分类**: cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.05721)

            ### 原文摘要
            arXiv:2512.05721v1 Announce Type: new  Abstract: We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\times$ variation in service quality, making it well suited for intelligent RAN deployments.


            
### AI分析（基于论文正文）
好的，作为一名资深论文总结者，我将为您提供一份关于论文《BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language》的详细总结。

***

### **论文概要**
本文提出了一种名为BERTO的自适应时间序列预测框架，旨在解决蜂窝网络中流量预测与能源优化的联合问题。该框架基于预训练的BERT模型，通过微调使其适用于网络负载的短期预测。BERTO的核心创新在于能够通过自然语言提示（如“Focus on power savings”）来理解并响应运营商对节能与服务质量之间权衡的偏好。该方法利用非对称的平衡损失函数，将不同的操作意图映射为不同的损失权重，从而使单一模型能够自适应地调整其预测行为。实验在真实数据集上进行，结果表明BERTO在预测精度上优于基线模型，并能在一个宽泛的功率节省（-731.88W至714.76W）和服务质量损失（0.026%至0.252%）范围内灵活运作。

### **研究动机**
现代蜂窝网络（如5G及未来网络）的动态性和复杂性日益增加，对网络关键性能指标（KPI）进行准确预测是实现智能决策（如负载均衡、异常检测和节能）的基础。传统的时间序列预测方法，如ARIMA和指数平滑，以及早期的深度学习方法如RNN和LSTM，在处理长程依赖和整合上下文信息方面存在局限。近年来，基于Transformer的模型（如Informer、Autoformer）因其强大的序列建模能力成为有力替代方案。然而，现有研究（如[5], [6], [7], [8]）主要聚焦于最小化预测误差，未能将运营商在节能和服务质量这两个相互竞争的目标之间的偏好纳入考量（见第I节）。

随着大语言模型（LLMs）的发展，为时间序列预测带来了新的范式。LLMs不仅能进行强大的时序建模，还具备处理多模态输入（如文本）的能力。尽管已有工作（如[9], [10]）探索将LLMs用于时间序列预测，但它们主要关注如何将时序数据编码为文本提示以进行预测。本文作者识别到一个关键的研究缺口：能否利用LLMs处理文本提示的能力，使模型能够根据当前需求（例如，侧重于节能或侧重于服务质量）进行自适应预测？具体而言，作者希望回答的问题是：能否通过自然语言提示，使模型关注特定方面，从而变得更具适应性（见第I节末尾）。这一动机旨在将预测模型从一个静态的误差最小化工具，转变为一个能够理解并执行高层操作意图的动态智能体。

### **核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面，它们共同构成了一个从静态预测到意图驱动自适应预测的完整框架：

1.  **针对网络时间序列预测的BERT微调架构：** 本文并非简单地将BERT应用于时间序列，而是设计了一套完整的微调方案。其创新性在于引入了**提示生成器**和**时间序列预测头**这两个任务特定组件（见第III-B节）。提示生成器将过去负载、时间、小区编号、平均使用量和使用偏差等关键特征（见图3）结构化为包含`[MASK]`令牌的文本提示，将预测任务转化为类似BERT预训练中的掩码语言建模任务。时间序列预测头则是一个定制的回归头，它接收BERT输出的上下文嵌入，通过2D平均池化降维，再经过一系列全连接层输出预测值（见图4）。这种设计使得预训练的语言模型能够有效地迁移到数值型时间序列预测任务上，实验结果表明，仅使用MSE损失的该模型（BERTMSE）在参数规模相近或更小的情况下，取得了优于Autoformer、Chronos等先进模型以及传统LSTM、ARIMA的预测精度（见表I）。

2.  **通过自然语言提示实现模型自适应：** 这是本文最核心的概念性创新。作者提出，将运营商的偏好（如“Focus on power savings”或“Focus highly on service quality”）以自然语言的形式**附加到输入提示中**（见图3）。在模型微调阶段，不同的提示被映射到**平衡损失函数**中不同的不对称参数`q`（见表III）。当`q > 1`时，损失函数对低估（underprediction）惩罚更重，促使模型倾向于高估负载，这有利于节能（因为高估负载会减少不必要的关断，保障服务）；反之，当`q < 1`时，对高估（overprediction）惩罚更重，模型倾向于低估负载，从而优先保障服务质量（避免因低估而导致过载）。这一机制使得**单一模型**能够根据输入提示动态调整其预测偏差，而无需为每个目标重新训练或调整模型参数（见第III-D节）。这超越了传统多目标优化或集成模型的方法，提供了一种更灵活、更符合人类操作习惯的交互方式。

3.  **将自适应预测与网络节能决策闭环集成：** 本文不仅提出了预测模型，还将其置于一个完整的网络节能应用场景中。预测出的负载被直接用于一个**小区开关方案**的决策（见第III-E节）。该方案根据预测负载是否低于阈值，决定是否关闭共址小区对中的一个高频小区以节省能耗。通过结合功率模型和吞吐量损失模型（见第III-F节），作者能够量化评估不同操作偏好提示下，BERTO模型在真实节能决策中带来的**功率节省**和**服务质量损失**的权衡结果（见表III）。这种端到端的评估方式，清晰地展示了自适应预测在实际网络优化中的价值和灵活性。

### **方法概述**
BERTO方法的实现流程可分为数据预处理、模型架构、训练与自适应机制三个主要部分，其运作细节如下：

**1. 数据预处理与提示生成：**
对于每个小区`i`在时间`t`的负载预测任务，模型输入为历史窗口`x_{t-h:t-1}`以及统计特征（均值`μ_x_t`和标准差`σ_x_t`）。**提示生成器**（图3）将这些数值特征与元数据（如小区编号、时间）组合，构造成一个结构化的文本字符串。例如：“Cell Number: 5, Time of Day: 14:30, Past Usage: [values], Average Usage: 75.2, Usage Deviation: 12.1, Current Traffic: [MASK]”。其中，待预测的当前值被`[MASK]`令牌替换。运营商的自然语言偏好（如“Focus on power savings”）被附加在这个字符串的末尾，共同构成完整的输入提示。

**2. 模型架构与预测流程：**
模型基于一个4层编码器、隐藏层维度为256的BERT-mini架构进行微调（见第III节开头）。其前向传播流程如下（图4）：
- **令牌化：** 完整提示文本通过BERT分词器转换为令牌ID序列和注意力掩码。
- **上下文编码：** 令牌ID和注意力掩码输入预训练的BERT编码器，获取最后一层隐藏状态的上下文嵌入。
- **特征提取与回归：** 在BERT输出之上，作者设计了一个**时间序列预测头**。首先，对编码后的特征进行2D平均池化（核大小3x3，步长3）以降低维度并聚合关键信息。随后，池化特征通过三个全连接层：第一层将维度映射到512，第二层降至64，最后一层输出一个标量值，即预测的负载`\hat{x}_t`（见第III-B.2节）。

**3. 训练与自适应机制：**
模型使用**平衡损失函数**进行微调。BLF的定义如公式(4)所示：`BLF = max( q*(y-\hat{y})/(q+1), (\hat{y}-y)/(q+1) )`。参数`q`控制损失的不对称性。
- **基础训练：** 首先，使用标准MSE损失对模型进行微调，得到基础模型BERTMSE。
- **自适应微调：** 为了实现自适应，作者准备多组训练数据，每组数据中的提示附加了不同的操作偏好文本（如“Focus on power savings”）。在微调时，**不同的偏好文本与BLF中特定的`q`值进行绑定**（见表III的映射关系）。例如，当提示包含“Focus highly on power savings”时，使用`q=10`的BLF进行损失计算；当提示包含“Focus on service quality”时，使用`q=0.5`。通过这种方式，模型在训练过程中学会了将特定的自然语言指令与相应的预测偏差行为（由`q`值控制）关联起来。在推理阶段，只需在输入提示中更改操作偏好语句，模型就会自动产生符合该偏好的预测结果，整个过程无需手动设置`q`值或更改模型参数。

### **实验说明**
**评估指标：**
1.  **预测精度：** 均方误差（MSE），用于比较各预测模型的纯数值精度。
2.  **应用性能：** 总节电量（Watt）、平均吞吐量损失（%），用于评估预测模型在小区开关节能方案中的实际效果。
3.  **推理延迟：** 单样本推理时间（ms），用于评估模型部署的实时性。

**数据集：**
- **Telecom Italia Dataset (米兰数据集)：** 一个开源的呼叫详细记录数据集。本文使用其互联网活动数据作为网络流量负载。数据粒度：10分钟/样本，

---

## 6. Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment

### 基本信息
- **作者**: Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu, Yinxinyu Chen, Encheng Gu, Ziyan Liu, Hongyi Cai, Yanwen Zou, Lixing Zou, Zhaoye Zhou, Gen Li, Bo Zhao
- **arXiv ID**: [oai:arXiv.org:2511.04555v2](https://arxiv.org/abs/2511.04555)
- **发布日期**: Mon, 08 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2511.04555)

            ### 原文摘要
            arXiv:2511.04555v2 Announce Type: replace  Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment》，生成一份符合顶级会议风格的详细总结。

***

### **论文总结：Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment**

#### **1. 论文概要**
本文提出Evo-1，一个轻量级的视觉-语言-动作模型，旨在解决现有VLA模型参数量大、计算成本高、依赖大规模机器人数据预训练以及端到端训练导致视觉-语言主干网络语义表征退化的问题。Evo-1采用一个紧凑的原生多模态视觉-语言模型作为主干，结合一个新颖的交叉调制扩散变换器和一个优化的集成模块。作者进一步引入一个两阶段训练范式，在渐进对齐动作与感知的同时，有效保留了主干网络的语义表征能力。实验表明，仅0.77B参数的Evo-1在多个仿真和真实世界机器人任务中达到了最先进的性能，且无需机器人数据预训练，同时具备高推理频率和低内存开销。

#### **2. 研究动机**
现有的大规模视觉-语言-动作模型在实现通用机器人控制方面展现出潜力，但面临四个关键限制（见第1节）。首先，其参数量通常高达数十亿，导致训练和推理时GPU内存占用巨大、计算成本高昂。其次，巨大的计算开销导致控制频率低下，限制了模型在交互式机器人任务中的实时响应能力。第三，广泛采用的端到端训练范式往往会破坏视觉-语言主干网络预训练的表征空间，导致模型在下游任务中泛化能力差和过拟合。第四，大多数模型严重依赖在大规模机器人数据集上的长时间预训练，而这些数据的收集成本高昂且费力。

尽管已有工作如TinyVLA和SmolVLA致力于构建轻量级VLA模型以提升效率，但其整体任务性能和鲁棒性在复杂操作场景中仍有不足（见第2节）。因此，本文的研究动机在于开发一个**既轻量高效，又能保持强大泛化能力，且无需依赖大规模机器人数据预训练**的VLA模型。作者旨在通过创新的架构设计和训练策略，在模型规模、计算效率、性能以及泛化能力之间取得更优的平衡。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面：

1.  **轻量高效的架构设计**：Evo-1提出了一种总参数量仅为0.77B的紧凑VLA架构。其轻量化主要源于：（a）采用InternVL3-1B作为视觉-语言主干，该模型通过原生多模态范式预训练，实现了视觉与语言的紧密对齐，避免了后融合带来的冗余（见第3.2.1节）。（b）设计了一个**交叉调制扩散变换器**作为动作专家。与先前VLA模型（如π0、SmolVLA）采用的交替自注意力和交叉注意力结构不同，Evo-1的动作专家完全由堆叠的交叉注意力层构成（见第3.2.2节）。这种设计简化了模型结构，减少了参数量，同时通过将噪声注入的动作序列作为查询，将融合的多模态表征和机器人状态作为键值，实现了高效的条件动作生成。

2.  **语义保持的两阶段训练范式**：为了在适应下游动作生成的同时，**最大限度地保留视觉-语言主干网络固有的多模态理解能力**，本文提出了一个两阶段训练范式（见第3.3节）。**第一阶段（动作专家对齐）**：冻结整个视觉-语言主干，仅训练集成模块和动作专家。这允许随机初始化的动作模块在不向主干传播噪声梯度的情况下，与多模态嵌入空间逐渐对齐。**第二阶段（全规模微调）**：在动作模块充分对齐后，解冻主干并进行全模型微调。这种渐进式策略有效防止了端到端训练对预训练语义空间的破坏。如图2和图7所示，经过两阶段训练后，Evo-1的注意力图保持了清晰的空间一致性和语义对齐，而对比模型则出现了语义漂移。

3.  **无需机器人数据预训练的卓越性能**：通过上述架构和训练创新，Evo-1在完全**不依赖任何大规模机器人数据预训练**的情况下，在多个具有挑战性的基准测试中达到了最先进的性能（见第4节）。这显著降低了对昂贵、劳动密集型数据收集的依赖，为高效VLA模型的开发提供了新路径。

#### **4. 方法概述**
Evo-1的方法是一个模块化的感知-推理-控制管道，其整体映射函数为 `at = f_Evo-1({I_i_t}, L_t, s_t; θ)`（公式1）。其技术方案具体如下：

*   **视觉-语言主干**：采用InternVL3-1B模型。视觉编码器使用轻量化的InternViT-300M，并对输入图像进行像素重排下采样以减少视觉令牌数量。语言分支使用Qwen2.5-0.5B。通过将图像块嵌入替换特定的 `<img>` 占位符令牌，在共享的Transformer解码器中进行视觉-语言融合。最终，从主干第14层提取融合的多模态表征 `z_t`（公式2），该层被经验证明具有更强的跨模态对齐特性（见第3.2.1节）。

*   **交叉调制扩散变换器（动作专家）**：基于流匹配范式，学习一个时间依赖的速度场，将噪声动作逐步推向真实动作。具体而言，通过线性插值在真实动作序列 `A_t` 和随机噪声 `ϵ` 之间生成噪声动作序列 `A^τ_t`（公式3）。动作专家（一个扩散变换器DiT）的目标是预测一个速度场 `v_θ`，以最小化其与目标流方向之间的误差（公式4）。在推理时，模型以融合表征 `z_t`、机器人状态 `s_t` 和插值动作 `A^τ_t` 为条件，生成未来H步的动作序列 `Â_t`（公式5）。**关键创新**在于其层结构：所有Transformer块均采用**交叉注意力**，其中查询是噪声动作 `A^τ_t`，键和值则来自集成模块提供的多模态上下文。

*   **集成模块**：该模块负责将多模态表征 `z_t` 与机器人本体感知状态 `s_t` 对齐，以供动作专家使用。Evo-1采用了**模块A**的设计（见第4.4.1节及图6a）：简单地将 `z_t` 与 `s_t` 拼接，并将拼接后的特征作为**所有**DiT层中交叉注意力块的键值输入。这种设计确保了多模态信息在动作生成过程中的一致传播，被证明优于引入自注意力层或分层注入特征的其他变体（图8a）。

*   **两阶段训练流程**：如第3.3节所述，训练分为两个阶段。第一阶段冻结主干，仅训练集成模块和动作专家，建立初步的感知-动作对齐。第二阶段解冻主干，进行全模型微调，实现更深层次的整合。这种策略旨在保护主干网络的语义空间免受破坏性更新的影响，从而提升模型的泛化能力。

#### **5. 实验说明**
*   **评估指标**：主要评估指标为任务成功率（%）。同时，在真实世界实验中评估了推理频率（Hz）和GPU内存消耗（GB）。
*   **数据集**：
    *   **仿真基准**：
        *   **Meta-World**：包含多种难度的单臂操作任务，分为简单、中等、困难、极难四个等级。
        *   **LIBERO**：包含40个任务，分为空间、物体、目标、长程四类，用于评估操作和推理能力。
        *   **RoboTwin**：用于评估双臂操作能力，包含“点击闹钟”、“倾倒垃圾箱”等任务，各有简单和困难两种难度。
    *   **真实世界数据**：使用6自由度xArm6机械臂收集了四个任务（拾放易拉罐、倾倒泡沫杯、手递手交付、易拉罐堆叠）的100条遥操作演示数据，用于训练和评估。
*   **对比基线方法**：
    *   **非VLA策略**：Diffusion Policy, ACT。
    *   **大规模VLA模型**：OpenVLA, π0, CoT-VLA, π0-FAST, GR00T N1。
    *   **轻量级VLA模型**：TinyVLA, SmolVLA。
    *   **其他**：RDT（双臂操作扩散基础模型）。
*   **实验条件**：论文中未明确说明训练和微调所使用的具体GPU数量及配置。**推理效率分析**（表2）是在一块RTX 4090 GPU上进行的，报告了平均内存使用量和推理频率。

#### **6. 改进建议和未来研究方向**
*   **已提及及潜在局限性**：
    1.  **任务与场景范围**：实验主要集中在桌面物体操作任务上。虽然涵盖了单臂、双臂和部分泛化场景，但模型在更复杂的非结构化环境（如动态场景、户外环境）、需要复杂物理推理或长期规划的任务中的能力尚未得到验证。
    2.  **动作表示**：当前模型输出低维连续动作向量。对于需要更高自由度或

---

