# arXiv论文监控报告 - 2025年12月05日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2025年12月05日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 7篇

---

## 1. Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach

### 基本信息
- **作者**: Siyuan Yang, Yang Zhang, Haoran He, Ling Pan, Xiu Li, Chenjia Bai, Xuelong Li
- **arXiv ID**: [oai:arXiv.org:2512.02834v1](https://arxiv.org/abs/2512.02834)
- **发布日期**: Thu, 04 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.02834)

            ### 原文摘要
            arXiv:2512.02834v1 Announce Type: cross  Abstract: Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文信息，生成一份符合要求的详细总结。

### **论文总结报告**

**论文标题：** Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach
**作者：** Siyuan Yang, Yang Zhang, Haoran He, Ling Pan, Xiu Li, Chenjia Bai, Xuelong Li
**arXiv ID：** 2512.02834v1

---

#### **1. 论文概要**
本文针对基于流匹配或扩散目标的视觉-语言-动作模型在特定下游任务上进行监督微调后，在推理时表现出的不稳定性问题。作者将这种不稳定性归因于微调后VLA策略的分布与下游任务数据集中稳定成功模式所诱导的理想策略分布之间存在偏移。为解决此问题，论文提出了TACO，一个无需梯度的测试时扩展框架。TACO利用一个轻量级的伪计数估计器作为动作块的验证器，在推理时从多个候选动作中选择伪计数最高的动作执行，从而将策略约束在成功模式的支持集内，实现了反探索原则。实验在四个仿真基准和一个真实世界双机械臂平台上验证了该方法能显著提升多种VLA模型的推理稳定性和任务成功率。

#### **2. 研究动机**
VLA模型通过在大规模多模态数据集上进行预训练，展现出强大的泛化能力。然而，当这些模型在特定下游任务的小规模数据集上进行监督微调后，在推理时会出现严重的不稳定性（见第1节，图1）。例如，仅因采样不同的初始噪声向量，同一模型在相同任务上的成功率可能从0%波动至80%。

作者深入分析了这一问题的根源（见第1节及第4.1节）：
1.  **预训练引入冗余模式：** VLA在预训练阶段吸收了来自多样化数据源的广泛动作模式，使其难以在微调时迅速将输出分布收敛到下游任务所需的、狭窄的成功行为集合上。因此，微调后的策略分布仍保留了大量与任务成功无关的冗余模式。
2.  **微调数据集本身的多模态性：** 下游任务的微调数据集通常由多人遥操作、脚本规划器或不同执行风格收集而来，其中可能包含次优或不理想的策略。这些数据中的冗余模式进一步加剧了分布偏移。

这种分布偏移在基于流匹配或扩散的生成式VLA中尤为明显，因为采样的随机噪声直接影响生成的动作。现有工作（如ATE）尝试在训练时通过潜在对齐或分类器引导来解决分布偏移，但忽略了推理时的不稳定性问题。此外，传统的离线强化学习方法虽然提出了反探索原则来约束策略在数据集支持集内，但其基于梯度优化的方式难以直接应用于涉及复杂去噪过程的VLA模型。因此，论文的动机是设计一种无需修改模型参数、计算高效的测试时方法，来缓解VLA在微调后的推理不稳定性，确保其输出始终贴近下游任务的成功模式。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点可归纳为以下三个方面：

1.  **提出了TACO：一个基于反探索原则的测试时扩展框架。** 这是首个将离线RL中的反探索原则系统性地应用于解决VLA模型推理不稳定性问题的工作（见第1、4节）。TACO的关键创新在于其“生成-验证”机制：在推理时，VLA模型并行生成多个候选动作块，随后由一个轻量级验证器（伪计数估计器）选择最可靠的（即伪计数最高的）动作执行。这种方法无需对VLA主干网络进行任何参数更新或梯度计算，完全在测试时运作，保留了预训练模型的泛化能力，同时有效约束了输出。

2.  **设计了一种耦合的、基于VLA内部表征的伪计数估计器。** 为了高效实现反探索，需要估计动作块在微调数据集中的“访问计数”。论文创新性地采用了硬币翻转网络作为伪计数估计器，但其核心设计在于**耦合的估计器**（见第4.2节）。与训练一个独立的编码器不同，TACO假设VLA模型本身能提供最丰富的表征。因此，CFN被实例化为一个轻量级MLP头，以VLA的内部隐藏状态 `h_θ` 作为输入。这种设计高度高效，利用了VLA已有的计算，并受益于其广泛的预训练知识。

3.  **针对去噪式VLA提出了高保真特征搜索方法。** 上述耦合设计对基于扩散或流匹配的VLA带来了挑战，因为这些模型只在带噪声的动作上训练，从未见过干净数据动作。直接输入干净动作 `a` 来提取特征 `h_θ` 可能得不到有意义的表征（见第4.2节）。为此，论文提出了**高保真特征搜索**：对于数据集中的每个 `(o, l, a)`，用 `N` 个不同噪声级别 `{σ_i}` 查询VLA，得到 `N` 个预测动作 `a_pre^(i)` 和对应特征 `h_θ^(i)`。然后选择预测动作最接近真实动作 `a` 的那个特征 `h_θ^(i*)`（公式(8)）。这样得到的特征既在VLA的特征分布内（因为来自带噪输入），又具有高保真度（因为能准确重建真实动作）。这一方法是实现高效、准确伪计数估计的关键技术细节。

#### **4. 方法概述**
TACO方法分为两个阶段：训练阶段（Stage 1）准备伪计数估计器，推理阶段（Stage 2）执行测试时反探索（整体流程见图2）。

**4.1 训练阶段（伪计数估计器训练）**
目标是为下游微调数据集 `D_sft` 中的每个 `(o, l, a)` 数据对，获取一个能用于估计伪计数的高保真VLA内部特征。
1.  **高保真特征搜索：** 对于 `D_sft` 中的每个数据点，执行第3点中描述的高保真特征搜索流程（公式(7)(8)），得到特征集合 `D_h = {h_θ^(i*)}`。
2.  **CFN训练：** 在特征集 `D_h` 上训练硬币翻转网络 `f_φ`。CFN的学习目标是最小化回归损失（公式(1)）：`min_φ E_{(h, c)} [ || f_φ(h) - c ||^2 ]`，其中 `c` 是一个随机生成的 `d` 维二元向量（±1）。根据CFN理论（公式(2)），对于特征 `h`，其伪计数 `ˆN_Dsft(h) ∝ 1 / ||f_φ(h)||^2`（公式(9)）。因此，`||f_φ(h)||^2` 越小，伪计数越高，代表该特征对应的动作越接近数据集中频繁出现（即成功）的模式。

**4.2 推理阶段（测试时扩展与反探索）**
给定当前观测 `o_t` 和语言指令 `l`，执行以下步骤：
1.  **候选动作生成：** 利用已微调的VLA模型 `π_θ` 作为候选生成器。对于扩散/流模型，采样 `M` 个不同的初始噪声向量 `{ε_i}`，以批量并行的方式进行完整的去噪过程，生成 `M` 个候选动作块 `{â_(t:t+H)^(i)}`，并同步提取它们的内部表征 `{h_θ^(i)}`（见第4.3节）。
2.  **伪计数验证与选择：** 将每个候选特征 `h_θ^(i)` 输入训练好的CFN `f_φ`，计算其伪计数 `ˆN_Dsft ∝ 1 / ||f_φ(h_θ^(i) )||^2`。选择伪计数最高的候选动作作为最终执行动作（公式见第4.3节）：`â_(t:t+H)^* = â_(t:t+H)^(i*)`, 其中 `i* = argmax_i 1 / ||f_φ(h_θ^(i) )||^2`。这一步骤直接实现了公式(6)的反探索目标，即选择数据集中密度最高（最支持）的动作。
3.  **高效推理优化（KV缓存）：** 为了降低并行生成 `M` 个候选动作的开销，论文提出了关键的优化方法（见第4.3节）。由于昂贵的VLA计算（如Transformer主干）仅依赖于共享的上下文（`o, l`），因此可以预先计算并缓存这些上下文的键（K）和值（V）。在生成 `M` 个候选动作时，复用同一个KV缓存，使得额外候选的边际计算成本极低。如图4（左）所示，该优化在 `M=32` 时将推理时间降低了73.2%。

#### **5. 实验说明**
**评估指标：** 任务成功率（%）。
**数据集：**
*   **仿真：**
    *   **RoboTwin 1.0 & 2.0：** 专注于双夹爪操作的基准，包含多种资源和任务类型。
    *   **Simpler (WindowX)：** 基于SAPIEN和ManiSkill2

---

## 2. VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling

### 基本信息
- **作者**: Weiqi Li, Quande Zhang, Ruifeng Zhai, Liang Lin, Guangrun Wang
- **arXiv ID**: [oai:arXiv.org:2512.02902v1](https://arxiv.org/abs/2512.02902)
- **发布日期**: Thu, 04 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.02902)

            ### 原文摘要
            arXiv:2512.02902v1 Announce Type: cross  Abstract: Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.


            
### AI分析（基于论文正文）
好的，作为一名熟悉顶级机器学习/人工智能会议风格的资深论文总结者，我将根据您提供的论文信息和要求，生成一份结构清晰、内容详实的论文总结。

***

### **论文总结：VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling**

#### **1. 论文概要**
本文针对视觉-语言-动作模型在遭遇新相机视角等视觉扰动时性能急剧下降的鲁棒性问题，提出了一种新颖的诊断视角和高效的解决方案。作者将VLA模型的功能解耦为**空间建模**和**物理建模**，并通过实验论证性能下降主要源于视觉编码器产生的空间表征与下游策略模块之间的**表征错位**，而非物理建模能力不足。为此，论文提出了两种轻量级的单样本适应方法：特征令牌调制和特征线性适应。这两种方法仅需极少的可训练参数，即可重新校准视觉表征，在LIBERO-V基准测试中显著恢复了模型对新视角和多种视觉扰动的鲁棒性，同时实现了高达99倍的参数效率提升。

#### **2. 研究动机**
当前，预训练的VLA模型在分布内任务上表现出色，但在面对真实世界中不可避免的分布外视觉变化时，其鲁棒性严重不足。现有提升鲁棒性的方法主要分为两类（见第1、2.2节）：
1.  **数据中心化方法**：通过收集大规模、多视角的机器人数据集来增加视觉多样性。然而，这种方法成本高昂，难以扩展到所有可能的现实场景，且持续适应不切实际。
2.  **表征中心化方法**：旨在通过几何一致性学习视角不变的表示，例如使用多视角监督或3D感知架构。尽管这些方法提升了视角不变性，但它们对背景杂乱、光照变化等与任务无关的视觉因素仍然敏感，表明在视角鲁棒性和通用视觉适应性之间存在差距。

现有研究普遍隐含地假设，提升鲁棒性需要额外的数据或复杂的3D一致架构，但鲜有工作明确指出性能下降的根本原因是否在于空间表征本身。本文的研究动机正是为了填补这一空白。作者基于一个概念性分解框架（见图1(a)及第1节），将VLA模型解耦为**空间建模**和**物理建模**。视角变化主要改变的是场景的空间配置，而非任务语义或动作动力学。因此，作者假设：VLA性能下降主要源于**空间建模**产生的表征发生错位，而**物理建模**组件（VLM和动作专家）本身的功能依然完备。这一假设将问题根源定位到视觉编码器的表征对齐上，从而引出了本文的核心研究问题：**能否通过极轻量级的、仅针对视觉表征的适应，而非大规模重训练或架构修改，来恢复VLA模型的鲁棒性？**

#### **3. 核心贡献与创新点**
本文的核心贡献在于提出了一个全新的诊断视角和一套高效的单样本适应框架，具体创新点如下：

1.  **对VLA模型鲁棒性根源的重新诊断与概念性分解**：论文首次明确地将VLA模型在视觉扰动下的性能下降归因于**空间建模**模块的表征错位，而非**物理建模**模块的能力不足（见第1、3节）。这一诊断视角是方法设计的基础，它挑战了“需要更多数据或更强架构”的传统思路，转而关注如何高效地“重新激活”预训练模型中已存在的潜在鲁棒性。

2.  **提出统一的单样本鲁棒性适应框架**：基于上述诊断，作者提出了一个轻量级的适应框架，该框架的核心创新在于**仅对视觉表征进行最小化的参数更新**，而保持VLA模型的其余部分（语言编码器、多模态解码器、动作专家）完全冻结。这显著区别于传统的对整个模型进行微调或更换视觉骨干网络的方法（见图3对比）。

3.  **设计两种极高效的适应机制**：
    *   **特征令牌调制**：这是一种极其简单但有效的机制（见第3.3节，公式(4)）。它仅在视觉编码器输出的令牌嵌入上施加一个**全局仿射变换**，即引入两个可学习的向量γ和β，对特征进行缩放和偏移：`F_hat = (1 + γ) ⊙ F + β`。该方法仅引入`2 * D_ViT`个参数（对于π0.5模型为4K），却能将新视角下的成功率从48.5%大幅提升至87.1%（见表1）。其创新性在于，它验证了即使是最简单的、全局的令牌级校正，也足以部分恢复表征对齐，从而强有力地支持了关于“错位根源”的核心假设。
    *   **特征线性适应**：作为对FTM的深化（见第3.4节，公式(5)），FLA将低秩适应技术应用于视觉编码器内部的线性层。对于每个线性层`W`，引入低秩分解`ΔW = B A`，其中`A`和`B`为可训练的低秩矩阵。与对整个VLA模型应用LoRA不同，FLA**仅针对视觉编码器**进行低秩更新。该方法以仅4.7M的参数（相比LoRA的467M，减少99倍），达到了与全参数LoRA微调相当甚至略优的性能（90.8% vs 90.3%，见表1）。其创新点在于，它证明了在空间建模模块内部进行更深层次的、结构化的轻量级调整，可以更精细地实现特征重对齐，且效率极高。

4.  **构建LIBERO-V综合视觉鲁棒性评测基准**：为了系统评估模型对各种视觉扰动的鲁棒性，论文整合了LIBERO的任务和Libero-Plus的视觉扰动，创建了LIBERO-V基准（见第4.1节，图4(b)）。该基准涵盖了相机视角、光照、背景纹理和图像噪声四种可控的扰动类型，为未来研究提供了一个统一的、全面的评测平台。

#### **4. 方法概述**
本文的方法流程清晰，围绕对视觉表征的轻量级适应展开，整体架构如图1(a)和图3(d,e)所示。

**基础模型与问题形式化**：作者采用π0.5作为基础VLA策略（见第3.1节）。在时间步t，策略接收视觉观测`v_t`和语言指令`l`，通过视觉编码器`f_v`和语言编码器`f_l`分别得到嵌入，再由多模态解码器`g`自回归地预测动作令牌`a_t`（公式(1)(2)）。在适应阶段，仅引入额外的轻量级适应参数`φ`，作用于视觉表征，预测分布变为`P_{θ,φ}(a_t | ...) = g( a_{<t}; [ A_φ(f_v(v)); ℓ] )`（公式(3)），其中`A_φ(·)`代表本文提出的适应变换。

**两种适应机制的具体实现**：
1.  **特征令牌调制**：如图4(a)所示，FTM作用于视觉编码器`f_v`输出的令牌序列`F ∈ R^{N×D}`。它学习两个全局向量`γ, β ∈ R^D`，对每个令牌的每个特征维度进行相同的仿射变换（公式(4)）。`γ`实现缩放，`β`实现平移。这个操作可以理解为对因视角变化而发生分布漂移的视觉特征进行“重中心化”和“重缩放”，使其与下游物理建模模块的期望输入重新对齐。训练时，仅优化`(γ, β)`，保持`f_v, f_l, g`冻结。
2.  **特征线性适应**：如图3(e)所示，FLA将LoRA模块集成到视觉编码器（本文使用SigLIP）的线性层中。对于编码器中的每一个线性变换`h = Wx`，在原始的冻结权重`W`上增加一个低秩更新`ΔW = BA`，其中`A ∈ R^{r×d_in}`, `B ∈ R^{d_out×r}`，秩`r`远小于输入输出维度。因此，前向传播变为`h = Wx + BAx`。在适应训练中，仅优化所有LoRA模块的`{A, B}`参数对，原始模型权重`W`和VLA其他部分均保持冻结。这种方法允许模型在特征提取的更深层次进行自适应调整，以更精细地纠正由扰动引起的内部表征偏差。

**训练流程**：两种方法均采用**单样本适应**范式。给定一个新视角（或视觉扰动）下的目标任务，收集**单条**人类演示轨迹。使用该轨迹数据，在基础预训练的π0.5模型上，仅针对新增的适应参数（FTM的`γ, β`或FLA的所有`{A, B}`对）进行优化。实验中使用Adam优化器，训练2000步，批量大小为32（见第4.3节）。这种设置凸显了方法的高效性和实用性。

#### **5. 实验说明**
*   **评估指标**：主要评估指标为任务**成功率**。
*   **数据集**：
    *   **LIBERO**：用于评估新相机视角下的鲁棒性。包含Spatial, Object, Goal, Long四个任务套件，每个套件10个任务。使用[27]提供的未见过的相机视角进行评估。
    *   **LIBERO

---

## 3. AdaPower: Specializing World Foundation Models for Predictive Manipulation

### 基本信息
- **作者**: Yuhang Huang, Shilong Zou, Jiazhao Zhang, Xinwang Liu, Ruizhen Hu, Kai Xu
- **arXiv ID**: [oai:arXiv.org:2512.03538v1](https://arxiv.org/abs/2512.03538)
- **发布日期**: Thu, 04 Dec 2025 00:00:00 -0500
- **分类**: cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.03538)

            ### 原文摘要
            arXiv:2512.03538v1 Announce Type: new  Abstract: World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \textbf{AdaPower} (\textbf{Ada}pt and Em\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《AdaPower: Specializing World Foundation Models for Predictive Manipulation》，生成一份符合顶级会议风格的详细总结。

***

### **论文总结：AdaPower: Specializing World Foundation Models for Predictive Manipulation**

#### **1. 论文概要**
本文旨在解决将通用世界基础模型应用于机器人精确控制时存在的“生成真实性与控制导向精度”之间的鸿沟。现有方法通常将WFM用作合成数据生成器，存在计算成本高、未充分利用预训练视觉-语言-动作模型等问题。为此，作者提出了AdaPower，一个轻量级适配框架，通过**时空测试时训练**和**记忆持久化**两个核心模块，将通用WFM高效地转化为面向控制的专家世界模型。该专家模型被集成到模型预测控制框架中，与预训练的VLA策略协同工作，在不重新训练策略的情况下，显著提升了在LIBERO基准测试上的任务成功率，同时保持了计算效率和泛化能力。

#### **2. 研究动机**
当前，基于互联网规模视频数据训练的世界基础模型在视觉动态模拟方面展现出卓越能力（第1节）。然而，将这些强大的生成模型直接部署到需要连续、细粒度环境交互的机器人操作领域面临根本性挑战（第1节）。核心矛盾在于：WFM擅长生成视觉上合理的未来预测，而机器人控制则需要**以动作为条件、精确可执行的动态预测**。这种生成能力与控制导向可靠性之间的脱节，阻碍了WFM在机器人学中的直接应用。

现有最先进的方法（如DreamGen）采用**合成数据生成范式**来弥合这一差距（第1节）。该范式首先在有限的机器人数据上微调WFM，然后使用适配后的模型生成合成视频，再通过逆动力学模型提取伪动作序列，最后从生成的神经轨迹中训练新的视觉运动策略。尽管结果有前景，但该范式存在三个根本性局限（第1节）：
1.  **高昂的计算成本**：生成海量视频数据集和训练新策略的计算开销巨大。
2.  **冗长的适应周期**：每个新任务或环境都需要重复整个流程。
3.  **对预训练策略的利用不足**：该范式绕过了增强现有通用VLA模型（如CogACT, π0）的潜力，转而训练新的专家策略，未能充分利用已有的强大先验知识。

因此，本文的研究动机是设计一种**高效、轻量级的模型适配方法**，直接改造通用WFM，使其能够**赋能而非替代**现有的预训练VLA策略，从而在保持计算效率的同时，实现对新环境的零样本泛化能力提升。

#### **3. 核心贡献与创新点**
本文的核心贡献在于提出了一套完整的框架和两个关键的技术模块，实现了从“数据生成范式”到“模型适配赋能范式”的转变。

1.  **提出了AdaPower整体框架**：这是一个新颖的、高效的适配框架，旨在将通用WFM转化为面向机器人操作的专家世界模型（第1、3节）。其创新性在于摒弃了生成合成数据再训练策略的繁重流程，转而通过**轻量级参数优化**直接提升WFM在特定领域的预测精度和确定性（见图1a），并将其无缝集成到MPC规划器中，与冻结的预训练VLA策略协同工作（见图1b, 3）。这实现了**计算效率、通用知识保留和真正的零样本性能提升**三者的统一。

2.  **设计了时空测试时训练模块**：这是本文第一个核心技术创新（第3.2节）。传统的测试时训练主要针对序列建模，学习通道维度的低秩先验（2D特征）。TS-TTT的创新在于，它识别并利用了视频数据在**时空维度上固有的低秩先验**（空间低秩：同一帧内相邻像素相似；时间低秩：连续帧间背景内容静态或渐变）。该模块通过两个并行的自学习分支（时空分支和通道分支），对4D视频特征进行联合低秩自监督优化（见公式1-6）。这种设计放大了自监督学习信号，使模型能够在推理时（即MPC规划过程中）自适应地优化部分参数，从而更好地泛化到未见过的测试时数据分布（见图2）。

3.  **设计了记忆持久化模块**：这是本文第二个核心技术创新（第3.3节）。为了解决WFM在长时程自回归预测中存在的**误差累积和知识遗忘**问题，MP模块通过跨注意力机制，将历史帧的上下文信息显式地注入到当前扩散Transformer块的推理过程中（见图2）。具体而言，它使用DINOv2编码器从历史帧中提取以物体为中心的记忆特征，并将其作为键和值，与当前DiT特征生成的查询进行跨注意力交互。这种机制确保了多步预测的**时间一致性**，减少了因早期观测影响逐渐减弱而导致的预测漂移，对于需要长视野规划的操控任务至关重要。

#### **4. 方法概述**
AdaPower方法的核心是在一个基于扩散Transformer的WFM骨干网络（采用Cosmos-Predict-2B）中，插入两个轻量级适配模块，并修改条件模态。

**整体架构与适配流程**（第3.1节）：如图2所示，适配器包含TS-TTT和MP两个模块。为了支持以动作为条件的预测，作者将原始WFM的文本编码器替换为一个由多层感知机构成的**动作编码器**。在适配训练阶段，**仅新引入的层（MP模块、TS-TTT模块、动作编码器）被训练**，而预训练的WFM参数保持冻结，确保了计算效率并保留了通用的物理动力学先验。

**TS-TTT模块的详细运作**（第3.2节）：给定视频输入特征 \( v \in \mathbb{R}^{T \times H \times W \times D} \)，TS-TTT执行以下流程：
*   **时空分支**：将 \( v \) 展平为 \( v_{ts} \in \mathbb{R}^{D \times THW} \)，将时空维度融合。通过一个参数为 \( W \) 的神经网络 \( f \)，对经过低秩投影 \( \Theta_K v_{ts} \) 的损坏输入进行重建，目标为另一个投影 \( \Theta_V v_{ts} \)（公式4）。在推理时，仅在线优化 \( W \)（公式2），而 \( \Theta_Q, \Theta_K, \Theta_V \) 在训练阶段与基础网络一同优化。输出为 \( z_{ts} \)。
*   **通道分支**：将 \( v \) 重塑为 \( v_c \in \mathbb{R}^{THW \times D} \)。采用与时空分支相同的低秩投影与重建机制，但使用独立的参数集，输出为 \( z_c \)。
*   **特征融合**：将 \( z_{ts} \) 和 \( z_c \) 重塑回与 \( v \) 相同的形状，并通过残差连接得到最终输出 \( v_o = v + z_{ts} + z_c \)（公式6）。

**MP模块的集成**（第3.3节）：在每一个DiT块（每7个块插入一次）中，MP模块执行跨注意力操作。历史帧 \( H \) 经DINOv2编码得到记忆特征 \( m \)，重塑后作为键和值。当前DiT特征作为查询。通过标准的注意力机制，将历史上下文信息融合到当前特征中，以维持长时程一致性。

**与MPC的协同部署**（第3.4节）：适配后的专家世界模型被部署在一个MPC框架中（图3）。流程如下：
1.  预训练的VLA根据初始状态和任务指令，生成多条候选动作序列。
2.  为增加探索多样性，对这些序列添加低方差高斯噪声进行增强。
3.  专家世界模型对每条候选序列进行前向模拟，生成预测的未来状态轨迹。
4.  基于ReWind方法的奖励模型根据任务指令和历史观测，评估每条轨迹的预期奖励。
5.  选择奖励最高的动作序列执行。
此架构结合了VLA的通用高层推理能力和专家世界模型精确的物理模拟能力，实现了零样本性能提升。

#### **5. 实验说明**
*   **评估指标**：主要评估指标为**任务成功率**。每个任务执行20次，报告平均成功率（第4.1节）。在模型预测质量评估中使用了**测试均方误差**（第4.3节，表3）。
*   **数据集**：
    *   **训练/适配**：使用从**LIBERO-90**数据集中采样的2000条轨迹（第4.1节）。这是一个大规模机器人操作数据集，涵盖多样物体、运动和接触丰富的交互。
    *   **评估**：在**10个未见过的LIBERO任务**上进行零样本评估（第4.1、4.2节）。此外，在真实世界实验中，收集了**200个机器人-环境交互视频片段**进行适配，并在5个物理任务上评估（第4.4节）。
*   **对比基线方法**：
    *   **参数高效适配器**：VACE（视频创建与编辑适配器）、LoRA（低秩适配）（第4.2节）。
    *   **全参数微调**：监督式微调（第4.2节）。
    *   **消融对比**：单独使用TS-TTT或MP模块的

---

## 4. Hierarchical Vision Language Action Model Using Success and Failure Demonstrations

### 基本信息
- **作者**: Jeongeun Park, Jihwan Yoon, Byungwoo Jeon, Juhan Park, Jinwoo Shin, Namhoon Cho, Kyungjae Lee, Sangdoo Yun, Sungjoon Choi
- **arXiv ID**: [oai:arXiv.org:2512.03913v1](https://arxiv.org/abs/2512.03913)
- **发布日期**: Thu, 04 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.03913)

            ### 原文摘要
            arXiv:2512.03913v1 Announce Type: new  Abstract: Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Hierarchical Vision Language Action Model Using Success and Failure Demonstrations》及其详细约束，生成一份结构清晰、内容详实的论文总结。

***

### **论文总结：Hierarchical Vision Language Action Model Using Success and Failure Demonstrations**

#### **1. 论文概要**
本文针对现有视觉-语言-动作模型通常仅利用成功的遥操作演示进行训练，而丢弃大量自然产生的失败尝试的问题，提出了一种能够利用混合质量（成功与失败）数据集的层次化VLA框架VINE。该框架受启发于分层强化学习，将高层推理与低层控制解耦。高层系统（System 2）在2D场景图抽象上进行可行性引导的树搜索，利用从成功和失败数据中学习到的价值函数评估候选子目标序列的可行性，并修剪脆弱分支。低层系统（System 1）则执行选定的子目标序列，不改变核心技能。实验表明，该方法在多个具有挑战性的操作任务中显著提升了成功率和鲁棒性，证明了失败数据对于将VLA的广泛能力转化为鲁棒执行至关重要。

#### **2. 研究动机**
当前，用于训练VLA模型的机器人数据主要来源于人类遥操作收集的离线轨迹。这些数据收集过程自然包含大量失败尝试（如不稳定抓取、碰撞），但传统上这些失败轨迹通常被视为噪声而被丢弃（见第I节）。然而，这些失败数据编码了关于不可行状态转移和不成功行为的关键信息，是提升模型鲁棒性的重要资源（见第I节）。

现有工作存在两个主要不足，构成了本研究的动机：
1.  **对失败数据的利用不足或低效**：虽然已有工作（如论文引用的[1], [4]）表明在混合质量数据上训练可以提升鲁棒性，但大多是在模仿学习框架下将失败数据作为噪声或正则化项，难以精细地利用失败信号来指导决策边界（见第I、II.A节）。在模仿学习中，显式地惩罚失败倾向的转移需要仔细调整，以避免扭曲学习到的策略（见第I节）。
2.  **缺乏显式的、失败感知的推理机制**：现有VLA模型多为端到端的反应式策略，缺乏在行动前进行显式规划、评估候选方案可行性并规避风险的能力（见第II.A节）。同时，一些使用大语言模型进行符号规划的工作（如Code as Policies, ReAct）在深度理解环境动态、预测失败或验证动作可行性方面能力有限（见第II.B节）。近期结合高层VLMs进行规划的工作（如Hi Robot, HAMSTER）也大多忽略了显式的失败预测和可行性验证（见第II.B节）。

因此，本文的核心动机是：**如何有效地将离线数据集中丰富的失败信号整合到VLA的决策循环中，并构建一个能够进行显式、失败感知推理的规划机制，从而在无需在线试错的情况下，将VLA的广泛能力转化为更鲁棒的执行。**

#### **3. 核心贡献与创新点**
本文提出了VINE框架，其核心贡献与创新点如下：

1.  **提出了一种利用失败数据进行失败感知推理的层次化VLA框架**：这是本文最核心的概念性创新。VINE受启发于分层强化学习，明确地将系统分为负责高层推理规划的System 2和负责低层控制执行的System 1（见第I、III.A节）。这种分离使得失败监督能够被“干净地”注入到高层规划循环中，用于学习可行性价值函数，而无需修改底层的动作生成技能（见第I节）。这与直接将失败数据与成功数据混合训练的统一模型有本质区别。

2.  **引入了基于场景图抽象的可行性引导树搜索规划器**：这是方法上的核心创新。System 2的规划并非简单的链式思维，而是在一个由2D场景图（节点）和子目标（边）构成的抽象状态空间上进行树搜索（见第I、III.C节）。具体而言：
    *   **抽象表示**：将原始状态（图像）抽象为包含物体及其空间语义关系的2D场景图节点（`n = φ(s)`），将子目标（如“拿起勺子”）抽象为边（`e`）（见第III.C.1-2节）。
    *   **可行性评分**：为每个搜索节点学习一个价值函数 `V(n)`，该函数估计从该节点出发，在进入失败集 `F` 之前成功到达目标集 `G` 的概率（`V(n) = Pr(τ_G < τ_F | n)`），即“到达-避免”成功概率（见第III.E节，命题1）。该价值函数利用非对称期望分位数损失（借鉴IQL）从包含大量失败的静态数据集中进行保守估计（见第IV.C.2节）。
    *   **搜索与执行**：通过改进的蒙特卡洛树搜索算法（算法1），结合节点提议、状态转移预测和价值评估，选择价值最高的路径。选定的子目标序列（边）和预测的下一状态（节点）被传递给System 1执行（见第IV.C.3节）。

3.  **设计了一个共享主干网络与适配器的双系统架构**：这是实现上的创新。VINE基于预训练的VLA模型（π0）和语言模型（PaliGemma）构建共享的视觉-语言主干网络，并通过线性插值合并参数（`θ_merge = λ_m * θ_π0 + (1-λ_m) * θ_PG`）（见第IV.B节）。在此之上，为System 1和System 2分别添加独立的LoRA适配器和专家头（动作/完成专家，语言/价值头），使两个系统能共享感知-语言表征，同时保持推理与控制解耦，并独立训练（见第IV.B节）。

#### **4. 方法概述**
VINE方法的核心流程分为离线训练和在线推理两阶段，其技术细节如下：

**A. 离线数据准备与问题形式化**：
首先，从混合质量的遥操作演示中构建层次化数据集。通过关键帧检测（基于夹爪状态事件）将连续轨迹分割为一系列“节点-边-节点”三元组 `(n_k, e_k, n_{k+1})`（见第III.C.2-3节）。由此构建两个数据集：`D_sys2` 用于训练System 2的规划组件（包含节点、边、奖励、上下文），`D_sys1` 用于训练System 1的控制组件（包含状态、动作、条件化的边和下一节点）（见第III.C.3节）。整个框架被形式化为一个半马尔可夫决策过程，其中边对应SMDP中的选项（见第III.B节）。

**B. 系统架构**：
模型基于合并后的共享主干（`θ_merge`）。System 2的适配器连接至**语言建模头**（用于自回归生成候选节点和边字符串）和**标量价值头**（用于预测节点价值`V_θ(n)`）（见第IV.B节）。System 1的适配器连接至**动作专家**（一个流匹配模型，用于生成动作块）和**完成专家**（一个分类器，用于预测子目标完成概率`β_θ`）（见第IV.B、D.1节）。

**C. System 2训练与推理**：
*   **训练**：
    1.  **语言建模**：使用标准LM损失训练节点和边的生成：`L_LM = -E[log P_θ(n_k) + log P_θ(e_k | n_k)]`（见第IV.C.1节）。
    2.  **价值学习**：使用基于首次退出引导的目标`y_k`（成功为1，失败为0，否则为折扣后的目标网络价值）和非对称期望分位数损失`L_val`来训练价值头，以从静态数据集中获得保守的价值估计（见第IV.C.2节）。
*   **推理（树搜索）**：给定初始状态和指令，执行算法1中的批处理树搜索。
    1.  **提议**：从当前前沿节点，使用束搜索通过LM头生成`k`个候选边`{e_i}`（见第IV.C.1节）。
    2.  **扩展与评估**：对于每个候选边`(n_p, e_i)`，贪婪解码预测下一节点`n_c`，并通过价值头计算`V(n_c)`（见算法1第5-6行）。
    3.  **回溯**：将子节点价值`V(n_c)`沿路径回溯，更新父节点的访问次数`N`和累计价值`W`，并计算平均价值`Q = W/N`（见算法1第7行）。
    4.  **选择**：经过`M`次迭代后，选择从根节点到具有最高`Q`值的前沿节点的路径作为最终计划`τ_sel`（见算法1第8行）。

**D. System 1训练与推理**：
*   **训练**：仅使用成功数据。动作专家通过流匹配损失`L_act`进行训练，匹配从噪声动作`A_t^τ`到干净动作`A_t`的Oracle速度场`u`。完成专家通过焦点损失`L_done`训练。总损失

---

## 5. PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention

### 基本信息
- **作者**: Ziwen Li, Xin Wang, Hanlue Zhang, Runnan Chen, Runqi Lin, Xiao He, Han Huang, Yandong Guo, Fakhri Karray, Tongliang Liu, Mingming Gong
- **arXiv ID**: [oai:arXiv.org:2512.03724v1](https://arxiv.org/abs/2512.03724)
- **发布日期**: Thu, 04 Dec 2025 00:00:00 -0500
- **分类**: cs.CV, cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.03724)

            ### 原文摘要
            arXiv:2512.03724v1 Announce Type: cross  Abstract: The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention》内容，生成一份结构清晰、内容详实的总结报告。

***

### **论文总结报告：PosA-VLA**

#### **1. 论文概要**
本文针对当前视觉-语言-动作模型在执行具身任务时存在动作冗余、轨迹不稳定、执行效率低的问题，提出了一种名为PosA-VLA的新框架。作者将问题根源归结为现有VLA模型的“空间均匀感知场”，即模型缺乏对任务相关区域和机器人末端执行器的持续、稳定关注。为解决此问题，PosA-VLA通过**姿态条件锚点注意力**机制，利用机器人末端执行器的真实姿态生成空间监督信号，引导模型的视觉注意力聚焦于任务相关区域和末端执行器位置。该方法不依赖外部感知模块（如分割或定位网络），基于轻量级架构实现了更精确、高效和稳定的动作生成。实验在多种机器人操作基准测试中验证了其有效性。

#### **2. 研究动机**
当前，视觉-语言-动作模型在连接AI的认知智能与物理世界交互方面展现出巨大潜力。然而，论文指出，现有VLA模型在实际应用中难以生成**一致且精确**的动作（见第1节）。具体表现为：1）运动轨迹存在冗余和不稳定行为；2）动作生成不一致，控制精度不足。如图1所示，基线模型在执行抓取任务时，末端执行器与目标点之间的距离曲线波动剧烈，表明存在频繁的动作反转和偏离最优轨迹的问题。这不仅增加了完成任务所需的步骤数，降低了效率，更可能导致不稳定接触、抓取失败或与周围物体碰撞等严重问题。

作者从注意力机制的角度深入分析了这一现象（见第1节），认为根本原因在于现有VLA模型（如OpenVLA、Smol-VLA）依赖**全局均匀的感知场**，缺乏将注意力明确引导至任务相关区域和末端执行器区域的机制。在没有这种空间选择性的情况下，模型无法建立反映真实交互动态的持续关注点。由于缺乏以机器人姿态为条件的内部注意力信号，这些模型倾向于被动地扫描整个场景，而非主动聚焦于最具信息量的区域。因此，它们难以在整个执行过程中保持稳定、连贯的注意力，导致感知与真正决定任务成功的区域错位，从而产生不准确或次优的动作。这一问题在杂乱或视觉复杂的环境中尤为明显，模型的注意力容易被任务无关的物体或背景元素分散，产生不稳定的运动轨迹，甚至完全错误的动作。基于此分析，论文旨在设计一种能够显式地、持续地将机器人姿态与视觉注意力关联起来的机制，以解决上述问题。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面：

1.  **对VLA不一致性的实证分析与问题归因**：论文不仅指出了现有VLA模型动作不精确的现象，更通过实验分析（如图1、图3）将其根源归因于模型**空间均匀的感知场**。这一分析超越了单纯提升模型性能的层面，从注意力机制的角度为理解VLA模型的局限性提供了新的视角，为后续的解决方案奠定了理论基础（见第1节及图3说明）。

2.  **提出姿态条件锚点注意力机制**：这是本文最核心的概念创新。与依赖外部分割模型（如Grounding DINO, SAM）进行空间定位的现有方法（见第2.2、2.3节）不同，PosA-VLA提出了一种**自包含的、基于机器人本体感知的空间注意力引导方法**。该机制利用机器人末端执行器的真实3D姿态（从演示数据中获得），通过相机投影将其转换为2D图像空间中的高斯热图，作为监督信号（“锚点”）。具体而言，它生成两种互补的锚点图（见第3.2节，公式(3)(4)）：**任务相关锚点图**（在末端执行器状态发生交互变化时生成，指示目标区域）和**末端执行器锚点图**（在每个时间步生成，跟踪末端位置）。这两个锚点共同作为姿态条件的空间先验，将连续的3D交互空间转化为局部化的2D监督，有效打破了模型的空间均匀感知。

3.  **设计了结合空间分类与批量对比学习的锚点损失函数**：为了有效地将上述锚点监督信号融入模型训练，论文设计了一个复合损失函数 `Lanchor`（见第3.3节，公式(5)-(11)）。该损失包含两部分：a) **空间注意力损失**：使用Focal Loss直接监督预测的注意力权重图 `Mt` 与真实锚点图 `Ff` 对齐；b) **批量对比损失**：这是一个新颖的设计，它在批次样本级别操作，通过计算文本-图像融合嵌入之间的相似度，鼓励来自相同指令的样本在锚点区域的特征对齐，而不同指令的样本特征分离。这种设计不仅增强了空间定位的准确性，还提升了跨样本的多模态对齐一致性，这是现有VLA工作所忽视的。

#### **4. 方法概述**
PosA-VLA框架的整体流程如图2所示，其技术方案可分解为以下几个关键步骤：

**A. 多模态特征提取与初步注意力生成**：
首先，使用CLIP文本编码器和图像编码器分别提取语言特征 `fx` 和来自头戴/腕戴相机的视觉特征 `FI`（见第3.2节）。随后，通过一个交叉注意力模块融合文本和视觉特征，生成初步的**任务相关注意力权重** `Mtask_t`。同时，使用一个代表末端执行器的查询文本嵌入（如“gripper”）生成**末端执行器注意力权重** `Mend_t`。两者沿通道维度堆叠，得到初步的锚点注意力权重 `Mt`（公式(2)）。

**B. 姿态条件锚点监督信号的构建**：
这是方法的核心。从演示数据中，获取机器人末端执行器的3D姿态 `pt`。对于**任务相关锚点**，识别末端执行器状态（如夹爪开合）发生变化的时刻，将该时刻的3D位置投影到图像平面，并以该点为中心生成一个高斯热图 `Ftask_f`（公式(3)）。对于**末端执行器锚点**，在**每一个**时间步都将当前末端3D位置投影，并生成一个方差更小（`σ_end < σ_task`）的高斯热图 `Fend_f`（公式(4)），以提供更精确的位置提示。

**C. 锚点损失监督与注意力精炼**：
使用第3点所述的复合锚点损失 `Lanchor` 对初步生成的注意力权重 `Mt` 进行监督。经过训练后，`Mt` 学会了聚焦于锚点指示的区域。然后，将学习到的 `Mt` 与更稠密的DINOv2图像特征 `F_DINO` 进行逐元素相乘（公式(12)），得到**精炼后的视觉特征** `Fref_v`。此操作相当于用一个空间掩码突出了对任务至关重要的视觉信息。

**D. 基于流匹配的动作生成**：
将精炼后的视觉特征 `Fref_v`、文本特征 `fx` 和机器人状态 `st` 融合，形成多模态观测表示 `~zobs_t`。动作生成采用**流匹配变换器**（见第3.4节）。与传统的扩散策略不同，FMT学习一个从简单先验分布（如高斯噪声）到目标动作分布的连续向量场（流）。在训练时，它通过一个速度匹配目标（公式(15)）直接学习如何将噪声样本“推”向真实动作。在推理时，只需对学习到的流场进行一次数值积分（公式(16)），即可从噪声中生成平滑的动作序列，避免了扩散模型的多步迭代去噪，从而显著提高了推理效率。最终的总损失为动作损失和锚点损失的加权和（公式(17)）。

#### **5. 实验说明**
- **评估指标**：
    - **主要指标**：任务成功率。对于抓取任务，定义为成功抓取并稳定抬升至空中；对于长视野任务（开盒放置），报告每个子步骤（开盖、抓取物体、放置物体）的成功率以及整体任务成功率（所有子步骤均成功）。
    - **效率指标**：单动作平均推理时间（毫秒）、完成任务所需的总动作步数、总执行时间（秒）、模型训练所需GPU小时数。

- **数据集**：
    - 使用自建的机器人演示数据集进行训练和评估。数据通过在5x5网格上随机放置目标物体和干扰物，并通过遥操作收集（见补充材料第7节及图5）。每个目标物体收集200条演示轨迹。
    - 测试环境包括多种设定以评估泛化能力（见图4）：基础场景、未见过的背景、变化的灯光、存在干扰物、未见过的物体以及长视野任务。

- **对比基线方法**：
    - **大规模VLA模型**：`π0`， `OpenVLA-OFT`。
    - **轻量级VLA模型**：`Smol-VLA`。
    - **依赖外部感知的VLA模型**：`DexGraspVLA`（依赖大型基础模型进行检测和分割）。
    - **参数高效微调变体**：`π0.5 (LoRA)` 和 `π0.5 (Full

---

## 6. FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction

### 基本信息
- **作者**: Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, Jingtai Liu
- **arXiv ID**: [oai:arXiv.org:2509.04018v2](https://arxiv.org/abs/2509.04018)
- **发布日期**: Thu, 04 Dec 2025 00:00:00 -0500
- **分类**: cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2509.04018)

            ### 原文摘要
            arXiv:2509.04018v2 Announce Type: replace  Abstract: Robotic manipulation is a fundamental component of automation. However, traditional perception-planning pipelines often fall short in open-ended tasks due to limited flexibility, while the architecture of a single end-to-end Vision-Language-Action (VLA) offers promising capabilities but lacks crucial mechanisms for anticipating and recovering from failure. To address these challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with a supervisor for failure prediction and correction. The supervisor evaluates action viability through vision-language queries and generates corrective strategies when risks arise, trained efficiently without manual labeling. A dual-stream fusion module further refines actions by leveraging past predictions. Evaluation results on multiple simulation platforms (SIMPLER and LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA outperforms state-of-the-art models in both zero-shot and fine-tuned settings. Successful real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong generalization and practical utility for building more reliable autonomous systems.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文信息，生成一份符合要求的详细总结。

### **论文总结：FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction**

---

#### **1. 论文概要**
本文提出了一种名为FPC-VLA的新型视觉-语言-动作（VLA）框架，旨在解决机器人操作中端到端VLA模型缺乏故障预测与自校正能力的问题。该框架的核心是一个双模型架构，将基础的VLA模型与一个基于视觉语言模型（VLM）的“监督器”相结合。监督器在关键帧（如夹爪状态改变时）被触发，通过结构化视觉-语言查询评估动作的可行性。若预测到潜在失败，它会生成包含方向和幅度信息的自然语言校正指令来修正原始动作。此外，论文还提出了一种从现有机器人数据自动生成大规模故障预测与校正数据集的方法，以及一个双流动作融合模块以平滑动作输出。实验在多个仿真平台（SIMPLER， LIBERO）和机器人实体（WidowX， Google Robot， Franka）上进行，并在小米机器人和ALOHA上进行了真实世界部署，结果表明FPC-VLA超越了现有先进方法。

#### **2. 研究动机**
论文的研究动机源于当前机器人操作领域两个关键且相互关联的挑战：传统方法的局限性与现有VLA模型的不足。

首先，论文指出传统的“感知-规划”流水线（见第1节）在结构化环境中有效，但严重依赖精确的物体识别和几何建模，在开放场景、动态变化或存在遮挡、杂乱的情况下，感知模块的失败会直接导致规划失败，限制了系统的灵活性与鲁棒性。

其次，尽管端到端的VLA模型（如RT系列、Octo、OpenVLA等，见第2.3节）通过整合感知、推理与动作生成，展现出强大的泛化能力，但它们存在根本性缺陷。论文明确指出（见第1节及图1(b)），现有VLA模型主要基于模仿学习，其训练数据通常仅包含成功的执行轨迹。这导致模型缺乏从偏离状态中恢复的能力，无法自主预测和纠正潜在失败。此外，收集大规模、标注了故障预测与校正的数据集本身就是一个瓶颈（见第3.4节），现有数据集多为事后标注，缺乏主动干预的能力。最后，许多方法在动作执行上存在信息丢失或处理不当的问题，例如仅执行预测动作序列的一部分，或简单地平均多模态动作，忽略了动作轨迹固有的多模态特性（即存在多种达成目标的合理方式）。

因此，论文的研究动机是构建一个既能保留VLA模型端到端泛化优势，又能主动预测、诊断并修正执行过程中潜在故障的鲁棒框架，以提升机器人在非结构化真实环境中的自主性和可靠性。

#### **3. 核心贡献与创新点**
本文提出了三项核心创新，每一项都针对了研究动机中指出的具体问题：

1.  **基于VLM的机器人故障预测与校正监督器**：这是框架最核心的创新。与现有工作（如RACER [31]仅提供失败后的任务级语言指令，或[29]仅识别失败原因）不同，FPC-VLA的监督器在**动作执行前**进行主动干预（见第3.5节及算法2）。其创新性在于：**a）触发机制**：仅在检测到夹爪状态即将改变的关键帧被激活（公式(17)），聚焦于高风险决策点。**b）结构化评估与校正**：监督器接收当前图像和精心设计的结构化提示（见表2），输出“是/否”判断。若为“否”，则生成包含**具体方向（如上、下、左、右）和幅度（大、小）** 的自然语言校正指令（如“No. Move down. Large. Rotate counterclockwise. Small.”）。**c）动作映射**：这些语言指令被映射回连续的机器人动作空间（∆p, ∆rz），用于直接修正VLA的原始输出（见第3.5节），实现了从语义反馈到底层控制的闭环。

2.  **自动化的大规模故障预测与校正数据集生成方法**：为解决数据瓶颈，论文提出了一种从现有RLDS格式的机器人数据中自动生成监督器训练数据的方法（见第3.4节）。其创新流程包括：**a）关键事件检测**：通过夹爪状态变化自动识别抓取/释放事件（公式(15)）。**b）校正目标生成**：为每个关键帧前的上下文窗口计算到达下一个关键事件所需的动作差值（∆a = ac* - at）。**c）离散化映射**：将连续的∆a通过阈值（δsmall, δlarge）映射为结构化的自然语言描述（如“Move left. Large”），如表1所示。这种方法无需人工标注，即可从海量成功轨迹中合成出“在何种场景下应采取何种校正”的问答对，为监督器的训练提供了可扩展的数据基础。

3.  **双流动作融合模块**：为了提升基础VLA模型输出动作的平滑性与时间一致性，并充分利用动作的多模态特性，论文设计了此模块（见第3.3节及算法1）。其核心创新在于**“双流”设计**：**a）姿态流**：对历史预测的姿态（pt）进行融合，权重由与当前预测的余弦相似度（公式(7)）经Sigmoid平滑（公式(8)）后，再乘以时间衰减（公式(9)）得到。这确保了融合倾向于与当前预测语义一致且较近的历史动作。**b）夹爪状态流**：对夹爪状态（gt）的融合**仅使用时间衰减权重**（公式(12)）。论文明确指出这是因为夹爪状态与末端姿态在语义和行为上是解耦的（例如，夹持物体时手臂仍可移动），使用基于姿态的余弦相似度来衡量夹爪状态是不合理的。这种分离处理方式更符合机器人动作的物理本质。

#### **4. 方法概述**
FPC-VLA的整体架构如图2所示，包含三个主要模块：基础VLA模型、双流动作融合模块和VLM监督器。其工作流程如下：

**基础VLA模型（第3.2节，图3）**：作为动作生成主干。它采用多模态编码器-解码器架构。**视觉编码器**融合了DINO和SigLIP的特征，并通过线性投影（公式(1)）与语言模型嵌入空间对齐。**语言模块**基于Llama2。视觉与语言token被拼接（公式(2)）后输入LLaMA，输出的认知token（Z）用于条件化**动作模块**。动作模块采用扩散Transformer（DiT），通过去噪过程生成未来多步动作序列（公式(3), (4)）。在推理时，从噪声开始，以Z为条件逐步去噪得到动作序列x0。

**双流动作融合模块（第3.3节，算法1）**：在每一时间步t，该模块收集基于过去N个观测（Ot-k）对当前时刻t做出的动作预测集合At（公式(5), (6)）。每个预测a(k)_t被解耦为姿态p(k)_t和夹爪状态g(k)_t。融合过程分两路：
1.  **姿态融合**：计算每个历史姿态p(k)_t与最新姿态p(0)_t的余弦相似度sim(k)（公式(7)），经Sigmoid缩放得到s(k)（公式(8)）。同时计算时间衰减权重d(k)（公式(9)）。最终姿态融合权重w(k)为s(k)和d(k)的乘积并归一化（公式(10)）。融合后的姿态ˆpt为加权和（公式(11)）。
2.  **夹爪状态融合**：权重仅使用归一化的时间衰减˜d(k)（公式(12)），对历史夹爪状态进行加权平均得到ˆgt。
最终输出动作为ˆat = [ˆpt, ˆgt]。

**VLM监督器的工作流程（第3.5节，算法2）**：当融合模块输出的动作ˆat中的夹爪状态gt与上一时刻gt-1的变化超过阈值δg时（公式(17)），触发监督器。监督器将当前图像It和根据任务定制的结构化提示Pt（包含任务要求、答案限制和示例，见表2）输入到一个预训练的VLM（如Qwen）中，得到自然语言响应Rt（公式(18)）。若Rt以“Yes”开头，则直接执行ˆat。若以“No”开头，则从响应文本中解析出平移校正（∆p）和旋转校正（∆rz），并将其映射为数值调整量，与ˆat相加得到修正后的最终动作a‘_t执行。

#### **5. 实验说明**
- **评估指标**：主要评估指标为**任务成功率**（Success Rate）。
- **数据集**：
    - **仿真**：使用SIMPLER基准（包含WidowX和Google Robot两个机器人平台的数据）和LIBERO基准（使用Franka机器人）。
    - **真实世界**：在Xiaomi Robot和ALOHA机器人平台上进行部署测试。
- **对比基线方法**：论文与多种先进的VLA模型进行对比，包括：**Octo-Base**, **OpenVLA**, **RT-

---

## 7. Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols

### 基本信息
- **作者**: Xianchao Zeng, Xinyu Zhou, Youcheng Li, Jiayou Shi, Tianle Li, Liangming Chen, Lei Ren, Yong-Lu Li
- **arXiv ID**: [oai:arXiv.org:2512.02787v2](https://arxiv.org/abs/2512.02787)
- **发布日期**: Thu, 04 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.02787)

            ### 原文摘要
            arXiv:2512.02787v2 Announce Type: replace  Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols》和严格的格式要求，生成一份详实的论文总结。

***

### **论文总结报告**

**1. 论文概要**
本文针对视觉-语言-动作模型在机器人操作中缺乏失败诊断与学习能力，以及现有失败数据集多为模拟生成、存在仿真到现实差距的问题，提出了ViFailback框架。该框架利用明确的视觉符号（如彩色箭头、准星）高效标注真实世界机器人操作失败视频，生成包含文本解释和视觉符号的视觉问答对。基于此，作者构建了包含58,126个VQA对的大规模真实世界数据集ViFailback，并设立了包含11个细粒度任务的ViFailback-Bench基准。通过在该数据集上微调Qwen3-VL-8B得到ViFailback-8B模型，该模型在基准测试中显著优于现有模型，并能生成用于纠正指导的视觉符号。最终，通过将ViFailback-8B与VLA模型集成，在真实机器人实验中验证了其能有效辅助策略从失败中恢复。

**2. 研究动机**
论文的研究动机源于当前机器人操作学习领域两个关键且相互关联的缺口。

首先，尽管基于模仿学习的视觉-语言-动作模型在通用操作上取得了显著进展（见第1节及参考文献[4, 19, 25]），但这些模型在面对分布外条件时容易失败，且缺乏诊断和从失败中学习的能力（见第1节）。现有VLA模型仅从成功数据中学习，形成了一个“盲点”：它们无法识别，更无法纠正失败（见第2.1节及引用[13, 36]）。因此，开发能够诊断、纠正并从失败中学习的机器人系统，对于实现开放世界中的鲁棒操作至关重要。

其次，现有提升VLM失败诊断能力的方法存在根本性局限。一些工作（如[8, 9, 27]）通过在仿真中注入扰动来生成大规模失败数据集，并用于微调VLM。然而，这种方法受限于仿真到现实的差距，在真实世界场景中的效率低下（见第1、2.2节）。事实上，在遥操作或策略执行过程中，机器人会产生大量失败数据，但对其进行高效、低成本的标注是一个重大挑战（见第1节）。此外，现有方法提供的反馈多为文本形式，而当前VLA模型脆弱的指令跟随能力（见参考文献[11, 24]）限制了机器人基于文本指导进行恢复的效果（见第2.2节）。同时，现有的机器人VLM基准（如Robo2VLM[5], ManipBench[40]）主要评估“做什么”和“怎么做”，而非“哪里出错”和“为什么出错”（见第2.2节）。

因此，本文旨在构建一个**基于真实世界失败数据**的、能够提供**可执行的视觉与文本双重指导**的失败诊断与纠正框架，以填补上述缺口。

**3. 核心贡献与创新点**
本文的核心贡献与创新点体现在方法论、数据集和模型应用三个层面，具体如下：

1.  **提出基于视觉符号的高效真实世界失败数据标注框架（ViFailback）**：这是本文最核心的概念创新。与在仿真中程序化生成失败数据不同，该框架直接利用真实世界失败轨迹（见第3.3.1节）。其创新性在于设计了**一套结构化的视觉符号系统**（见第3.1节及图2左部），包括运动符号（彩色直线箭头、半圆箭头）、空间关系符号（双准星、准星）和状态符号（开/关标签、禁止图标、回退图标）。这些符号允许标注者通过鼠标绘制直观地提供低层动作指导，极大简化了复杂、抽象概念（如失败原因、高层策略）的标注过程。VLM可以基于这些视觉线索，自动化生成相应的文本描述，从而实现了半自动化的高效数据标注流程（见第3.2、3.3.2节）。

2.  **构建大规模真实世界机器人失败诊断与纠正数据集及基准（ViFailback Dataset & Benchmark）**：基于上述框架，作者发布了包含**58,126个高质量VQA对**的ViFailback数据集，数据来源于5,202条真实世界操作轨迹，覆盖100个不同任务（见第3.3节）。更重要的是，作者从中提取并建立了**ViFailback-Bench基准**，该基准系统地定义了11个细粒度任务，全面评估VLM的失败诊断与纠正能力（见第3.3.3节及图3）。其创新性在于：a) **任务定义的完整性**：涵盖了从失败检测、关键帧定位、子任务定位、类型识别到原因分析的完整诊断链条，以及从低层动作到高层策略、从文本到视觉的全面纠正指导（见第3.2节）。b) **评估设置的层次性**：包含“Lite”（闭集问答，评估核心能力）和“Hard”（开集问答，评估复杂推理与思维链生成）两个互补设置，提供了更全面的模型能力评估维度（见第3.3.3节）。

3.  **训练能够生成视觉符号的VLM（ViFailback-8B）并验证其在真实机器人失败恢复中的有效性**：通过在ViFailback数据集上微调Qwen3-VL-8B，得到了ViFailback-8B模型（见第4.1节）。该模型的创新点在于**学会了生成用于绘制纠正视觉符号的代码**（见第4.2节，图4中“Generating Visual Symbols‘ Codes”任务）。这使得模型不仅能进行文本诊断，还能提供**可直接被下游策略或控制器理解的视觉提示**。作者进一步将ViFailback-8B作为外部监督器与VLA模型（π0.5）集成，在真实机器人实验中（见第4.3节），通过两种方式（VSF：微调VLA跟随视觉符号；PMC：使用底层控制器解析符号）利用生成的视觉符号引导机器人从失败中恢复，平均任务成功率提升了22.2%（见表4），实证了该框架在闭环失败恢复中的实用价值。

**4. 方法概述**
ViFailback框架是一个从数据收集、标注到模型训练与部署的完整技术方案，其运作流程紧密结合了上述创新点。

**A. 数据标注流水线（核心机制）**：如图2左侧所示，标注过程分为三个阶段（见第3.3.2节）：
1.  **基础语义信息填充**：标注者通过UI控件（滑块、按钮）完成失败诊断的基础标注，如失败检测、关键帧定位等。任务描述由Qwen2.5-Max预先分解为子任务。
2.  **文本指导选择与视觉符号绘制**：在选定的关键帧上，标注者从预定义类别中选择合适的动作纠正选项，并通过**鼠标拖拽绘制第3.1节定义的七类视觉符号**，以指示具体的纠正动作（如用红色箭头表示末端执行器需要向前/后移动）。
3.  **开集描述生成与精炼**：将前两阶段的所有标注信息（包括绘制的视觉符号）输入给强大的VLM（Qwen3-VL-235B），由其生成失败原因分析和高层文本指导的描述。这些描述经人工验证和精炼后，形成最终的VQA对。

**B. 模型训练与视觉符号生成**：使用LoRA技术对Qwen3-VL-8B进行微调（见第4.1节）。训练数据包含来自标注流水线的各种VQA对，特别是那些与**绘制视觉符号**相关的问答对。模型学习的目标不仅是回答关于失败的问题，还包括**生成能够复现人类标注者所绘视觉符号的代码**（例如，生成指定起点、终点、颜色和方向的箭头绘制代码）。这使得ViFailback-8B具备了“视觉绘图”能力，为后续的机器人干预提供了基础。

**C. 机器人失败恢复系统工作流**：如图2右侧及第4.3节所述，系统在机器人执行任务时并行运行：
1.  **监听与诊断**：VLA模型（π0.5）在文本任务提示下控制机器人。ViFailback-8B以固定间隔监听机器人的视频流（仅使用头部摄像头观测）。
2.  **失败检测与指导生成**：若ViFailback-8B检测到失败，它会生成一个包含思维链的诊断，并输出低层纠正指导，**特别是基于当前观测生成绘制视觉符号的代码**。
3.  **视觉提示注入与策略纠正**：生成的视觉符号被叠加到机器人摄像头视图上，并与纠正指导的文本提示一同输入给经过调整的π0.5模型（通过VSF或PMC方法使其能理解视觉符号），从而引导机器人执行纠正动作，从失败中恢复。

**5. 实验说明**
*   **评估指标**：
    *   **闭集问答（ViFailback-Bench Lite）**：使用准确率（%）作为评估指标（见第4.1节，表2）。
    *   **开集问答（ViFailback-Bench Hard）**：使用基于GPT-4o的评估器，从语义相似性、内容完整性和功能对等性三个维度对生成内容进行评分，并计算平均分作为总分（见第4.1

---

