# arXiv论文监控报告 - 2026年01月15日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2026年01月15日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 3篇

---

## 1. ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation

### 基本信息
- **作者**: Zhenyang Liu, Yongchong Gu, Yikai Wang, Xiangyang Xue, Yanwei Fu
- **arXiv ID**: [oai:arXiv.org:2601.08325v1](https://arxiv.org/abs/2601.08325)
- **发布日期**: Wed, 14 Jan 2026 00:00:00 -0500
- **分类**: cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.08325)

            ### 原文摘要
            arXiv:2601.08325v1 Announce Type: new  Abstract: Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook the importance of active perception: they typically rely on static, wrist-mounted cameras that provide an end-effector-centric viewpoint. As a result, these models are unable to adaptively select optimal viewpoints or resolutions during task execution, which significantly limits their performance in long-horizon tasks and fine-grained manipulation scenarios. To address these limitations, we propose ActiveVLA, a novel vision-language-action framework that empowers robots with active perception capabilities for high-precision, fine-grained manipulation. ActiveVLA adopts a coarse-to-fine paradigm, dividing the process into two stages: (1) Critical region localization. ActiveVLA projects 3D inputs onto multi-view 2D projections, identifies critical 3D regions, and supports dynamic spatial awareness. (2) Active perception optimization. Drawing on the localized critical regions, ActiveVLA uses an active view selection strategy to choose optimal viewpoints. These viewpoints aim to maximize amodal relevance and diversity while minimizing occlusions. Additionally, ActiveVLA applies a 3D zoom-in to improve resolution in key areas. Together, these steps enable finer-grained active perception for precise manipulation. Extensive experiments demonstrate that ActiveVLA achieves precise 3D manipulation and outperforms state-of-the-art baselines on three simulation benchmarks. Moreover, ActiveVLA transfers seamlessly to real-world scenarios, enabling robots to learn high-precision tasks in complex environments.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation》内容，生成一份符合要求的详细总结报告。

***

### **论文总结报告**

**1. 论文概要**
本文针对现有视觉-语言-动作模型在机器人精细操作任务中，因依赖固定视角摄像头而导致的感知受限问题，提出了一种名为ActiveVLA的新型框架。该框架通过向VLA模型中注入主动感知能力，使机器人能够动态调整观察视角和分辨率。其核心采用由粗到细的两阶段流程：首先定位3D场景中的关键区域，然后基于此进行主动视角选择和3D局部放大，以获取更优的观察信息，从而提升在遮挡、长视野和精细操作任务中的性能。实验在RLBench、COLOSSEUM和GemBench三个仿真基准以及真实机器人平台上验证了其有效性。

**2. 研究动机**
现有基于预训练视觉-语言模型的VLA方法（如BridgeVLA、RVT等）在机器人操作中展现出强大的泛化能力。然而，这些方法普遍存在一个根本性局限：它们依赖于静态的、通常安装在机械臂末端的摄像头，提供固定且以末端执行器为中心的视角（见第1节及图1）。这种被动感知模式使得模型无法根据任务上下文动态调整观察位置或分辨率，导致在复杂场景中（如存在遮挡、需要观察细小物体或进行长视野任务时）获取的信息不充分或不完整，从而限制了策略的鲁棒性和泛化能力（见第1节，引用了[16-18, 28, 38, 52]等文献）。

作者指出，尽管已有工作开始探索将3D模态（如点云）融入机器人学习（如3DVLA、PointVLA、SpatialVLA），但这些方法大多仍缺乏“主动”调整感知的能力（见第2节“3D Modalities in Robotic Learning”）。它们通常处理给定的3D数据，而非主动“选择”如何感知环境。因此，论文的核心动机是弥补这一缺口，将“主动感知”这一在具身智能中被认为是核心的能力（文中引用了Richard Gregory的观点）系统地整合到VLA范式中，使机器人能够像人类一样，通过主动调整“视线”来更好地理解和交互复杂的三维世界。

**3. 核心贡献与创新点**
本文的核心贡献在于提出并实现了一个将主动感知机制系统化集成到VLA框架中的新范式。具体创新点如下：

1.  **提出了首个集成主动感知的VLA框架（ActiveVLA）**：这是论文最核心的概念性创新。与以往处理固定输入（2D图像或3D点云）的VLA模型不同，ActiveVLA引入了一个闭环的、由粗到细的感知-行动流程，使机器人能够根据任务需求“主动地”决定观察什么以及如何观察（见第1节摘要及第3节引言）。这包括主动选择最优相机位姿（Active View Selection）和对关键区域进行虚拟光学变焦（Active 3D Zoom-in）。

2.  **设计了一种基于多目标优化的主动视角选择策略**：在方法层面，论文创新性地提出了一种用于评估和选择虚拟相机位姿的评分函数（见第3.2节，公式(7)）。该函数综合了三个关键目标：**可见性**（判断视线是否被遮挡，公式(5)）、**距离**（偏好适中的观察距离以平衡视野与细节）和**多样性**（确保所选视角在空间上分布广泛以减少感知歧义，公式(6)）。这种将主动感知形式化为一个可优化的多目标问题的做法，是该框架实现自适应观察的关键。

3.  **引入了虚拟3D放大机制**：为解决精细操作中分辨率不足的问题，论文提出了“Active 3D Zoom-in”机制（见第3.2节）。这并非物理相机的光学变焦，而是在虚拟渲染空间中，通过缩小渲染视场角（FoV）来对已识别的关键3D区域进行高分辨率重渲染（公式(8)）。这种方法利用已有的3D点云信息，在不损失几何细节的前提下模拟了“凑近看”的效果，从而为预测精细的末端执行器位姿提供了更丰富的视觉细节。

4.  **构建了一个结合3D空间推理与VLM语义理解的由粗到细管道**：在架构层面，论文设计了一个两阶段流程（见图2）。**粗粒度阶段**：将3D点云投影到多视角正交图像，利用预训练VLM（PaliGemma）生成2D热力图，并反投影回3D空间以定位关键区域（见第3.1节，公式(2)-(3)）。**细粒度阶段**：以上述关键区域为中心，执行主动视角选择和3D放大，并将优化后的视图再次输入VLM进行最终的动作预测（见第3.3节，公式(9)）。这种设计巧妙地桥接了3D几何信息与2D VLM的强语义理解能力。

**4. 方法概述**
ActiveVLA的框架运作流程严格遵循其由粗到细的两阶段设计，具体实现细节如下：

**第一阶段：3D关键区域感知（粗粒度）**
1.  **多视角渲染**：给定校准相机拍摄的RGB-D图像，系统首先重建场景的3D点云。为了适配预训练的2D VLM骨干网络（PaliGemma），将点云渲染为三个正交投影视图（顶视、前视、右视）。每个渲染图像包含7个通道：RGB、深度以及该像素对应点的世界坐标(x, y, z)（见第3.1节，公式(2)）。世界坐标通道用于建立不同视图间像素的3D对应关系。
2.  **热力图预测与3D定位**：将三个正交视图与语言指令一起输入VLM骨干网络。由于VLM的全局特征不足以精确定位，论文引入了一个**凸上采样模块**（Convex Upsampling Block）来从VLM输出的图像块（patch）令牌中恢复出细粒度的空间注意力热力图（公式(3)）。该模块学习像素级权重，比固定插值方法能恢复更精细的空间细节。训练时使用交叉熵损失来预测热力图。最后，将所有视图预测的2D热力图反投影回3D空间，通过投票或累积得分的方式确定最关键的3D区域（例如目标物体的质心）。

**第二阶段：3D主动感知与动作预测（细粒度）**
1.  **主动视角选择**：以第一阶段定位的关键点`pf`为中心，在一个球面上通过细分二十面体的方法均匀生成大量候选相机位姿`ci`（公式(4)）。对每个候选位姿，计算其多目标评分`si`（公式(7)）：
    *   **可见性评分**：从`ci`到`pf`的射线上采样点，计算其到场景点云的最短距离，若所有距离大于阈值`r`则视为无遮挡（公式(5)）。
    *   **距离评分**：对`ci`到`pf`的距离进行归一化，偏好中等距离。
    *   **多样性评分**：计算`ci`的观察方向与其他所有候选方向之间的总角度差（公式(6)），鼓励选择空间分布不同的视角。
    选择得分最高的K个视角作为主动观察的相机位姿。
2.  **主动3D放大**：对于选出的最优视角，保持相机位姿不变，但将渲染视场角（FoV）除以一个放大因子`z`（`z > 1`）进行重渲染（公式(8)）。这相当于在虚拟空间中进行了光学变焦，使得关键区域在图像中占据更多像素，从而提高了该局部区域的视觉分辨率，便于进行抓取点等精细位姿的预测。
3.  **3D动作预测**：将主动选择并可能经过放大的新视图（以及原始的正交视图）输入VLM，再次预测热力图。对于平移预测，将这些2D热力图反投影到3D网格上并累积形成得分体（公式(9)），得分最高的网格点即为预测的平移目标。对于旋转、夹爪状态和碰撞标志的预测，采用一个**分层特征融合模块**：通过最大池化获取每个视图的全局上下文令牌，同时使用ROI感知采样器从关键区域提取局部细节令牌，将全局与局部令牌拼接后通过MLP头进行预测（见第3.3节）。

**5. 实验说明**
*   **评估指标**：主要评估指标为任务**成功率**。在COLOSSEUM基准上还报告了**平均排名**（Avg. Rank），排名越低表示综合性能越好。
*   **数据集**：
    1.  **RLBench**：包含18个长视野和精细操作任务，使用Franka Panda机器人，每个任务提供100条演示。
    2.  **COLOSSEUM**：基于RLBench的鲁棒性评测基准，包含12种扰动类型（如物体颜色、纹理、大小、光照、背景、相机位姿变化等），共14个泛化场景。
    3.  **GemBench**：基于RLBench的分层组合式基准，包含16个训练任务和44个测试任务，分为

---

## 2. VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory

### 基本信息
- **作者**: Shaoan Wang, Yuanfei Luo, Xingyu Chen, Aocheng Luo, Dongyue Li, Chang Liu, Sheng Chen, Yangang Zhang, Junzhi Yu
- **arXiv ID**: [oai:arXiv.org:2601.08665v1](https://arxiv.org/abs/2601.08665)
- **发布日期**: Wed, 14 Jan 2026 00:00:00 -0500
- **分类**: cs.RO, cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.08665)

            ### 原文摘要
            arXiv:2601.08665v1 Announce Type: new  Abstract: VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文节选内容，为您生成一份符合要求的、详实的论文总结。

***

### **论文总结：VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**

#### **1. 论文概要**
本文提出VLingNav，一个用于具身导航的视觉-语言-动作模型，旨在解决现有VLA模型在复杂、长视野导航任务中缺乏显式推理能力和持久记忆的问题。核心创新在于引入了自适应思维链机制和视觉辅助语言记忆模块。前者使智能体能够根据任务复杂度动态切换快速执行与深思熟虑的规划模式；后者构建了一个跨模态的语义记忆，以支持长期空间依赖关系的处理。此外，作者构建了目前最大的具身导航推理数据集Nav-AdaCoT-2.9M，并采用在线专家引导的强化学习进行后训练。实验表明，VLingNav在多个标准具身导航基准测试中取得了最先进的性能，并能以零样本方式迁移到真实世界机器人平台。

#### **2. 研究动机**
论文的研究动机源于当前基于VLA的具身导航模型在迈向可靠现实世界部署时存在的两个关键能力缺口：**自适应推理能力**和**基于语言的长期记忆能力**。

首先，现有VLA导航模型（如NaVid、Uni-NaVid、NaVILA等）大多是反应式系统，采用固定的推理预算，无法在面对环境模糊性或任务复杂性时增加思考的深度（见第1节）。例如，OctoNav虽然引入了思维链，但需要手动配置固定的思考频率，这限制了模型根据情境动态调整推理强度的潜力（见第2.2节）。作者认为，借鉴人类认知的双过程理论（快思考与慢思考），智能体应具备**自适应推理**能力，在简单场景下快速反应，在复杂或不确定场景下则触发显式的、深思熟虑的规划。

其次，现有模型通常缺乏**持久、显式的语义记忆**结构。它们要么仅通过有限的上下文窗口隐式地保留历史视觉特征（如基于视频的VLA模型），要么采用基于潜在表示或地图的记忆，这些方式要么存在语义信息衰减问题，要么与VLA框架的语言模态对齐不佳（见第2.3节）。这导致智能体在长轨迹任务中难以追踪进度，容易陷入重复探索或循环行为，也无法有效处理动态环境中的目标移动趋势。因此，需要一种**以语言为基础、视觉为辅助的持久记忆**，来存储和回忆关键的环境语义信息。

此外，在训练范式上，大多数现有工作依赖模仿学习进行监督微调，这限制了模型超越专家演示的能力（见第1节）。虽然强化学习后训练在提升大语言模型和VLM性能上被证明有效，但在具身导航VLA模型中的应用仍处于初步阶段，且多局限于离散动作空间（见第2.4节）。因此，探索一种能够优化连续控制策略的在线RL后训练方法，以获取更鲁棒、自我优化的导航行为，是另一个重要的研究动机。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面：

1.  **提出了集成自适应思维链与视觉辅助语言记忆的VLingNav框架**（见第1、3.3节）：
    *   **自适应思维链机制**：这是本文的核心概念创新。与固定频率（如OctoNav）或无条件触发CoT的方法不同，AdaCoT赋予模型**自主决策何时进行显式推理**的能力。模型在每一步首先预测一个CoT指示符（`<think_on>`或`<think_off>`）。仅当输出`<think_on>`时，模型才会生成包含环境感知、任务分析、位置判断和动作决策的详细推理内容（`<think>...</think>`）以及对当前观察的环境总结（`<summary>...</summary>`）。这种设计实现了“快思考”与“慢思考”的动态平衡，在保证效率的同时提升了复杂场景下的决策质量。
    *   **视觉辅助语言记忆模块**：这是一种新颖的**跨模态、持久化、显式的记忆机制**。记忆的核心载体是语言形式的`<summary>`内容，它比隐式视觉特征或潜在表示更具语义清晰性和与VLM的兼容性。同时，该模块通过**动态采样与池化策略**（见第3.3.1节）对历史视觉特征进行选择性保留和压缩，为语言记忆的生成和更新提供关键的视觉辅助信息。VLingMem使智能体能够回忆过去访问过的区域（防止重复探索）并推断动态目标的移动趋势，从而处理长视野空间依赖。

2.  **构建了大规模自适应CoT数据集并引入了在线专家引导RL后训练范式**（见第4、5.2节）：
    *   **Nav-AdaCoT-2.9M数据集**：这是迄今为止**规模最大、任务最全**的具身导航推理标注数据集（见第4节及表1）。它不仅包含290万步的轨迹级动作标注，还包含47.2万个**自适应CoT标注**。这些标注通过一个自主的、基于Qwen2.5-VL-72B的流水线生成，并经过两阶段过滤和质量验证（见图4）。该数据集首次将物体目标导航、具身视觉跟踪和图像目标导航三大任务统一，并提供了“何时思考”和“思考什么”的灵活推理范式，是训练自适应推理能力的关键基础。
    *   **在线专家引导强化学习后训练**：在监督微调的基础上，本文引入了一个**在线专家引导的RL阶段**（见第5.2节）。与仅使用结果奖励的RL不同，该方法将专家演示作为先验知识融入RL框架，以**提高在线学习的效率和策略性能**。同时，VLingNav采用基于MLP的连续动作模型，克服了现有VLA-RL框架中离散自回归动作策略空间有限或基于流的连续动作推理速度慢的缺点（见第2.4节），实现了更高效、更灵活的连续控制策略优化。

3.  **在多个基准测试和真实世界部署中验证了卓越性能**（见第6节）：
    *   论文通过广泛的实验证明，VLingNav在HM3D、MP3D、OVON、EVT-Bench等多个具身导航基准测试的**成功率和效率指标上均达到了最先进的水平**，尤其在长视野推理和动态跟踪任务中优势显著。
    *   更重要的是，VLingNav展示了**强大的零样本跨领域和跨任务泛化能力**，能够不经任何额外微调，直接部署到真实世界机器人平台上，成功执行包括未见过的、未训练过的任务在内的多种实用导航任务（见图1），这验证了其语言驱动认知框架的有效性。

#### **4. 方法概述**
VLingNav以基于视频的VLM（LLaVA-Video-7B）为骨干，集成了一个动作模型，其整体架构和在线推理流程如算法1所示。方法运作的核心流程与创新点紧密结合，具体如下：

**A. 观察编码与动态采样池化**（见第3.3.1节）：
为了解决在线推理时视频帧序列过长导致的计算负担和相邻帧冗余问题，作者提出了**动态FPS采样策略**。该策略受艾宾浩斯遗忘曲线启发，根据历史帧与当前帧的时间间隔动态调整采样率：近期帧（短期记忆）采样率高，远期帧（长期记忆）采样率低（公式1）。采样后，使用预训练的视觉编码器（SigLIP-400M）提取特征。为了进一步压缩并保留关键信息，对历史视觉特征进行**基于时间间隔的网格池化**（公式2，3），时间间隔越久，池化步长越大。此外，为消除动态采样带来的时间不一致性，为每一帧引入一个**时间感知指示符令牌**，它通过RoPE编码绝对时间间隔信息（公式4），帮助模型理解观察的时间顺序。最后，通过一个两层MLP投影器将视觉特征映射到VLM的潜在空间。

**B. 自适应CoT与VLingMem的协同工作**（见第3.3.2节及图2）：
投影后的视觉令牌、语言指令令牌、时间感知令牌以及**从记忆库中取出的语言记忆令牌**被拼接为VLM的输入序列。VLM首先预测CoT指示符。若为`<think_on>`，则自回归生成两部分内容：
1.  **推理内容**（`<think>...</think>`）：对环境进行感知、分析任务、判断是否到访过当前位置、决定下一步动作。
2.  **环境总结**（`<summary>...</summary>`）：以简洁语言概括当前观察的关键语义信息。
生成的`<summary>`内容将被存入一个**持久化的语言记忆库**中，作为后续步骤的输入。这个记忆库是VLingMem的核心，它通过语言形式保留了跨时间步的语义上下文。而之前动态处理过的视觉特征则为生成准确、相关的`<summary>`提供了辅助。

**C. 连续动作生成**（见第3.3.3节）：
动作模型`A_θ(·)`是一个MLP，它将VLM骨干输出的最后一个预测令牌的隐藏状态向量`h_t^pred`作为条件，直接预测机器人的未来运动轨迹`τ_hat_t =

---

## 3. On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning

### 基本信息
- **作者**: Changyu Liu, Yiyang Liu, Taowen Wang, Qiao Zhuang, James Chenhao Liang, Wenhao Yang, Renjing Xu, Qifan Wang, Dongfang Liu, Cheng Han
- **arXiv ID**: [oai:arXiv.org:2601.06748v2](https://arxiv.org/abs/2601.06748)
- **发布日期**: Wed, 14 Jan 2026 00:00:00 -0500
- **分类**: cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.06748)

            ### 原文摘要
            arXiv:2601.06748v2 Announce Type: replace  Abstract: Vision-Language-Action models have recently emerged as a powerful paradigm for general-purpose robot learning, enabling agents to map visual observations and natural-language instructions into executable robotic actions. Though popular, they are primarily trained via supervised fine-tuning or training-time reinforcement learning, requiring explicit fine-tuning phases, human interventions, or controlled data collection. Consequently, existing methods remain unsuitable for challenging simulated- or physical-world deployments, where robots must respond autonomously and flexibly to evolving environments. To address this limitation, we introduce a Test-Time Reinforcement Learning for VLAs (TT-VLA), a framework that enables on-the-fly policy adaptation during inference. TT-VLA formulates a dense reward mechanism that leverages step-by-step task-progress signals to refine action policies during test time while preserving the SFT/RL-trained priors, making it an effective supplement to current VLA models. Empirical results show that our approach enhances overall adaptability, stability, and task success in dynamic, previously unseen scenarios under simulated and real-world settings. We believe TT-VLA offers a principled step toward self-improving, deployment-ready VLAs.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文节选内容，严格按照要求生成一份结构清晰、内容详实的论文总结。

***

### **论文总结报告**

**论文标题：** On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning
**作者：** Changyu Liu, Yiyang Liu, Taowen Wang, Qiao Zhuang, James Chenhao Liang, Wenhao Yang, Renjing Xu, Qifan Wang, Dongfang Liu, Cheng Han
**arXiv ID：** oai:arXiv.org:2601.06748v2

---

#### **1. 论文概要**
本文针对现有视觉-语言-动作模型在动态、未见过的部署环境中适应能力不足的问题，提出了一种名为TT-VLA的测试时强化学习框架。该框架的核心是在推理阶段，利用从环境交互中实时获取的、基于任务进度的稠密奖励信号，对预训练的VLA策略进行在线微调，而无需重新训练或人工干预。实验表明，该方法能有效提升多种主流VLA基座模型在模拟和真实世界动态场景下的任务成功率、适应性和稳定性。

#### **2. 研究动机**
当前主流的VLA模型主要通过监督微调或训练时强化学习进行优化（见第1、2.1节）。这些方法依赖于精心策划的静态数据集或受控环境，导致模型策略固定，缺乏在部署后应对环境动态变化（如物体位置移动、视觉外观改变、语义指令变化）的能力（见第1节，引用了Dulac-Arnold et al., 2021等）。一旦部署，模型无法根据实时反馈调整行为，在面对分布偏移时表现脆弱。

尽管在语言和视觉领域，测试时训练方法已被探索用于模型在线适应（见第2.2节，引用了Sun et al., 2020; Hu et al., 2025等），但作者指出这些方法无法直接应用于VLA领域。原因在于：1）机器人任务涉及感知、语言和动作的多模态交互，其分布偏移更复杂、动态性更强；2）纯粹基于自监督（如最小化困惑度）或基于输出共识的测试时目标，无法提供与任务目标对齐的有效学习信号（见第4.5节及表3的分析）。例如，TLM方法过度强调表征一致性而非决策质量，TTRL的多数投票奖励无法反映动作质量。

因此，论文的研究动机是填补VLA模型在**测试时自主、高效适应**这一关键空白，提出一个专门针对VLA多模态、交互式特性的测试时强化学习框架，使机器人能在单次任务执行过程中根据环境反馈实时优化策略。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点可归纳为以下三个方面：

1.  **提出了首个专为VLA设计的测试时强化学习框架（TT-VLA）：** 这是论文最核心的概念创新。与传统的训练-部署分离范式不同，TT-VLA允许预训练的VLA策略在**单次推理 episode 内**进行持续的在线策略微调（见第3.2节，图2）。它创造性地将测试时训练与强化学习结合，利用环境交互产生的实时奖励来闭合决策环路，实现了无需重训练周期、无需人工干预的“飞行中”适应。

2.  **设计了一种任务无关的、基于进度差的稠密奖励机制：** 这是实现有效测试时适应的关键技术创新。针对测试时数据极度有限、无法使用稀疏终端奖励的问题，作者提出利用一个预训练的任务进度估计器Φ（如VLAC模型），根据观测历史和指令实时估算当前任务进度pt ∈ [0, 1]（公式6）。每一步的奖励rt定义为相邻步的进度差：rt = pt - pt-1（公式7）。这种奖励设计具有三个关键属性（见第3.2节）：a) 无需外部监督，完全自主；b) 提供步进式的稠密反馈，支持中段策略修正；c) 鼓励单调进展，抑制倒退或振荡行为。

3.  **推导并应用了一种适用于测试时场景的“无价值函数”PPO变体：** 这是针对测试时约束的重要算法创新。作者从理论上分析了在进度差奖励和理想价值函数（V(st) = 1 - pt-1）的假设下，标准PPO的优势估计信号会消失或产生负偏差（命题1及推论1，见第3.3节）。鉴于测试时学习样本少、时延要求高，学习一个准确的价值函数不切实际。因此，TT-VLA移除了价值函数学习和熵正则项，将目标函数简化为仅包含裁剪策略损失（公式8）。同时，通过将GAE中的折扣因子γ和迹参数λ均设为0，将优势估计简化为即时奖励：Ât = rt（公式9）。这使得策略更新直接、快速地反映每一步动作对即时进展的贡献，非常适合测试时快速适应的需求。

#### **4. 方法概述**
TT-VLA方法的工作流程如下（结合图2及第3.2节）：

**A. 问题建模与组件：**
*   **问题建模：** 将机器人操作任务建模为部分可观测马尔可夫决策过程（公式1）。VLA策略πθ接收历史观测ot-H+1:t和语言指令l，输出动作at（公式2）。
*   **核心组件：** 1) **预训练VLA策略πθ**：作为基础策略；2) **进度估计器Φ**：预训练的多模态模型（如VLAC），用于根据观测历史和指令输出进度标量pt；3) **优化器**：采用上述“无价值函数”PPO变体。

**B. 在线适应流程：**
1.  **初始化：** 每个任务episode开始时，VLA策略接收初始观测o0和指令l。
2.  **交互与评估循环（对每个时间步t）：**
    *   **动作生成：** 策略πθ根据当前观测（和历史）生成并执行动作at。
    *   **进度与奖励计算：** 环境转移到新状态，产生新观测ot+1。进度估计器Φ根据截至ot+1的观测历史和指令l计算新进度pt+1（公式6）。随后计算当前步的稠密奖励rt = pt - pt-1（公式7，初始p-1可设为0）。
    *   **策略更新：** 使用计算得到的奖励rt，通过简化的PPO目标函数（公式8）和优势估计（公式9）计算策略梯度。采用LoRA等参数高效微调技术更新策略参数θ -> θ‘。更新后的策略πθ’ 将用于生成后续动作。
3.  **持续优化：** 上述过程在整个episode中持续进行，实现策略的在线、实时 refinement。

**C. 理论依据：**
第3.3节的理论分析为方法设计提供了支撑。命题1和推论1揭示了在进度差奖励下，使用标准价值函数和GAE会导致学习信号失效或产生偏差，从而**论证了移除价值函数的必要性**。引理1则形式化地说明了当γ=0且λ=0时，GAE退化为即时奖励，**为使用Ât = rt提供了理论依据**。这些分析共同证明了TT-VLA所采用的简化PPO形式在测试时适应场景下的合理性与优势。

#### **5. 实验说明**
*   **评估指标：** 任务成功率（%）。
*   **数据集/任务设置：**
    *   **模拟实验：** 在ManiSkill 3环境中进行拾放操作任务。评估涵盖三个泛化维度（见第4.1节）：
        *   **执行泛化：** 随机化机器人、物体、容器的初始姿态，并引入episode中物体重新定位的动态变化。
        *   **视觉泛化：** 改变动态纹理、未知桌面、图像级噪声。
        *   **语义泛化：** 引入未知物体/容器、指令复述、多物体/多容器任务、干扰容器等。
    *   **真实世界实验：** 在Franka Research 3平台上进行9个未见过的拾放任务，同样评估执行、视觉和语义三个方面的泛化能力（见图3）。
*   **对比基线方法：**
    *   **SFT-based VLAs：** Nora (基于Qwen-2.5-VL-3B)， OpenVLA (基于Llama-2-7B)。
    *   **Training-time RL-augmented VLA：** OpenVLA-RL (在OpenVLA基础上进行训练时RL微调)。
    *   **Advanced VLA：** TraceVLA (通过视觉轨迹提示增强时空推理)。
    *   **Test-time Adaptation Methods：** TLM (测试时自监督方法)， TTRL (测试时RL方法，基于输出共识奖励)。
*   **实验条件：** 论文中未明确说明训练、微调、推理所使用的具体GPU型号、数量及配置。对于实现细节，仅提及（见第4.1节）：使用LoRA进行微调（秩为{16, 32}），学习率从{1e-5, 5e-5, 1e-4}中选取，使用AdamW优化器，裁剪参数ϵ=0.2，每个episode固定为160步。

#### **

---

