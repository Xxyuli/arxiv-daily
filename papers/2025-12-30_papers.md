# arXiv论文监控报告 - 2025年12月30日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2025年12月30日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 3篇

---

## 1. Motus: A Unified Latent Action World Model

### 基本信息
- **作者**: Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao, Yao Feng, Chendong Xiang, Yinze Rong, Hongyan Zhao, Hanyu Liu, Zhizhong Su, Lei Ma, Hang Su, Jun Zhu
- **arXiv ID**: [oai:arXiv.org:2512.13030v2](https://arxiv.org/abs/2512.13030)
- **发布日期**: Mon, 29 Dec 2025 00:00:00 -0500
- **分类**: cs.CV, cs.LG, cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.13030)

            ### 原文摘要
            arXiv:2512.13030v2 Announce Type: replace-cross  Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文信息和要求，生成一份结构清晰、内容详实的论文总结。

***

### **论文总结：Motus: A Unified Latent Action World Model**

#### **1. 论文概要**
本文提出了一种名为Motus的统一潜在动作世界模型，旨在解决具身智能中模型功能碎片化的问题。现有方法通常将视觉语言理解、世界建模、视频生成和动作控制等功能分离建模，阻碍了多模态生成能力的统一和大规模异构数据的利用。Motus通过一种混合专家Transformer架构，整合了三个预训练专家模型，并引入基于光流的潜在动作表示，以像素级的“增量动作”来学习跨具身的可迁移运动知识。通过一个包含三阶段训练流程和六层数据金字塔的规模化训练方案，Motus在一个统一框架内实现了五种关键建模范式。实验表明，Motus在仿真和真实世界场景中均显著优于现有方法。

#### **2. 研究动机**
论文的研究动机源于构建通用具身智能体所面临的两个核心挑战（详见第3节）。

**第一，统一多模态生成能力的挑战。** 一个强大的具身智能体需要将场景理解、指令遵循、未来想象、结果预测和动作生成等一系列认知功能整合为一个统一整体。然而，现有方法将这些能力孤立建模，形成了五种独立的建模范式：视觉语言动作模型、世界模型、逆动力学模型、视频生成模型以及视频-动作联合预测模型（见第1节）。尽管已有工作（如UWM）尝试在单一扩散模型主干中统一这些范式，但它们通常是从头训练或基于较小基础模型构建，缺乏从大规模预训练视觉语言模型或视频生成模型中继承的、全面的视觉理解先验或物理交互先验（见第2.1节）。因此，如何在一个框架内统一建模视觉、语言和动作的多种分布，同时继承强大的通用多模态先验，成为一个亟待解决的问题。

**第二，有效利用大规模异构数据的挑战。** 具身智能需要从互联网视频、第一人称人类演示和多机器人轨迹等大规模异构数据中学习。然而，不同具身的动作空间在维度、范围和语义上差异巨大，且大多数视频数据缺乏动作标签，这使得动作专家难以进行大规模预训练以学习通用运动先验（见第3节）。现有方法主要依赖带标签的机器人轨迹，无法有效整合无动作标注但富含运动和物理交互线索的大规模视频数据，限制了动作专家学习通用运动先验的能力（见第2.2节）。因此，需要一种能够从视觉动态中学习、并能在不同具身间迁移的通用动作表示。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面：

**1. 基于混合专家Transformer的统一架构设计：** Motus提出了一种新颖的混合专家架构，将预训练的视频生成专家、视觉语言理解专家和动作专家通过共享的多头自注意力层（称为“三模型联合注意力”）连接起来（见第4.1节，图1）。与直接将观测和动作令牌拼接处理的统一世界模型不同，Motus允许每个专家保持独立的Transformer模块，仅共享注意力层。这种设计既保留了各专家的专业功能，避免了任务干扰，又实现了跨模态特征的有效融合，使不同来源的预训练知识能够互补（见第4.1节）。这是首次将成熟的预训练视觉语言模型和视频生成模型以这种方式整合进一个统一的具身模型框架。

**2. 基于光流的潜在动作表示与规模化训练方案：** 为了利用无标签的大规模视频数据，Motus创新性地引入了基于光流的潜在动作表示（见第4.2节）。该方法使用深度压缩自编码器将光流（由DPFlow计算）编码为低维的潜在向量，作为像素级的“增量动作”（见图3）。通过混合90%的无标签数据（用于自监督重建）和10%的带标签数据（包括任务无关数据和标准机器人演示）进行训练，并使用对齐损失（公式(2)中的第二项）将潜在动作分布与真实动作分布对齐（见第4.2节）。这使得动作专家能够从海量无动作标签的视频中学习通用运动先验。配合此表示，论文设计了一个系统性的三阶段训练流程（视频生成预训练、潜在动作统一训练、目标机器人微调）和六层数据金字塔（从网络数据到目标机器人任务轨迹），实现了从通用先验到具身特定知识的渐进式知识融合（见第4.3节，表1，图4）。

**3. 支持多模式推理的统一调度器与预测策略：** Motus借鉴UniDiffuser的思想，为视频和动作分配不同的修正流时间步和噪声尺度，形成了一个统一的调度器（见第4.1节）。这使得模型能够以统一的方式建模边缘分布、条件分布和联合分布，并能在推理时灵活切换至不同的具身模型模式（如VLA、世界模型、IDM等）。此外，针对动作分块预测导致的视频令牌远多于动作令牌的不平衡问题，论文提出了“动作密集-视频稀疏预测”策略（见图2），通过下采样视频帧率来平衡令牌数量，防止模型过度拟合视频预测而削弱动作预测能力，提升了训练和推理效率（见第4.1节）。

#### **4. 方法概述**
Motus的方法体系围绕其统一架构、潜在动作学习和规模化训练流程展开。

**模型架构与训练目标：** Motus的核心是一个包含三个专家的混合Transformer。生成专家采用Wan 2.2 5B视频基础模型；理解专家基于Qwen3-VL-2B视觉语言模型，取其最后一层的对应令牌作为输入；动作专家则构建了一个与Wan深度相同的Transformer块（见第4.1节）。三模型联合注意力机制是跨专家交互的关键。模型的训练目标基于修正流，同时预测未来视频块和动作块的速度场。总损失是视频观测损失和动作损失之和：`lθ = lθ_obs + lθ_action`（见第4.1节公式）。通过为视频和动作分配不同的时间步`τo`和`τa`，模型实现了多模态生成的协调。

**潜在动作学习流程：** 潜在动作的学习分为两步。首先，使用深度压缩自编码器学习光流的压缩表示。输入连续帧通过DPFlow计算光流并转为RGB图像，经DC-AE编码器压缩为4个512维的令牌，再通过一个轻量级编码器投影为14维的潜在动作向量`zt`（与典型机器人动作空间尺度匹配）。解码器则负责从潜在动作重建光流（见第4.2节，图3）。训练时，总损失`L`（公式(2)）包含光流重建损失`Lrecon`、潜在动作与真实动作的对齐损失`||areal - apred||2`以及KL正则化损失`LKL`。通过混合带标签和不带标签的数据，模型学习到既反映通用运动模式又与真实控制分布对齐的潜在动作。

**三阶段训练流程：**
1.  **阶段1（视频生成预训练）：** 仅微调视频生成专家，使用数据金字塔的第2、3、5层（第一人称视频、合成数据、多机器人任务轨迹），使其能根据语言指令和初始图像生成合理的未来任务视频序列，奠定物理交互先验基础。
2.  **阶段2（潜在动作统一训练）：** 冻结视觉语言理解专家，使用第2、3、4、5层数据，联合训练整个Motus模型（包含三个专家），以视频、语言和**潜在动作**作为监督目标。此阶段初始化动作专家，将运动知识嵌入潜在动作空间。
3.  **阶段3（目标机器人微调）：** 使用第6层（目标机器人任务轨迹数据）对完整模型进行监督微调，将学习到的先验完全适配到特定具身的动力学和运动学上，并使用**真实动作**进行监督（见第4.3节，表1）。

#### **5. 实验说明**
**评估指标：** 在仿真环境中，使用任务成功率（Success Rate）进行评估。在真实世界实验中，由于任务多为长视野、可分解的，采用了部分成功率（Partial Success Rate），通过将任务分解为子任务并分配分数来更细致地评估性能（见第5.3节，表4，表5）。

**数据集：**
*   **仿真：** RoboTwin 2.0基准测试中的50个代表性操作任务，分别在干净场景和随机化场景（随机背景、杂乱桌面、光照等）下进行评估（见第5.2节）。
*   **真实世界：** 在两个双臂机器人平台（AC-One和Agilex-Aloha-2）上执行一系列复杂任务，如折叠毛巾、用滴滤咖啡机制作咖啡、用研磨机研磨咖啡豆等（见第5.3节，图5）。

**对比基线方法：**
*   **π0.5：** 一个具有开放世界泛化能力的视觉-语言-动作模型（见第5.1节）。
*   **X-VLA：** 另一个先进的视觉-语言-动作模型（见第5.1节）。
*   **消融对比模型：** 论文还比较了“无预训练”（w/o Pretrain）和“仅阶段1预训练”（Stage1）的Motus变体

---

## 2. StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision

### 基本信息
- **作者**: Shengliang Deng, Mi Yan, Yixin Zheng, Jiayi Su, Wenhao Zhang, Xiaoguang Zhao, Heming Cui, Zhizheng Zhang, He Wang
- **arXiv ID**: [oai:arXiv.org:2512.21970v1](https://arxiv.org/abs/2512.21970)
- **发布日期**: Mon, 29 Dec 2025 00:00:00 -0500
- **分类**: cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.21970)

            ### 原文摘要
            arXiv:2512.21970v1 Announce Type: new  Abstract: Stereo cameras closely mimic human binocular vision, providing rich spatial cues critical for precise robotic manipulation. Despite their advantage, the adoption of stereo vision in vision-language-action models (VLAs) remains underexplored. In this work, we present StereoVLA, a VLA model that leverages rich geometric cues from stereo vision. We propose a novel Geometric-Semantic Feature Extraction module that utilizes vision foundation models to extract and fuse two key features: 1) geometric features from subtle stereo-view differences for spatial perception; 2) semantic-rich features from the monocular view for instruction following. Additionally, we propose an auxiliary Interaction-Region Depth Estimation task to further enhance spatial perception and accelerate model convergence. Extensive experiments show that our approach outperforms baselines by a large margin in diverse tasks under the stereo setting and demonstrates strong robustness to camera pose variations.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision》，生成一份结构清晰、内容详实的总结。

***

### **论文概要**

本文针对视觉-语言-动作模型在机器人操作中几何感知能力不足的问题，提出了一种名为StereoVLA的新型模型。该方法首次系统性地将双目立体视觉引入VLA框架，以获取鲁棒的空间线索。其核心创新在于设计了一个几何-语义特征提取模块，该模块融合了来自立体视觉的密集几何特征和来自单目视图的丰富语义特征。此外，论文还提出了一种辅助的交互区域深度估计任务，以增强模型对精细空间细节的感知。实验表明，StereoVLA在立体视觉设置下，于多种需要精确感知的操作任务上显著优于现有基线方法，并对相机位姿变化展现出更强的鲁棒性。

### **研究动机**

VLA模型通过整合预训练的视觉-语言模型，在机器人技能泛化和语义理解方面展现出强大潜力。然而，为了与VLM的预训练范式对齐，现有VLA模型（如OpenVLA）普遍依赖单目RGB图像作为视觉输入，这限制了其进行精确操作所必需的几何感知能力（见第I节）。为解决此问题，先前研究主要探索了三种补充传感器：腕部相机、深度传感器和额外的第三人称相机。但论文指出（见图1及第I节），这些方案均存在固有缺陷：腕部相机视野有限且易被遮挡，硬件增加碰撞风险；深度传感器对透明或镜面物体测量噪声大；多相机设置增加了硬件部署复杂性，且视图多样性会阻碍模型对相机位姿变化的泛化能力。

这些局限性促使作者重新审视受人类启发的感知方式：双目立体视觉通过视差提供鲁棒的空间线索，并能捕获全局场景而无需额外硬件。尽管立体视觉在计算机视觉领域已有成熟的基础模型（如FoundationStereo），但其在机器人学习框架，特别是VLA模型中的整合仍未被充分探索（见第II-C节）。现有工作仅在大型预训练数据集中引入了少量立体数据，而未系统评估立体线索带来的益处。因此，如何有效利用强大的立体视觉表征来增强VLA模型的几何感知能力，同时保持其泛化能力，成为一个关键的研究缺口。本文旨在填补这一空白，系统地探索并验证立体视觉在VLA模型中的价值。

### **核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下三个方面：

1.  **首次系统性地将双目立体视觉引入VLA框架，并提出StereoVLA模型**。与以往仅使用单目或多视角RGB的VLA工作不同（如OpenVLA、π0.5、GraspVLA），本文是首个专门为立体相机设置设计并全面评估的VLA模型（见第I、II节）。它论证了立体配置在任务性能、对相机位姿变化的鲁棒性以及部署简易性之间的最佳平衡（见第IV-B节及表II）。

2.  **提出新颖的几何-语义特征提取模块**。该模块的核心创新在于对来自不同视觉基础模型的特征进行精心选择和高效融合（见第III-A节及图2(b)）。
    *   **几何特征选择**：作者并未直接使用立体匹配模型的最终输出（如视差图），而是深入分析了FoundationStereo的内部结构，选择其**经过混合成本滤波模块处理后的成本体积V‘c**作为几何特征源（见第III-A节“Geometric Feature Extraction”部分）。选择依据是：V‘c是密集特征体积，蕴含丰富的几何信息；相比原始成本体积Vc，它通过注意力机制融入了长程空间关联；相比相关性体积Vcorr（仅包含匹配分数），它信息更丰富；且避开了迭代细化步骤带来的额外计算开销。消融实验（表I）证实了V‘c的优越性。
    *   **语义特征补充**：针对FoundationStereo缺乏语义信息的问题，模块从**左视图**中提取SigLIP（用于高级语义）和DINOv2（用于视觉细节）的特征（见第III-A节“Semantic Feature Extraction”部分）。消融实验表明，融合语义特征能显著减少抓取错误对象的情况（表I）。
    *   **高效特征融合**：为避免简单拼接视觉令牌序列导致计算量翻倍，模块采用**通道维度拼接**融合不同分辨率的特征图（先对FoundationStereo特征进行空间池化以对齐分辨率）。实验证明该方法在性能和效率上均优于序列拼接（图5）。

3.  **提出交互区域深度估计辅助任务**。为了在训练中进一步增强模型对精细几何信息的捕获能力，作者设计了一个与动作预测协同训练的辅助任务（见第III-B节）。其创新点在于**将深度估计的采样点限制在“交互区域”**，即包含夹爪和目标物体的区域（由物体2D边界框定义）。这与在整个图像上均匀采样的朴素方法形成对比。该设计鼓励模型专注于对操作至关重要的空间细节，提高了训练效率和最终性能。消融实验（图6）验证了该策略的有效性，表明监督模型感知关键空间细节是有益的。

### **方法概述**

StereoVLA的整体架构如图2(a)所示。其流程可概括为：立体图像对输入 → 几何-语义特征提取模块 → 视觉令牌 → 与大语言模型（InternLM-1.8B）联合处理 → 动作专家预测动作。

**1. 几何-语义特征提取模块（核心组件）**：
给定立体图像对 \(I_l, I_r \in R^{H \times W \times 3}\)，该模块执行以下步骤（见第III-A节）：
*   **几何特征提取**：将\(I_l, I_r\)输入FoundationStereo模型。该模型首先通过一元特征提取器计算单目特征\(f_l, f_r\)。随后，特征经过两个并行路径。本方法仅采用第一条路径：将\(f_l, f_r\)拼接形成4D成本体积\(V_c \in R^{C \times \frac{D}{4} \times \frac{H}{4} \times \frac{W}{4}}\)，其中D为最大视差范围。\(V_c\)随后经过一个基于注意力的**混合成本滤波模块**，输出过滤后的成本体积\(V‘_c\)。**本方法选择\(V‘_c\)作为几何特征**，并将其空间池化至步长为14的分辨率。
*   **语义特征提取**：仅从左视图\(I_l\)中，分别使用SigLIP和DINOv2模型提取语义和外观细节特征。
*   **特征融合**：将池化后的几何特征图与SigLIP、DINOv2的特征图在**通道维度进行拼接**，形成一个混合特征表示。随后通过一个MLP投影器生成最终的视觉令牌序列。这种方法避免了令牌序列的拼接，控制了计算开销。

**2. 模型训练与辅助任务**：
*   **主干与动作预测**：生成的视觉令牌与语言指令令牌一同输入预训练的InternLM-1.8B模型。利用模型中间产生的视觉-语言特征（键值缓存），一个3亿参数的动作专家通过**流匹配**方式预测以末端执行器位姿增量为表示的动作块（见第III节）。
*   **交互区域深度估计**：在训练阶段，模型被要求预测采样点\((x, y)\)的度量深度\(d\)。**采样策略是关键**：并非在全图均匀采样，而是仅在目标物体的2D边界框定义的交互区域内采样点。该任务的损失\(L_{depth}\)与动作预测损失\(L_{action}\)、边界框预测损失\(L_{bbox}\)、关键帧位姿预测损失\(L_{pose}\)共同构成总损失\(L\)（公式(1)）。四类任务的样本在训练中以5:2:2:1的比例进行平衡。

**3. 数据集与训练细节**：
由于现有大规模机器人数据集缺乏立体图像对，作者使用MuJoCo和Isaac Sim合成了500万条立体视觉的拾放操作轨迹（见第III-C节）。训练在32张NVIDIA H800 GPU上进行，批量大小为384，共训练16万步，学习率为1.6e-4（见第III-D节）。

### **实验说明**

**1. 评估指标**：
主要评估指标为**任务成功率**。为确保评估严谨性，论文设定了严格标准：每次试验只允许单次执行（一次抓取前闭合和一次抓取后张开）；禁用某些基线方法中使用的“夹爪粘滞”启发式方法；仅当任务被完全完成时才计为成功（见第IV-A.4节）。

**2. 数据集**：
*   **训练数据**：500万条自合成的立体视觉机器人操作轨迹（见第III-C节）。
*   **评估任务套件**（见第IV-A.1节）：
    *   **通用任务**：拾取/放置常见物体（如面包、玩具恐龙）、堆叠立方体和碗等。
    *   **条形物体抓取**：评估抓取笔、叉子等物体，测试物体在0°、45°、90°不同朝向下的性能。
    *   **中小型物体抓取**：抓取中型（3~5 cm）和小型（

---

## 3. LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models

### 基本信息
- **作者**: Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu
- **arXiv ID**: [oai:arXiv.org:2510.13626v3](https://arxiv.org/abs/2510.13626)
- **发布日期**: Mon, 29 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.CL, cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.13626)

            ### 原文摘要
            arXiv:2510.13626v3 Announce Type: replace  Abstract: Visual-Language-Action (VLA) models report impressive success rates on robotic manipulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively analyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models》内容，生成一份符合要求的、详实的论文总结。

***

### **论文概要**

本文对当前主流的视觉-语言-动作（VLA）模型进行了系统性、多维度的鲁棒性分析。研究通过在LIBERO基准测试中引入七个维度的受控扰动（物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声），揭示了这些模型在高基准分数下隐藏的脆弱性。核心发现包括：模型对相机视角和机器人初始状态变化极度敏感；对语言指令存在显著的忽视现象；在组合扰动下表现出负向的组合泛化缺口。基于此分析，作者构建了一个包含10,030个任务的诊断性基准LIBERO-Plus，并验证了基于泛化数据集的训练能有效提升模型鲁棒性。

### **研究动机**

当前VLA模型在标准化基准（如LIBERO）上报告了接近完美的成功率，但这些“头条数字”可能掩盖了模型在真实场景中应对环境变化时的根本性缺陷（见第1节）。作者指出，主流的评估方法（如Liu等人，2023；Li等人，2024c）侧重于静态、理想条件下的聚合成功率，未能捕捉模型在现实变化下的稳定性和可靠性。这种评估方式掩盖了模型处理任何现实任务场景中固有细微变化的能力不足（Wang等人，2025；Müller， 2019；Zhang等人，2024）。

具体而言，论文动机源于对现有VLA模型“虚假能力”的质疑。在固定相机角度或恒定光照下训练表现出色的模型，在面对视角轻微偏移或机器人初始配置微小变化时，往往无法泛化。这种差距对于必须整合多模态信息并在任何输入通道受到扰动时仍能保持连贯行为的VLA模型尤为严重。因此，论文旨在通过系统性的诊断分析，揭示当前VLA模型在看似强大的基准性能背后，其泛化能力的真实边界和内在脆弱性，从而挑战“高基准分数等同于真正能力”的假设，并推动评估实践向关注现实变化下的可靠性转变。

### **核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下四个方面：

1.  **系统性、多维度的VLA模型鲁棒性诊断框架**：论文首次在VLA领域提出了一个覆盖七个关键扰动维度的系统性分析框架（见第2.1节）。这七个维度（物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理、传感器噪声）全面涵盖了机器人操作任务中可能遇到的环境和任务参数变化。该框架超越了以往工作（如VLATest、COLOSSEUM等，见第7.2节及表3）通常只关注少数几个维度的局限，提供了更全面的脆弱性评估。

2.  **对VLA模型“语言忽视”现象的深入揭示与验证**：论文不仅发现语言扰动对模型性能影响最小（见第2.3节，表1），还通过精心设计的控制实验，深入探究并证实了其根本原因。通过“空白指令实验”（见第4.1节，图3a）和“目标替换实验”（见第4.2节，图3b），作者证明了当前VLA模型在很大程度上忽略了语言指令，其行为更接近于一个仅依赖视觉的模型（Vision-Action）。这一发现挑战了VLA模型“真正理解并遵循语言指令”的预设，揭示了模型架构或训练范式在模态融合上的深层缺陷。

3.  **提出并量化“组合泛化缺口”**：论文创新性地从统计学角度定义了“组合泛化缺口”（Compositionality Gap），用以衡量模型在应对组合扰动时的表现是否可分解为应对单个扰动的简单叠加（见第5.1节，公式(8)）。实验结果表明，该缺口普遍为负值（见第5.2节，图4），表明不同维度的扰动之间存在负向交互效应，共同施加时会引入超出独立效应之和的额外困难。这揭示了当前模型学习到的表征是纠缠的，缺乏捕捉高阶依赖关系以应对复杂现实变化的机制。

4.  **构建大规模、细粒度、自动化的诊断基准LIBERO-Plus**：基于上述分析，作者构建了LIBERO-Plus基准（见第6节，图6）。其创新性在于：**全面性**（覆盖7个维度、21个子组件）、**自动化**（自动生成大量多样化任务）、**细粒度**（根据模型表现将任务划分为L1-L5五个难度等级，见图5）。该基准不仅是一个评估工具，其构建的泛化数据集（超过20,000条轨迹）还被用于模型训练，并验证了基于此数据集的微调能显著提升模型（特别是对相机视角变化）的鲁棒性（见第6.2节，表2），为改进模型提供了可行路径。

### **方法概述**

本文的方法论核心是一个“分析-诊断-构建”的闭环流程，具体运作如下：

**第一阶段：系统性扰动分析与诊断（第2-5节）**
1.  **扰动施加**：在LIBERO基准的评估回合中，独立且系统地施加七类单维度扰动。每类扰动有具体的操作定义，例如“相机视角”变化包括改变第三人称相机的位姿和视野；“物体布局”分解为添加干扰物体和改变目标物体位置两个子类（见第2.1节及附录A）。
2.  **模型评估**：选取10个具有代表性的开源VLA模型（涵盖自回归、扩散、世界模型、强化学习等不同架构和训练范式，见第2.2节及附录B），在施加了各类扰动的任务上进行评估，记录任务成功率。
3.  **深度调查实验**：
    *   **物体语义理解**：通过对比“添加干扰物”和“位移目标物”下的性能差异（见第3节，图1），分析模型是关注物体语义还是记忆位置。
    *   **光照鲁棒性根源**：设计“全黑”（所有相机输入为黑帧）和“第三人称黑”（仅屏蔽第三人称视图）的极端消融实验（见第3节，图2），探究模型抵抗光照变化的机制是否源于腕部相机提供的稳定几何线索。
    *   **语言指令跟随**：进行“空白指令”和“目标替换”实验（见第4节，图3），直接测试模型对语言模态的依赖程度和真实指令跟随能力。
    *   **组合泛化分析**：对选定的扰动维度（排除语言）进行两两组合测试。基于公式(1)-(8)的统计框架，计算实际联合成功率与基于独立性假设的预期成功率（即两个单维度成功率的乘积）之间的差值，即组合泛化缺口Δij（见第5节）。

**第二阶段：基准LIBERO-Plus的构建与验证（第6节）**
1.  **基准构建**（见第6.1节）：基于第一阶段的分析发现，对原始LIBERO基准进行系统性扩展和丰富，应用七类扰动因素，并通过过滤和平衡得到任务池。然后，使用四个代表性模型在该任务池上评估，根据模型的准确率分布将任务划分为五个难度等级（L1最简单，L5最难），最终形成包含10,030个任务的LIBERO-Plus基准（结构见图6）。
2.  **训练验证**（见第6.2节）：利用自动化流程，构建一个包含超过20,000条成功轨迹的泛化训练数据集。以此数据集对OpenVLA-OFT_m模型进行混合微调（+PT）。最后，在LIBERO-Plus基准上评估微调后的模型，并与所有基线模型对比，验证泛化数据训练的有效性（结果见表2）。

### **实验说明**

*   **评估指标**：核心评估指标为**任务成功率（Task Success Rate, %）**。在组合泛化分析中，使用了基于条件概率和协方差的**组合泛化缺口（Δij）** 作为量化指标。
*   **数据集**：核心实验基于**LIBERO**基准（Liu等人，2023）进行。本文构建的新基准为**LIBERO-Plus**，包含10,030个任务，涵盖7个扰动维度的21个子组件。
*   **对比基线方法**：论文评估了10个开源VLA模型，按架构/范式可归类如下：
    *   **自回归模型**：OpenVLA及其变体（OpenVLA-OFT, OpenVLA-OFT_w, OpenVLA-OFT_m）、π0、π0-fast、Nora、UniVLA、RIPT-VLA。
    *   **扩散模型**：π0（亦被提及为扩散模型）。
    *   **世界模型**：WorldVLA。
    *   （注：部分模型如π0在文中被不同章节引用为不同范式，这反映了其架构的复合性。）
*   **实验条件**：论文中未明确说明训练、微调、推理所使用的GPU具体型号、数量及配置。对于分析实验，仅提及进行了大量重复试验以确保统计显著性（如组合泛化实验进行了2000次独立重复）。对于LIBERO-Plus上的训练，仅说明从官方OpenVLA-OFT权重开始进行混合微调，未给出硬件细节。

### **改进建议和未来研究方向**

基于本文的研究方法、实验结果和

---

