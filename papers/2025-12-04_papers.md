# arXiv论文监控报告 - 2025年12月04日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2025年12月04日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 8篇

---

## 1. Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach

### 基本信息
- **作者**: Siyuan Yang, Yang Zhang, Haoran He, Ling Pan, Xiu Li, Chenjia Bai, Xuelong Li
- **arXiv ID**: [oai:arXiv.org:2512.02834v1](https://arxiv.org/abs/2512.02834)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.02834)

            ### 原文摘要
            arXiv:2512.02834v1 Announce Type: cross  Abstract: Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文信息，生成一份符合顶级会议风格、结构清晰且内容详实的论文总结。

***

### **论文概要**

本文针对基于流匹配或扩散目标的视觉-语言-动作模型在监督微调后存在的推理时不稳定问题。作者指出，该问题源于预训练模型学到的冗余动作模式与下游任务成功模式之间的分布偏移。为此，本文提出了一种名为TACO的测试时缩放框架，该框架将离线强化学习中的“反探索”原则引入VLA推理过程。TACO利用一个轻量级的伪计数估计器作为验证器，从多个候选动作块中选择伪计数最高的动作执行，从而在不修改模型参数的情况下，将策略输出约束在成功模式的支持集内。实验在四个模拟基准和一个真实世界双臂平台上验证了该方法能显著提升多种VLA模型的推理稳定性和任务成功率。

### **研究动机**

VLA模型通过在大规模多模态数据集上进行预训练，展现出强大的泛化能力。然而，当这些模型在特定下游任务的小规模数据集上进行监督微调后，其推理性能表现出严重的不稳定性（见第1节图1）。例如，仅因采样不同的初始噪声向量，同一微调模型在相同任务上的成功率可能从0%波动至80%。作者深入分析了这一现象的根源（见第1.1节及第4.1节），认为主要原因有二：

1.  **预训练引入的冗余模式**：预训练阶段，VLA模型从多样化的数据源（如不同的人类遥操作、脚本策略）中吸收了广泛的动作模式。这使得模型难以通过有限的下游任务数据，迅速将其输出分布收敛到任务成功所必需的狭窄行为集合上。因此，微调后的策略分布仍保留了大量与任务成功无关的冗余模式。
2.  **微调数据集本身的多模态性**：下游任务的微调数据集通常由多个操作者或不同执行风格收集而来，其中可能包含次优或不理想的策略。这些数据中的冗余模式同样会污染学习到的策略。

这种冗余模式导致了VLA策略与下游任务理想成功模式策略之间的**分布偏移**。在基于流匹配或扩散的生成框架中，采样不同噪声向量会触发不同的模式，从而放大了推理时的不稳定性。现有工作（如ATE）尝试在训练时通过潜在对齐或分类器引导来解决分布偏移，但忽略了推理时的不稳定性这一关键且常被忽视的问题（见第2节“Vision-Language-Action Models”部分）。本文的动机正是要系统地诊断这一推理不稳定性，并将其建模为一个**支持集外**问题，进而提出一种无需梯度更新、仅在测试时生效的校正机制。

### **核心贡献与创新点**

本文的核心贡献在于提出并实现了一个将离线RL的“反探索”原则应用于VLA模型测试时推理的框架。具体创新点如下：

1.  **将VLA推理不稳定性形式化为支持集外问题，并引入反探索原则**：本文首次明确将微调后VLA的推理不稳定性归因于策略生成了下游任务成功模式支持集外的动作（见第4.1节）。受离线RL中“反探索”原则（Rezaeifar et al., 2022）的启发，作者提出了一个理论目标：选择在微调数据集`D_sft`中访问计数最高的动作（公式(6)）。这为后续的测试时选择方法提供了理论依据，使其从一个启发式方法升华为一个原则性的实现。
2.  **提出TACO：一个无需梯度、基于伪计数的测试时缩放框架**：本文提出了TACO框架，其核心创新在于**不修改VLA模型参数**，仅通过测试时增加计算量（采样多个候选动作）并利用一个轻量级验证器进行选择，来实现反探索（见第4.3节）。这与需要复杂RL训练或大型验证器的方法（如RoboMonkey）形成鲜明对比。TACO的“生成-验证”流程（图2）保持了预训练VLA的强泛化能力，同时通过选择机制约束了输出。
3.  **设计了一种面向去噪式VLA的耦合伪计数估计器**：为了估计动作的伪计数（即公式(6)中的`N_Dsft`），本文创新性地采用了**耦合设计**的Coin-Flipping网络（见第4.2节）。关键点在于：CFN的输入并非原始观测-动作对，而是**VLA模型自身在去噪过程中产生的内部特征表示** `h_θ`。这种设计高效地利用了VLA已有的强大表征能力。此外，针对扩散/流模型在训练时只见过带噪动作的问题，作者提出了**高保真特征搜索**流程（公式(7)-(8)），通过向VLA输入不同噪声水平的动作并选择预测结果最接近真实动作的特征，来确保用于训练CFN的特征既在VLA的特征分布内，又能高保真地代表干净动作。
4.  **实现了高效的KV缓存优化，使测试时缩放实用化**：为了抵消因并行采样多个候选动作带来的计算开销，本文提出了一项关键的工程优化（见第4.3节“Enabling Efficient Inference via KV Cache Optimization”）。由于所有候选动作共享相同的观测和语言指令上下文，TACO预先计算并缓存Transformer中的Key和Value，在后续并行生成动作时重复利用。如图4（左）所示，该优化在采样32个动作时将推理时间降低了73.2%，使得高性能的反探索采样在实际部署中成为可能。

### **方法概述**

TACO方法分为两个阶段：**CFN训练阶段**和**测试时缩放推理阶段**。整体流程见图2。

**第一阶段：基于SFT数据训练耦合伪计数估计器（CFN）**
1.  **数据准备**：对于微调数据集`D_sft`中的每个数据点`(o, l, a)`，执行**高保真特征搜索**：
    *   将真实动作`a`添加`N`个不同噪声级别`{σ_i}`的噪声，得到`{a_σ_i}`（公式(7)）。
    *   将`(o, l, a_σ_i)`输入已微调的VLA模型，得到去噪后的动作预测`a_pre^(i)`和对应的内部特征`h_θ^(i)`。
    *   选择预测动作与真实动作`a`的L2距离最小的那个特征`h_θ^(i*)`作为该数据点的高保真特征（公式(8)）。
2.  **CFN训练**：用上述方法为`D_sft`中所有数据生成高保真特征集`D_h`。CFN `f_ϕ`是一个轻量级MLP，以特征`h_θ`为输入。训练目标是最小化公式(1)所示的回归损失，即让`f_ϕ(h_θ)`拟合一个随机生成的`d`维二元向量`c_i`（±1）。根据CFN理论（公式(2)），训练完成后，特征`h_θ`的伪计数与`1/||f_ϕ(h_θ)||^2`成正比（公式(9)）。

**第二阶段：测试时反探索推理**
1.  **并行生成候选动作**：给定当前观测`o_t`和指令`l`，采样`M`个不同的初始噪声向量`{ε_i}`，利用**KV缓存优化**（共享`(o, l)`的KV计算），批量并行执行VLA的完整去噪过程，得到`M`个候选动作块`{â_(t:t+H)^(i)}`及其对应的内部特征`{h_θ^(i)}`。
2.  **验证与选择**：将每个候选特征`h_θ^(i)`输入训练好的CFN `f_ϕ`，计算其伪计数`1/||f_ϕ(h_θ^(i))||^2`。根据反探索原则（公式(6)），选择伪计数最高的候选动作作为最终执行的动作：
    `â_(t:t+H)* = â_(t:t+H)^(i*), 其中 i* = argmax_i 1/||f_ϕ(h_θ^(i))||^2`。
    该过程直接实现了理论目标，即从VLA的多模态分布中确定性地选择最接近下游任务成功模式支持集（表现为高伪计数）的动作。

### **实验说明**

**评估指标**：主要评估指标为**任务成功率**。

**数据集**：
*   **模拟基准**：
    1.  **RoboTwin 1.0 & 2.0**：专注于双爪操作的基准，包含多种资源和任务类型。
    2.  **SimplerEnv (Simpler-WindowX)**：基于SAPIEN和ManiSkill2的仿真基准。
    3.  **LIBERO**：终身学习决策基准，本文专注于最具挑战性的`LIBERO-long`任务套件。
*   **真实世界平台**：在**RealMan75双臂机器人**平台上构建了5个长视野任务（接收书本、收纳充电器、纸笔清理、笔记本电脑收纳、同时取书）。

**对比基线方法**：
*   **主流VLA模型**：作为性能基准，包括`RT-1-X`, `Octo`, `Rob

---

## 2. VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling

### 基本信息
- **作者**: Weiqi Li, Quande Zhang, Ruifeng Zhai, Liang Lin, Guangrun Wang
- **arXiv ID**: [oai:arXiv.org:2512.02902v1](https://arxiv.org/abs/2512.02902)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.AI, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.02902)

            ### 原文摘要
            arXiv:2512.02902v1 Announce Type: cross  Abstract: Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文信息，生成一份符合要求的详细总结。

***

### **论文概要**

本文探讨了视觉-语言-动作（VLA）模型在视觉扰动下的泛化能力问题。研究发现，模型在遇到新相机视角时性能急剧下降，其主要原因并非物理建模（任务推理与控制）能力不足，而是**空间建模**（视觉编码器产生的空间表征）与下游模块的**表征错位**。为此，论文提出了一种**单样本鲁棒性适应框架**，包含两种轻量级方法：特征令牌调制（FTM）和特征线性适应（FLA）。该方法仅需极少参数更新，即可在LIBERO-V基准测试中显著恢复模型对新视角及其他视觉扰动的鲁棒性，揭示了预训练VLA模型中未被充分利用的泛化潜力。

### **研究动机**

VLA模型在分布内任务上表现出色，但在面对现实世界中不可避免的分布外视觉扰动（如未见的相机视角、光照变化）时，其性能会急剧下降，这严重限制了其实用性（见第1节）。现有提升鲁棒性的方法主要分为两类，但均存在明显不足。

**数据中心化方法**（如Libero-Plus）通过收集大规模、多视角的机器人数据来增加视觉多样性。然而，这种方法成本高昂、扩展性差，难以适应持续变化的真实环境（见第1节）。**表征中心化方法**则致力于学习几何一致或3D感知的表征（如GeoAware-VLA, Adapt3r），以提高视角不变性。但论文指出，这些方法尽管具备几何感知能力，仍对任务无关的视觉因素（如背景杂乱、光照变化）敏感，表明其在视角鲁棒性与通用视觉适应性之间存在差距（见第1节）。

更重要的是，先前工作隐含地假设鲁棒性需要额外的数据或复杂的3D感知架构，但**鲜有研究明确识别性能下降的根本原因是否源于空间表征本身**（见第1节）。为此，本文采纳了一个概念性框架，将VLA模型分解为**空间建模**（视觉编码器，负责从图像中构建物体间的空间关系）和**物理建模**（VLM和动作专家，负责整合语言、空间表征和动作历史进行推理与控制）（见图1(a)及第1节）。作者推断，视角变化主要改变了观察到的场景空间配置，而非底层任务语义或动作动力学。因此，性能下降很可能源于空间建模产生的表征与物理建模组件之间的**错位**，而非物理建模组件本身的能力缺陷。这一假设构成了本研究的核心动机：**通过极轻量级的、针对空间建模的适应来纠正这种表征错位，从而高效地恢复模型的鲁棒性**。

### **核心贡献与创新点**

本文的核心贡献在于系统性地诊断了VLA模型鲁棒性问题的根源，并提出了高效、轻量级的解决方案，具体创新点如下：

1.  **对VLA模型鲁棒性根源的重新评估与诊断**：论文首次明确地将VLA模型在视觉扰动下的性能下降归因于**空间建模模块的表征偏移**，而非物理建模能力的不足（见第1节及第3节开篇）。这一诊断性见解是通过将模型概念性分解并设计针对性实验（如FTM）来验证的。实验表明，仅对视觉令牌进行全局仿射变换（FTM）就能将新视角成功率从48.5%大幅提升至87.1%，这强有力地支持了“错位假说”，而非“能力不足假说”（见表1及第3.3节）。

2.  **提出统一的单样本鲁棒性适应框架**：基于上述诊断，论文提出了一个无需大规模数据或复杂架构重训练的适应框架。该框架的核心创新在于其**极致的参数效率**和**针对性**，仅更新视觉通路上的少量参数（见第3节）。框架包含两种具体方法：
    *   **特征令牌调制（FTM）**：这是一种概念上极其简单但效果显著的新方法。它仅在视觉编码器的输出令牌上施加一个**全局可学习的仿射变换**（缩放γ和偏移β），公式为 `ˆF = (1 + γ) ⊙ F + β`（见公式(4)及图3(d), 4(a)）。其创新性在于，它验证了仅通过重新校准特征分布的均值和方差，就能有效纠正由视角变化引起的表征分布漂移。FTM仅引入`2 * D_ViT`（约4K）个参数，是参数效率的极致体现。
    *   **特征线性适应（FLA）**：作为FTM的深化，FLA将低秩适应（LoRA）技术**专门且仅应用于视觉编码器（ViT）内部的线性层**（见第3.4节及图3(e)）。其创新点在于将参数高效微调（PEFT）的焦点从整个VLA模型或策略模块，**精准定位到被认为是问题根源的空间建模组件（视觉编码器）本身**。通过低秩分解 `ΔW = BA` 更新线性层权重（见公式(5)），FLA能以极少的参数量（4.7M）实现深层的特征重对齐。

3.  **构建LIBERO-V（Visual）基准测试**：为了系统评估模型对各种视觉扰动的鲁棒性，论文整合了LIBERO的任务和Libero-Plus的扰动，创建了LIBERO-V基准（见第4.1节及图4(b)）。该基准涵盖了**相机视角、光照、背景纹理和图像噪声**四种可控扰动类型，为全面、统一地评估VLA模型的视觉鲁棒性提供了标准化的测试平台。

4.  **揭示了预训练VLA模型的潜在鲁棒性**：通过FTM和FLA的成功，论文得出了一个重要结论：现有的预训练VLA模型**本身已编码了潜在的鲁棒性**，但其激活被空间建模的表征错位所阻碍。通过极轻量级的、针对性的适应机制（而非增加数据或模型容量），即可“解锁”这种潜在的泛化能力（见第5节）。这一发现挑战了“鲁棒性需要更多数据或更大模型”的常见假设。

### **方法概述**

本文方法的核心思想是：鉴于性能下降主要源于空间建模（视觉编码器）的输出与物理建模组件的输入之间的表征错位，因此只需对视觉通路进行轻量级适应，而保持VLM和动作专家模块冻结。

**1. 基础框架与问题形式化**：
论文以π0.5 VLA模型为基础策略（见第3.1节）。该策略在时间步t接收视觉观测v_t和语言指令l，通过视觉编码器f_v(·)生成视觉令牌序列z，语言编码器f_ℓ(·)生成语言嵌入ℓ，最后由多模态Transformer解码器g(·)基于拼接序列`[z; ℓ]`自回归地预测动作（见公式(1),(2)）。适应阶段，仅引入轻量级可学习参数φ，通过变换A_φ(·)作用于视觉表征，预测分布变为 `P_θ,φ(a_t | ...) = g( a_<t; [ A_φ(f_v(v)); ℓ] )`（见公式(3)）。A_φ(·)即代表下文提出的FTM或FLA模块。

**2. 特征令牌调制（FTM）运作流程**：
FTM作用于视觉编码器输出的令牌序列`F ∈ R^(N×D_ViT)`（见第3.3节）。
*   **机制设计**：学习两个全局向量：缩放因子`γ ∈ R^(D_ViT)`和偏移量`β ∈ R^(D_ViT)`。这两个向量与批次和令牌序列长度无关，是跨所有空间位置和样本共享的。
*   **变换操作**：对每个视觉令牌的特征向量，执行逐元素的仿射变换：`ˆF = (1 + γ) ⊙ F + β`（公式(4)）。`(1+γ)`的设计确保初始状态（γ=0）时变换为单位映射。
*   **训练与推理**：在单样本适应阶段，仅优化γ和β，视觉编码器f_v、语言编码器f_ℓ和解码器g的参数全部冻结。优化完成后，ˆF将替代原始的F输入给下游解码器。FTM仅引入2*2048=4096个参数，实现了极致的轻量化。

**3. 特征线性适应（FLA）运作流程**：
FLA将LoRA机制应用于视觉编码器内部的线性层，进行更深度的适应（见第3.4节）。
*   **机制设计**：对于视觉编码器中任意一个线性层`h = Wx`（W为原始冻结权重），引入低秩适配器。适配器表示为`ΔW = BA`，其中`A ∈ R^(r×d_in)`, `B ∈ R^(d_out×r)`，秩r远小于d_in和d_out。
*   **变换操作**：前向传播时，该线性层的计算变为 `h = Wx + BAx`。仅A和B为可训练参数，原始W保持不变。
*   **应用范围与训练**：论文将LoRA适配器应用于π0.5的SigLIP视觉主干中的所有线性层（具体哪些层

---

## 3. AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention

### 基本信息
- **作者**: Lei Xiao, Jifeng Li, Juntao Gao, Feiyang Ye, Yan Jin, Jingjing Qian, Jing Zhang, Yong Wu, Xiaoyuan Yu
- **arXiv ID**: [oai:arXiv.org:2511.18960v2](https://arxiv.org/abs/2511.18960)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.LG, cs.CV, cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2511.18960)

            ### 原文摘要
            arXiv:2511.18960v2 Announce Type: replace  Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention》内容，生成一份符合顶级会议风格的详细总结。

***

### **论文概要**

本文针对现有视觉-语言-动作模型在处理动态序列决策任务时，因将每个时间步的视觉输入视为独立、隐含地建模为马尔可夫决策过程而忽略历史上下文的问题，提出了一种名为AVA-VLA的新框架。该框架从部分可观测马尔可夫决策过程的视角出发，引入循环状态作为智能体信念状态的神经近似，并设计了一个主动视觉注意力模块。该模块利用循环状态动态调制当前帧的视觉处理，使模型能够基于历史信息主动聚焦于任务相关的视觉区域。实验表明，AVA-VLA在LIBERO和CALVIN等机器人基准测试中达到了最先进的性能，并在真实世界双手机器人平台上验证了其实际应用性和鲁棒的仿真到现实迁移能力。

### **研究动机**

现有基于预训练视觉-语言模型构建的VLA模型，通常将每个时间步的视觉帧作为独立输入进行处理（见第1节及公式(3)）。这种设计隐含地将机器人操作任务建模为马尔可夫决策过程，即假设当前视觉观测包含了完整的世界状态。然而，在真实的机器人操作场景中，当前视觉帧仅是对环境真实状态的部分观测，其中包含了跨时间序列的非可观测动态（如内部状态、遮挡信息）。这种忽略历史上下文的方法，对于动态、序列化的决策过程是次优的（见第1节）。

这种基于MDP假设的局限性对VLA模型的视觉处理能力产生了显著影响。VLA建模的本质是构建一个动态反馈控制系统，智能体先前的动作会直接改变其当前的视觉输入。然而，通过独立处理帧，由静态语言指令引导的视觉注意力权重被迫在每个决策步从头开始重新评估独立的视觉信息。这种缺乏全局上下文理解的能力，意味着模型无法有效过滤时间上的冗余信息，并聚焦于因过去动作而变得关键的视觉区域，导致视觉系统是“被动”而非“主动”的（见第1节）。尽管近期一些工作（如[52, 22, 46]）开始利用历史信息进行视觉令牌剪枝以提高效率，但其主要关注模型效率，而非提升泛化质量（见第1节）。因此，设计一个更动态、上下文感知的视觉处理范式，以实现更有效的视觉令牌处理并增强VLA的泛化质量，仍然是一个重大挑战。

### **核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下三个方面：

1.  **从POMDP视角重新形式化VLA问题，并提出AVA-VLA框架**：本文首次明确地将VLA模型中的序列决策问题从部分可观测马尔可夫决策过程的视角进行理论重构（见第3.2节）。作者指出，在POMDP框架下，最优策略应同时依赖于当前观测和信念状态。基于此，他们将VLA策略重新形式化为公式(4)：`A_t ~ P_θ(A_t | x_t, b_{t-1})`。这为设计一个利用历史上下文来增强VLA泛化能力的视觉处理范式提供了理论基础。与先前将任务隐含建模为MDP的工作（如OpenVLA、OpenVLA-OFT）相比，这是一个概念性的创新。

2.  **引入循环状态作为信念状态的神经近似，并设计主动视觉注意力模块**：这是本文最核心的技术创新。由于直接计算理论上的信念状态`b_{t-1}`通常是不可行的，作者提出学习一个压缩表示`r_{t-1}`作为其神经近似，并称之为“循环状态”（见第3.2节）。该状态从上一时间步模型输出的、与动作预测相关的隐藏状态`h_{t-1}^M`通过一个MLP模块`B`计算得到（公式(5)）。基于此，作者设计了**主动视觉注意力模块**。该模块的核心功能是利用循环状态`r_{t-1}`和当前视觉观测`x_t`，计算出一组软权重`ω_t`（公式(10)），该权重直接表征了每个视觉令牌的重要性。这些软权重随后被用于动态调制LLM主干中所有层的注意力计算（公式(11)和(12)），使模型能够基于历史信念（而非纯静态语言指令）来过滤和聚焦其注意力。这实现了从“被动”到“主动”视觉处理的范式转变。

3.  **提出基于状态的占位符初始化策略**：为了在并行解码框架（如OpenVLA-OFT）中有效利用并保持丰富的历史信息，作者创新性地使用循环状态`r_{t-1}`来初始化动作占位符嵌入`p_t`，即`p_t = r_{t-1}`（见第3.2节）。这与基线方法（如OpenVLA-OFT）中将占位符初始化为零向量的做法形成鲜明对比。该策略将历史信息直接注入到模型的输入序列中，与AVA模块协同工作，共同构成了完整的AVA-VLA框架（公式(6)）。消融实验（表4）证实了该组件与AVA模块各自的有效性及其组合的协同优势。

### **方法概述**

AVA-VLA框架基于并行解码的VLA模型（以OpenVLA-OFT为基石）构建，其核心流程如下：

**1. 循环状态的生成与利用**：
在时间步`t`，模型接收当前观测`x_t`（包含图像`x_t^I`和指令`x_t^S`）以及上一时间步的循环状态`r_{t-1}`。`r_{t-1}`由上一时间步模型第`M`层（最后一层）输出的隐藏状态`h_{t-1}^M`通过一个MLP模块`B`得到：`r_{t-1} = B(h_{t-1}^M)`（公式(5)）。在序列开始时（`t=0`），`r_{-1}`初始化为零向量。

**2. 主动视觉注意力模块的运作流程**：
AVA模块`V`是框架的核心，其输入为当前视觉特征`z_t^I`、语言特征`z_t^S`和编码后的循环状态`\hat{r}_{t-1}`，输出为视觉令牌的软权重`ω_t`。具体步骤包括（见第3.3节及图2）：
*   **特征编码与调制**：首先通过模态特定的MLP将视觉和语言特征投影到更低维空间`d‘`，得到`\bar{z}_t^I`和`\bar{z}_t^S`。然后使用FiLM层，以语言特征为条件对视觉特征进行调制：`\hat{z}_t^I = F_γ(\bar{z}_t^S) ⊙ \bar{z}_t^I + F_β(\bar{z}_t^S)`。
*   **交叉注意力计算重要性**：将调制后的视觉令牌`\hat{z}_t^I`作为查询`Q_t`（公式(7)），将编码后的循环状态`\hat{r}_{t-1}`作为键`K_t`和值`V_t`（公式(8)），进行交叉注意力计算。这步旨在评估每个视觉令牌相对于历史信念状态的重要性。
*   **自注意力与权重预测**：将交叉注意力的输出通过一个自注意力层进行进一步整合，得到`O_t`（公式(9)）。随后，`O_t`经过一个前馈网络、一个线性层`W`和一个沿特征维度的Softmax函数，预测出每个视觉令牌被“增强”或“削弱”的logits，最终得到软权重向量`ω_t ∈ R^{L_I×2}`（公式(10)）。其中，`ω_t`的两个分量分别由可学习标量`γ_0`和`γ_1`加权。

**3. 软权重在LLM主干中的应用**：
软权重`ω_t`被用于构造一个软注意力掩码矩阵`U_t`（公式(12)）。在LLM主干的每一层`m`计算注意力时，原始的注意力分数矩阵`C_{t,m}`在与`U_t`逐元素相乘后，再经过Softmax操作，得到最终的注意力矩阵`A_{t,m}`（公式(11)）。这使得对视觉令牌的注意力可以根据其重要性分数`ω_t`进行动态缩放。

**4. 训练与推理**：
*   **训练**：由于在整个轨迹上进行时间反向传播计算成本过高，作者采用**截断时间反向传播**策略（见第3.4节）。他们将模型在时间上展开一个固定的短视界（实验中`T=4`），计算动作预测的MAE损失，并添加一个对软权重均值`μ(ω_t)`的L2惩罚项（公式(13)），以引导权重分布符合预期。总损失为各时间步预测损失与惩罚损失的加权和（公式(14)）。
*   **推理**：模型以完全循环的方式运行。在每个时间步`t`，执行一次前向传播（公式(6)），预测当前动作`A_t`并更新循环状态`r_t`，用于下一时间步。

### **实验说明**

**评估指标**：
*   **成功率**：用于评估LIBERO基准测试（四个任务套件

---

## 4. Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment

### 基本信息
- **作者**: Libo Wang
- **arXiv ID**: [oai:arXiv.org:2512.00783v2](https://arxiv.org/abs/2512.00783)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.LG, cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.00783)

            ### 原文摘要
            arXiv:2512.00783v2 Announce Type: replace  Abstract: To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named "Sigma" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment》内容，生成一份结构清晰、内容详实的论文总结。

***

### **论文概要**

本文旨在解决人形机器人视觉-语言-动作模型中，语义理解与连续控制之间缺乏一个可随时间更新、可解释的“中介思维空间”的问题。为此，作者提出了名为“Sigma”的VLA模型。该模型以开源π0.5_base为基础，通过LoRA微调和推理阶段适配器进行优化。其核心创新在于引入了一个名为“心灵感应因子”的潜在认知向量，构建了感知、推理、行为三个协同工作的模块，旨在实现“心灵感应”式的对齐。实验通过离线闭环回放，在svla_so101_pickplace数据集上对比了Sigma与未调优的π0.5_base模型。结果表明，Sigma在向量、片段和轨迹三个时间尺度上的控制均方误差均稳定降低，同时保持了心灵感应范数和语义-文本对齐质量不变，验证了其通过架构设计实现意图驱动行为对齐的有效性。

### **研究动机**

论文的研究动机源于当前VLA模型在高级人形机器人任务中面临的一个关键架构缺陷：在语言语义与连续控制之间，缺乏一个持续更新且可解释的中介心理空间（见第1节）。作者指出，现有工作主要沿着两条技术路线发展：一是以RT-2、OpenVLA等为代表的，侧重于利用预训练视觉语言模型进行语义推理和任务泛化的路线；二是部分研究团队强调的，关注与特定机体全身动态耦合及接触稳定性的路线（见第1节及相关引用）。

然而，这两种路线均未能提供一个能够承载高层语义上下文并与行为残差进行精确对齐的抽象层（见第1节）。这一缺陷导致当指令具有多层语义、未明确定义操作目标或需要依赖拟人化联想来维持动作一致性时，人形机器人会出现策略决策碎片化、意图偏离和语义错位等问题（见第1节）。其根本原因在于，当VLA模型无法在内部构建并维持一个与人类认知结构对齐的潜在思维空间时，“想法”的传递过程会被打断，从而导致高级任务中的策略失衡和行为失配（见第1节）。

因此，本文的核心动机是填补这一技术空白，通过设计一个显式的、可更新的“心灵感应”工作空间，将深层的语义和联想结构压缩为连续的内部思维状态，并利用该状态来对齐未言明的人类意图，从而决定具体的控制行为，以实现更稳定、更符合人类认知的机器人控制。

### **核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下几个方面：

1.  **提出“心灵感应对齐”的VLA架构概念与“心灵感应因子”：** 论文的核心概念创新是提出了一个名为“心灵感应”的对齐机制。这并非指超自然现象，而是指模型将指令背后的深层语义和联想结构压缩为一个连续的内部思维状态（τ，即心灵感应因子），并利用该状态来推断和匹配人类未言明的意图，从而调制具体行为（见第1节，图1）。τ作为一个跨时间共享的潜在认知向量，是整个模型实现语义与动作对齐的关键媒介。

2.  **设计了协同工作的三模块架构（认知视觉、语言意图、动作轨迹工作空间）：** 论文在方法上创新性地设计了一个由三个紧密耦合的工作空间组成的架构（见第3节，图1）。
    *   **认知视觉工作空间：** 该模块的创新在于引入了基于FiLM的门控机制和感知器式重采样。视觉特征首先被投影并重采样为固定长度的基础令牌`V_base`。随后，来自语言模块的τ通过一个视觉调制器生成FiLM参数（γ, β），对`V_base`进行通道级的缩放和平移，从而将场景表征约束在当前语义和联想状态之下（见第3.1节及对应公式）。这实现了感知内容受高层思维调制的功能。
    *   **语言意图工作空间：** 该模块的创新在于构建了**语义工作空间记忆**和**心灵感应投影器**。MLLM骨干网络处理多模态令牌后，语义因子头从中读取语义因子。语义工作空间记忆通过门控递归公式整合当前语义因子与历史记忆`m_{t-1}`，形成具有时间连续性的语义记忆`m_t`（见第3.2节及对应公式）。同时，独立的摘要头提取环境上下文、行为摘要和文本摘要。意图头融合`m_t`、环境与行为摘要推断出潜在意图`z_intent`。最后，心灵感应投影器汇聚`m_t`、`z_intent`、各类摘要和语义因子池化表示，生成全局对齐向量——心灵感应因子τ（见第3.2节及对应公式）。这一设计实现了从多模态输入中蒸馏出持续演进的“思维状态”。
    *   **动作轨迹工作空间：** 该模块的核心创新是**心灵感应残差头**。它并非直接输出原始动作，而是计算当τ为零时的安全基线行为（`a_base`），然后将τ和高层表示映射为一个显式的残差Δa（见第3.3节及对应公式）。最终动作由基线行为与心灵感应残差融合而成（`a_τ = a_base + Δa`）。这种方法在引入高层语义调制的同时，最大程度地保留了基础模型的物理稳定性先验。

3.  **提出了两项针对性的训练算法：**
    *   **心灵感应残差动作聚焦算法：** 该算法（见第4.2.1节）的创新在于其训练策略。它使用π0.5的基线动作作为参考，仅学习三个动作输出（向量、片段、轨迹）的心灵感应残差。同时，算法计算样本级难度分数，并对Top-K困难片段施加额外损失权重，使模型能将学习能力集中在最具挑战性的对齐场景上，实现了高效且稳定的微调。
    *   **心灵感应语义对齐课程算法：** 该算法（见第4.2.2节）的创新在于其课程学习设计。它通过随时间线性增加的课程权重`w_sem(t)`, `w_intent(t)`, `w_τ(t)`，引导训练过程。早期阶段主要侧重于动作回归以稳定基础控制；后期阶段则逐步加强语义一致性损失、意图关联损失和心灵感应正则化损失，迫使模型将其内部思维结构与显式轨迹收敛到同一个“心理坐标系”中（见第4.2.2节及对应公式）。

### **方法概述**

Sigma方法的核心是一个端到端但模块化的VLA架构，其运作流程紧密围绕“心灵感应因子”τ的生成与应用展开。以下详细说明其实现细节：

**1. 前向传播流程：**
模型以前向传播方式整合三个工作空间。首先，**认知视觉工作空间**接收传感器数据（图像、深度、音频、机器人状态）。通过专用编码器（Vision Encoder, State Encoder）和投影器，将多源信号转换为统一`d_model`维度的令牌序列。感知器式重采样器（Perceiver-style Resampler）使用可学习查询将其压缩为固定长度`N_v`的视觉基础令牌`V_base`（见第3.1节公式）。此时，τ尚未生成，因此视觉调制暂时跳过或使用零值τ。

接着，处理后的视觉令牌、状态令牌和文本令牌被送入**语言意图工作空间**。MLLM骨干通过多层跨模态注意力形成隐藏序列。语义因子头从中读取K个语义因子`z_sem`。语义工作空间记忆执行递归更新：`m_t = λ ⊙ m_{t-1} + (1-λ) ⊙ u`，其中`u`是当前语义因子的投影，`λ`是门控系数，以此维护跨时间的语义连续性（见第3.2节公式）。同时，环境、行为、文本摘要头并行工作。意图头融合`m_t`、环境与行为摘要，推断出潜在意图`z_intent`。最终，心灵感应投影器将`m_t`、`z_intent`、所有摘要及语义因子池化表示拼接，通过一个MLP生成本时间步的τ（见第3.2节公式）。τ生成后，会反馈给视觉调制器，对`V_base`进行FiLM调制，输出经过τ调制的视觉令牌`V_mod`。同时，语言调制器使用τ对隐藏序列进行偏置调制，输出高层语义表示`high_level_rep`。

最后，**动作轨迹工作空间**接收`high_level_rep`、τ和状态令牌。动作条件投影器生成条件向量`c_act`。动作查询生成器结合`c_act`和状态令牌，通过交叉注意力和Transformer细化可学习查询种子，生成动作查询令牌`q_t`（见第3.3节公式）。随后，三个并行的动作头（Action Token Head, Action Chunk Head, Trajectory DiT/Diffusion Policy）分别以`q_t`为基础，产生向量级、片段级和轨迹级的**基线动作** `a_base_vec`, `a_base_chunk`, `a_base_traj`。与此同时，心灵感应残差头以τ和`high_level_rep`为输入，通过一个MLP `g(·

---

## 5. GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation

### 基本信息
- **作者**: Yunfei Li, Xiao Ma, Jiafeng Xu, Yu Cui, Zhongren Cui, Zhigang Han, Liqun Huang, Tao Kong, Yuxiao Liu, Hao Niu, Wanli Peng, Jingchao Qiao, Zeyu Ren, Haixin Shi, Zhi Su, Jiawen Tian, Yuyang Xiao, Shenyu Zhang, Liwei Zheng, Hang Li, Yonghui Wu
- **arXiv ID**: [oai:arXiv.org:2512.01801v2](https://arxiv.org/abs/2512.01801)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.01801)

            ### 原文摘要
            arXiv:2512.01801v2 Announce Type: replace-cross  Abstract: We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation》内容，生成一份符合要求的、详实的论文总结。

***

### **论文概要**
本文提出了GR-RL，一个用于长时程、灵巧且高精度机器人操作的强化学习增强训练框架。该框架旨在将一个通用的视觉-语言-动作（VLA）策略转化为针对特定复杂任务的专家策略。其核心是解决在灵巧操作任务中，人类演示数据存在噪声、次优性以及训练与部署行为不匹配的问题。GR-RL采用了一个多阶段训练流程：首先，通过离线强化学习学习任务进度评估器，用于过滤次优的演示数据；其次，应用形态对称性数据增强提升策略的泛化能力；最后，通过在线强化学习在潜在噪声空间进行结构化探索，以对齐策略的部署行为。实验在极具挑战性的“穿鞋带”任务上进行，GR-RL实现了83.3%的成功率，证明了其有效性。

### **研究动机**
论文的研究动机源于当前通用视觉-语言-动作（VLA）策略在迈向实际部署时存在的两个根本性不足（见第1节）。首先，现有VLA策略在**灵巧性与精度**方面存在瓶颈，尤其是在处理毫米级精度控制和可变形物体（如鞋带）时能力不足。其次，在**长时程鲁棒性**方面，误差会随着操作步骤累积，当与高精度灵巧操作结合时，问题会加剧。以“穿鞋带”任务为例，它同时要求机器人具备处理可变形物体的灵巧性、毫米级的控制精度以及应对意外情况的长时程规划能力。

作者指出，现有方法存在两个关键瓶颈（见第1节）。**瓶颈一：次优的人类演示**。在极端精密和灵巧的操作场景下，人类演示者会减速、犹豫，从而为策略引入了噪声和次优的演示数据。直接模仿所有数据（即行为克隆）会导致策略学习到次优和有限的技能。**瓶颈二：演示与推理的不匹配**。标准的离线训练中，VLA策略通过滑动窗口从人类演示中预测固定长度的动作块来模仿人类。然而，为了实现平滑的推理和控制，系统层面通常会应用后处理优化（如时间集成、异步滚动时域控制）。这些优化对于策略的平滑执行是必要的，但不可避免地导致了模型训练（原始动作）与部署（优化后动作）之间的行为不匹配，在精密操作中这种不匹配不可忽视。

因此，论文的研究动机是开发一种方法，能够从存在噪声、次优且存在训练-部署不匹配的人类演示数据中，训练出能够在长时程、高精度灵巧操作任务中可靠执行的机器人策略。

### **核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面：

1.  **基于离线强化学习的任务进度评估与数据过滤机制**：这是论文的首要创新点。针对人类演示数据中的次优片段难以人工标注的问题，作者提出利用离线强化学习（采用TD3+BC算法）训练一个分布式的评论家（Critic）模型作为**任务进度评估器**（见第3.1节）。该评论家在稀疏奖励（仅在任务成功时给予奖励）设置下进行训练，其预测的Q值分布的平均值被定义为任务进度ρ（公式2）。关键创新在于，作者发现并利用了**分布式评论家在稀疏奖励下的鲁棒性**（见第2、3.1节及图7）。与无界回归的评论家相比，分布式评论家将价值预测约束在[0,1]区间内，在长时程、稀疏奖励场景下能更稳健地收敛，避免价值高估，从而更准确地反映任务进度。如图3所示，该评估器能敏感地捕捉到操作失误（如鞋带滑出鞋眼）导致的进度下降。基于此，论文提出自动过滤标准：如果一个动作块对应的进度序列中出现超过阈值δ的下降，则判定该动作为次优并予以剔除。这提供了一种**数据驱动的、无需人工标注的自动清洗机制**，与直接使用所有数据进行行为克隆（如基线GR-3）或使用简单的回归模型预测进度相比，显著提升了离线策略的质量。

2.  **面向双手机器人的形态对称性数据增强范式**：这是一个简单而有效的工程创新（见第3.2节）。针对双手机器人操作任务中固有的形态对称性，作者设计了一套完整的数据增强方案。该方案不仅对图像观测进行水平翻转和左右手腕图像交换，还对本体感知状态和动作在世界坐标系下进行镜像对称变换，并相应翻转语言指令中的空间描述（如将“左边的孔”改为“右边的孔”）。这种增强**显式地扩充了数据集，使策略能够更好地泛化到对称的任务配置中**，从而提高了策略的鲁棒性和整体成功率（如图5所示，使过滤后行为克隆的成功率从61.6%提升至72.7%）。

3.  **潜在噪声空间的结构化在线探索与策略对齐方法**：为了解决训练-部署不匹配问题并进一步提升策略性能，论文提出了一种样本高效的在线强化学习方法（见第3.3节）。其创新在于**在潜在噪声空间进行结构化探索，而非直接对高维动作空间添加噪声**。具体而言，作者在共享的VLM骨干网络后添加一个可训练的噪声预测器πθ‘，用于为动作扩散Transformer（DiT）预测初始噪声ϵt。为了避免生成偏离离线数据分布的动作，在目标函数中加入了约束项，惩罚输出噪声与标准正态分布过大的偏差（公式3）。同时，在噪声空间蒸馏一个Q函数Qϕ‘，以避免在策略优化时反向传播通过整个流匹配模型（公式4）。这种方法允许策略在保持与离线学习分布相近的同时，通过微调初始噪声来**“引导”** 生成更优的动作序列，从而有效地对齐部署行为并探索改进策略。

### **方法概述**
GR-RL方法是一个多阶段训练流程，其核心架构是一个包含5B参数的混合Transformer（Mixture-of-Transformer, MoT），由一个视觉-语言-动作（VLA）策略模型πθ和一个多任务评论家模型Qϕ组成（见第2节及图2）。

**第一阶段：基于任务进度评估的离线过滤与行为克隆**
1.  **数据准备与评论家训练**：收集包含成功和失败轨迹的人类演示数据。通过标注演示中的“重试关键帧”， hindsight地创建更多失败轨迹以丰富数据（第3.1节）。使用TD3+BC算法，在稀疏奖励（公式1）下训练一个**分布式评论家**Qϕ。该评论家采用Q-chunking技术，为每个动作块预测一个Q值分布。
2.  **进度评估与数据过滤**：使用训练好的评论家计算数据集中每个转移（transition）的任务进度ρt = mean(Qϕ(ot, l, st, at))。如图3所示，进度曲线能识别失误。定义过滤规则：若一个动作块对应的进度序列ρt:t+k中出现跌幅大于阈值δ，则判定该动作为次优并剔除。
3.  **策略初始化**：使用过滤后的高质量数据集，通过行为克隆（最小化预测动作与演示动作的差异）训练VLA策略πθ。该策略以Qwen2.5-VL-3B-Instruct作为VLM骨干，并使用动作扩散Transformer（DiT）通过流匹配目标生成动作块。

**第二阶段：形态对称性数据增强**
在离线行为克隆训练期间，同步应用第3.2节描述的对称性增强。对于每个训练样本，生成其镜像对称的副本（包含变换后的观测、状态、动作和指令），并将这些增强数据与原始数据混合用于训练，从而提升策略的泛化能力。

**第三阶段：潜在噪声空间的在线强化学习**
1.  **模型与缓冲区准备**：初始化在线RL阶段。添加噪声预测器πθ‘（51.5M参数）和噪声空间评论家Qϕ‘。使用离线训练好的策略进行一定轮次的交互，收集初始在线轨迹，用于预热离策略回放缓冲区，并微调评论家头Qϕ和Qϕ‘以适应模型 rollout 的分布。
2.  **结构化探索与训练**：在线交互时，策略通过πθ‘预测噪声ϵt，再通过πθ生成动作。收集的轨迹分别存入**在策略缓冲区**（仅保留最近两个检查点的数据）和**离策略缓冲区**。训练时，从两个缓冲区均匀采样批次。
3.  **优化目标**：
    *   **噪声预测器πθ‘**：其损失函数（公式3）旨在最大化噪声空间Q值Qϕ‘(ot, l, st, ϵt)，同时通过约束项确保预测的噪声ϵt不偏离标准正态分布N(0,1)太远（阈值β）。
    *   **噪声空间评论家Qϕ‘**：其损失函数（公式4）是交叉熵损失，目标是使Qϕ‘的预测分布与原始动作空间评论家Qϕ在对应动作上的分布对齐。为了确保Qϕ‘在噪声空间有良好的覆盖，输入噪声ϵt以0.5的概率从N(0,1)采样，0.5的概率从πθ‘采样。
    *   **原始动作空间评论家Qϕ**

---

## 6. RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning

### 基本信息
- **作者**: Yuhong Zhang, Zihan Gao, Shengpeng Li, Ling-Hao Chen, Kaisheng Liu, Runqing Cheng, Xiao Lin, Junjia Liu, Zhuoheng Li, Jingyi Feng, Ziyan He, Jintian Lin, Zheyan Huang, Zhifang Liu, Haoqian Wang
- **arXiv ID**: [oai:arXiv.org:2512.02729v1](https://arxiv.org/abs/2512.02729)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.02729)

            ### 原文摘要
            arXiv:2512.02729v1 Announce Type: new  Abstract: We introduce Robowheel, a data engine that converts human hand object interaction (HOI) videos into training-ready supervision for cross morphology robotic learning. From monocular RGB or RGB-D inputs, we perform high precision HOI reconstruction and enforce physical plausibility via a reinforcement learning (RL) optimizer that refines hand object relative poses under contact and penetration constraints. The reconstructed, contact rich trajectories are then retargeted to cross-embodiments, robot arms with simple end effectors, dexterous hands, and humanoids, yielding executable actions and rollouts. To scale coverage, we build a simulation-augmented framework on Isaac Sim with diverse domain randomization (embodiments, trajectories, object retrieval, background textures, hand motion mirroring), which enriches the distributions of trajectories and observations while preserving spatial relationships and physical plausibility. The entire data pipeline forms an end to end pipeline from video,reconstruction,retargeting,augmentation data acquisition. We validate the data on mainstream vision language action (VLA) and imitation learning architectures, demonstrating that trajectories produced by our pipeline are as stable as those from teleoperation and yield comparable continual performance gains. To our knowledge, this provides the first quantitative evidence that HOI modalities can serve as effective supervision for robotic learning. Compared with teleoperation, Robowheel is lightweight, a single monocular RGB(D) camera is sufficient to extract a universal, embodiment agnostic motion representation that could be flexibly retargeted across embodiments. We further assemble a large scale multimodal dataset combining multi-camera captures, monocular videos, and public HOI corpora for training and evaluating embodied models.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning》内容，生成一份结构清晰、内容详实的总结报告。

***

### **论文概要**
本文提出了RoboWheel，一个将真实世界人手-物体交互（HOI）视频转化为跨形态机器人学习监督数据的数据引擎。该引擎从单目RGB(D)视频出发，通过高精度HOI重建、基于物理约束的优化，生成物理上合理的交互轨迹，并将其灵活地重定向到多种机器人形态（如机械臂、灵巧手、人形机器人）。为了扩展数据覆盖范围，论文在Isaac Sim仿真器中实施了丰富的领域随机化与数据增强策略。基于此流程，作者构建了一个大规模多模态数据集HORA，并在主流视觉-语言-动作（VLA）和模仿学习模型上验证了所生成数据的有效性，首次定量证明了HOI数据可作为机器人学习的有效监督信号。

### **研究动机**
获取大规模、高质量的机器人可执行监督数据是机器人学习的关键瓶颈。现有方法主要存在两大不足，限制了其可扩展性和通用性。

首先，**传统机器人数据收集方法成本高昂且形态特定**。如引言（第1节）所述，遥操作和动作捕捉系统需要专用硬件和精心策划，限制了行为多样性和跨机器人形态（embodiment）的迁移能力。同时，**纯仿真生成的数据**往往无法反映真实世界的感知和接触分布。

其次，**海量的人手-物体交互（HOI）视频数据未被有效利用**。尽管存在大量包含丰富操作策略的HOI视频（如公开数据集、在线视频），但由于**重建噪声、物理不合理性以及形态不匹配**等问题，这些信号很少被转化为机器人可用的训练数据（见引言及第2节“HOI datasets and monocular reconstruction”）。具体而言，现有单目HOI重建方法（如HORT、HOLD、DiffHOI）在接触估计一致性、手-物体穿透、轨迹时间平滑性以及动力学约束方面存在缺陷（第2节）。

因此，论文旨在填补这一缺口，构建一个**实用、可扩展的**处理流程，需满足三个关键需求（见引言）：(i) 能够大规模、连续地从真实世界操作空间中获取物理合理的机器人-物体交互轨迹；(ii) 支持将这些轨迹灵活重定向到多样化的机器人形态，同时保持交互语义；(iii) 通过有效的数据增强策略组合保持可扩展性。RoboWheel正是为解决这些联合需求而设计的。

### **核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面：

1.  **物理合理的HOI重建与跨域重定向框架**：论文提出了一套端到端的流程，将单目HOI视频转化为跨形态的机器人可执行动作。其创新性在于**将最先进的感知技术与基于物理的优化深度融合**。具体而言（见第3.2节及图2）：
    *   **感知与初始化**：整合了人手/全身姿态估计（如[28, 49]）和物体姿态跟踪（如[39]），并利用多视图3D生成器（如[52]）和深度信息恢复物体的度量尺度，为后续优化提供高质量的初始轨迹。
    *   **物理合理性优化**：这是一个两阶段优化过程。**第一阶段**使用截断符号距离函数（TSDF）进行碰撞优化，最小化手部顶点与物体TSDF值的平方，以消除穿透（见第3.2节公式描述及图2右上角）。**第二阶段**引入一个**残差强化学习（RL）策略**（受[22]启发），在避免碰撞的基础上，进一步优化手-物体相对姿态，以鼓励物理一致的接触并确保机器人可达性。奖励函数（公式2）综合了几何跟踪误差（∆h, ∆p）、动力学平滑性（∆ḣ, ∆ṗ）和接触力（C_t）的约束。

2.  **仿真增强的数据飞轮**：论文并非简单地将重建轨迹重放到仿真中，而是构建了一个**数据增强飞轮**，在保持交互语义和物理合理性的前提下，极大地丰富了观测和动作的多样性（第3.4节及图4）。关键创新增强策略包括：
    *   **多形态重定向**：将HOI轨迹通过GPU加速的逆运动学（IK）求解器（cuRobo [32]）映射到五种不同的6/7自由度机械臂（UR5, Panda等）的关节空间，生成异构的联合控制指令（公式3）。
    *   **物体检索与替换**：基于形状（Chamfer距离）、包围盒兼容性和语义嵌入的融合相似度（公式4），从大型物体库中检索几何/语义相似的物体进行替换，保持原有EE运动计划不变。
    *   **轨迹增强**：将轨迹按接触状态（抓持/放开）分段，对抓持段施加物体坐标系下的刚性变换，对非抓持段进行锚点重映射和线性路径插值（公式5），以生成新的可行轨迹。

3.  **大规模多模态数据集HORA**：基于RoboWheel引擎，作者构建了HORA数据集（第4节，表1）。其创新性在于**首次系统性地融合了多源异构数据，并提供了统一的、跨形态的机器人学习监督**。HORA包含三个子集：
    *   **动作捕捉子集**：使用自定义多视图系统与触觉手套采集，提供高精度HOI信号、触觉地图及机器人模态。
    *   **录制视频子集**：通过RGB(D)录制并经完整RoboWheel流程处理生成。
    *   **公开HOI子集**：将现有HOI数据集（如DexYCB, HO3D）的模态转化为规范动作空间后，进行重定向和增强。
    总计超过15万条轨迹，同时支持HOI分析（手部参数、物体位姿、接触标注）和机器人学习（多视角观测、EE/关节空间轨迹、任务描述），为相关研究提供了丰富的资源。

### **方法概述**
RoboWheel方法是一个系统性的四阶段管道（见图2）：HOI重建与物理优化 → 跨形态重定向 → 仿真增强 → 数据采集与模型训练。

**第一阶段：HOI重建与物理优化（第3.2节）**
1.  **输入与状态定义**：输入为单目RGB(D)视频帧序列 {I_t}。目标恢复手部状态 h_t（包括姿态参数、全局旋转和平移）和物体6D位姿 p_t（公式1）。
2.  **运动恢复**：根据视频判断是手部还是全身运动，分别使用专用估计器（如[28]或[49]）恢复参数。同时，结合语义分割、深度图和多视图生成器G恢复带尺度纹理的物体网格 M_o，并通过跟踪器F估计物体位姿 T^c_o(t)。利用[34]估计相机外参，将所有重建转换到世界坐标系。
3.  **物理优化**：
    *   **碰撞优化**：利用物体水密TSDF φ_o，通过最小化 φ_o^2(V^{palm}_h, t) 优化手部参数，避免手掌与物体穿透。进一步优化手腕参数，避免手部其他部分与物体穿插。
    *   **残差RL优化**：在碰撞优化的基础上，训练一个RL策略来微调轨迹。状态 s_t 包含手/物体位姿、速度及接触力。策略旨在最小化与目标状态的误差，同时鼓励合理的接触。奖励函数 r_t（公式2）由几何跟踪奖励（λ_geoΦ_geo）、动力学平滑奖励（λ_dynΦ_dyn）和接触奖励（λ_conΦ_con）加权组成。

**第二阶段：跨形态重定向（第3.3节）**
1.  **机械臂重定向**：将优化后的手部关节映射到平行夹爪的末端执行器（EE）位姿 {T_g(t), g(t)}。创新点在于根据接触几何（全手主导或仅指尖主导）采用两种互补的**方向构建方法**，并使用kNN分类器进行选择（图3）。夹爪开合状态通过跟踪物体上关键点的运动（使用CoTracker[17]）来判断。
2.  **灵巧手与人形机器人重定向**：对于灵巧手，基于运动学相似性和接触保持约束，将重建的手部运动重定向到目标机器人手的关节空间。对于全身演示，则利用SMPL-H全身估计，通过逆运动学和动力学感知优化适配到人形机器人关节树。

**第三阶段：仿真中的数据增强（第3.4节）**
在Isaac Sim中重放重定向后的轨迹，并应用多种增强：
*   **多臂重定向**：对同一EE轨迹，使用cuRobo的GPU并行IK为五种不同机械臂求解可行的关节轨迹 {q_t}（公式3）。
*   **物体替换与轨迹增强**：如核心贡献第2点所述，执行物体检索替换（公式4）和轨迹分段增强（公式5）。
*   **其他增强**：包括背景纹理随机化、桌面杂乱配置、手部运动镜像等。
*   **回放评估与语言标注**：使用Qwen2.5-VL[

---

## 7. Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols

### 基本信息
- **作者**: Xianchao Zeng, Xinyu Zhou, Youcheng Li, Jiayou Shi, Tianle Li, Liangming Chen, Lei Ren, Yong-Lu Li
- **arXiv ID**: [oai:arXiv.org:2512.02787v1](https://arxiv.org/abs/2512.02787)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.RO, cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.02787)

            ### 原文摘要
            arXiv:2512.02787v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols》生成一份符合要求的详细总结。

***

### **论文总结：Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols**

#### **1. 论文概要**
本文针对机器人操作中视觉-语言-动作模型在失败诊断与学习方面的局限性，提出了一个名为ViFailback的框架。该框架旨在利用显式的视觉符号高效标注真实世界的失败数据，并生成包含文本和视觉修正指导的数据集。基于此，作者构建了包含58,126个视觉问答对的大规模数据集ViFailback，并设立了ViFailback-Bench基准，用于评估视觉语言模型的失败诊断与修正能力。通过微调Qwen3-VL-8B得到的ViFailback-8B模型在基准测试中表现优异，并能生成视觉符号指导。最终，通过将该模型与VLA模型集成，在真实机器人实验中验证了其能有效帮助策略从失败中恢复。

#### **2. 研究动机**
当前，基于模仿学习的视觉-语言-动作模型在机器人操作任务中取得了显著进展（第1节）。然而，当部署到真实世界时，这些模型不可避免地会遇到分布外情况，导致操作失败（第1节）。因此，使机器人系统能够诊断、学习并从失败中恢复，对于实现开放世界中的鲁棒通用操作至关重要。

现有工作存在两个主要不足。首先，在失败诊断与修正方面，尽管视觉语言模型在任务规划等方面表现出色，但它们通常难以准确分析和修正机器人操作失败（第1节）。其次，在数据层面，现有方法大多通过在仿真中注入扰动来自动生成大规模失败数据集，用于微调VLM（第1节，第2.2节）。然而，这种方法从根本上受到“仿真到现实”差距的限制，在真实世界场景中的效率低下（第1节）。事实上，在遥操作数据收集或策略执行过程中，机器人除了成功演示外，也会产生一定量的失败数据，但以简单高效的方式标注这些数据仍然是一个重大挑战（第1节）。

因此，本文的研究动机是：**如何利用真实世界的失败数据，构建一个高效、可扩展的标注框架，以提升VLM的失败诊断与修正能力，并最终赋能VLA模型实现从失败中学习与恢复**。论文旨在弥合真实世界失败数据利用效率低、以及现有VLM/VLA模型缺乏细粒度失败分析与视觉化修正指导能力的缺口。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下四个方面：

1.  **提出基于视觉符号的高效标注框架ViFailback**：这是本文最核心的概念创新。与依赖纯文本描述或仿真扰动生成失败数据的方法不同，ViFailback引入了一套精心设计的**视觉符号系统**（第3.1节，图2左部），使人类标注者能够通过鼠标在视频帧上直接绘制符号来提供低成本的修正指导。这些符号（如彩色箭头、十字准线、开关标签等）承载了丰富的空间和动作语义（例如，红/绿/蓝箭头分别对应前-后、左-右、上-下移动）。该框架将标注流程分解为诊断与修正两个部分，并利用VLM辅助生成复杂的文本描述（如失败原因分析），从而实现了半自动化的高效标注（第3.2节，第3.3.2节）。这与前人工作（如RoVI, CrayonRobo）主要将视觉符号用于初始任务指导而非实时失败修正有本质区别（第2.3节）。

2.  **构建大规模真实世界失败诊断与修正数据集ViFailback及评估基准ViFailback-Bench**：基于上述框架，作者收集了5,202条真实世界操作轨迹（涵盖100个不同任务），生成了包含58,126个高质量VQA对的ViFailback数据集（第3.3节）。更重要的是，他们从中提取并构建了ViFailback-Bench基准，该基准包含**Lite**（封闭式问答）和**Hard**（开放式问答）两个版本，系统地评估VLM在11个细粒度任务上的能力，包括失败检测、关键帧定位、失败类型识别、失败原因分析以及低/高层级修正指导等（第3.3.3节，图3）。这填补了现有机器人VQA基准（如Robo2VLM, ManipBench）在评估“哪里出错及为何出错”方面的空白（第2.2节）。

3.  **开发并验证了具备视觉符号生成能力的ViFailback-8B模型**：通过使用LoRA在ViFailback数据集上微调Qwen3-VL-8B，得到了ViFailback-8B模型（第4.1节）。该模型不仅在全基准上显著超越了包括GPT-4o和Gemini-2.5-Pro在内的16个先进基线模型（表1），更重要的是，它**学会了生成用于绘制视觉符号的代码**（第4.2节，图4中“Generating Visual Symbols‘ Codes”任务）。这使得模型能够提供端到端的视觉修正指导，而不仅仅是文本建议。

4.  **在真实机器人系统中验证了框架的有效性**：作者将ViFailback-8B作为外部监督器与VLA模型（π0.5）集成，构建了一个策略修正系统（第4.3节）。当检测到失败时，ViFailback-8B生成包含视觉符号代码的修正指导，并通过两种方式（VSF：微调VLA跟随符号；PMC：基于点的运动控制）引导机器人恢复。真实世界实验表明，集成ViFailback-8B后，任务平均成功率提升了22.2%（表4），直观证明了该框架能够帮助VLA模型从失败中学习并恢复，这是实现策略真正“从失败中学习”的关键一步（第5节）。

#### **4. 方法概述**
ViFailback框架的整体流程如图2所示，主要包括数据标注、模型训练与部署应用三个环节。

**A. 数据标注流水线（第3.3.2节）**：
这是一个三阶段、人机协作的流程。**阶段1（基础语义信息填充）**：标注者通过UI控件（滑块、按钮）完成失败诊断的基础标注，如失败检测、关键帧定位等。任务描述由Qwen2.5-Max预先分解为子任务。**阶段2（文本指导选择与视觉符号绘制）**：在选定的关键帧上，标注者从预定义类别中选择合适的动作修正选项，并通过鼠标拖拽绘制相应的视觉符号（如箭头、十字准线）来指示修正动作。**阶段3（开放式描述生成与精炼）**：将前两阶段的所有标注信息和视觉符号输入给Qwen3-VL-235B，提示其生成失败原因分析和高层文本指导等开放式描述。这些输出经过人工验证和精炼以确保质量。

**B. 视觉符号系统设计（第3.1节）**：
系统定义了7种视觉符号，分为三类：1) **运动符号**：包括用颜色编码3D方向的**彩色直线箭头**（红：前/后，绿：左/右，蓝：上/下）和指示末端执行器旋转方向的**半圆箭头**。2) **空间关系符号**：包括用于对齐两个目标的**双十字准线**和用于高亮理想物体/区域的**十字准线**。3) **状态符号**：包括表示夹爪开闭的**ON/OFF标签**、表示夹爪应停止的**禁止图标**和表示应返回之前状态的**回退图标**。这些符号的参数（如类别、起点、终点）被记录，供VLM学习。

**C. 模型训练与基准评估（第4.1，4.2节）**：
使用ViFailback数据集的训练集（52,416个VQA对）对Qwen3-VL-8B进行LoRA微调，得到ViFailback-8B。评估时，对封闭式问题使用准确率，对开放式问题使用基于GPT-4o的评估器，从语义相似性、内容完整性和功能对等性三个维度进行评分并取平均。

**D. 机器人策略修正系统（第4.3节）**：
系统工作流程如下：1) 机器人使用微调后的π0.5模型执行任务。2) ViFailback-8B以固定间隔“监听”机器人的视频流。3) 一旦检测到失败，ViFailback-8B生成基于思维链的诊断和低层指导，特别是基于当前观察绘制视觉符号的代码。4) 视觉符号被叠加到机器人相机视图上，并与修正指导的文本提示一同输入给π0.5模型，引导其恢复。为了处理π0.5无法原生跟随视觉符号的问题，论文提出了两种集成方法：**VSF方法**收集视觉符号跟随轨迹数据对π0.5进行额外微调；**PMC方法**则使用一个底层运动控制器来解析并执行视觉符号指示的动作（如移动到目标点），必要时结合GraspNet进行抓取。

#### **5. 实验说明**
- **评估指标**：
    - **ViFailback-Bench Lite（封闭式）**：使用准确率（%）。
    - **ViFailback-Bench Hard（开放式）**：

---

## 8. Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling

### 基本信息
- **作者**: Yueru Jia, Jiaming Liu, Shengbang Liu, Rui Zhou, Wanhe Yu, Yuyang Yan, Xiaowei Chi, Yandong Guo, Boxin Shi, Shanghang Zhang
- **arXiv ID**: [oai:arXiv.org:2512.03044v1](https://arxiv.org/abs/2512.03044)
- **发布日期**: Wed, 03 Dec 2025 00:00:00 -0500
- **分类**: cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2512.03044)

            ### 原文摘要
            arXiv:2512.03044v1 Announce Type: new  Abstract: Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling》内容，生成一份符合要求的详细总结。

***

### **论文总结：Video2Act**

#### **1. 论文概要**
本文提出Video2Act，一个用于机器人操作策略学习的异步双系统视觉-语言-动作模型。该研究旨在解决现有基于视频扩散模型的机器人策略未能充分利用其内在时空一致表征的问题。Video2Act的核心创新在于从视频扩散模型中显式提取并精炼空间结构（通过Sobel算子）和运动动态（通过快速傅里叶变换）表征，并将其作为条件输入到一个快速执行的扩散变换器动作头中。通过设计一个异步双系统架构，慢速的视频扩散模型（系统2）提供低频更新的时空条件，而快速的动作头（系统1）结合高频视觉输入生成实时动作。实验在RoboTwin仿真基准和真实世界ALOHA双手机器人平台上进行，结果表明Video2Act在平均成功率上超越了现有最先进方法。

#### **2. 研究动机**
机器人策略学习需要鲁棒的感知和动态建模能力。当前主流的视觉-语言-动作模型通常依赖静态图像编码器（如SigLIP、DINOv2）提取视觉特征，这种方法仅能提供多帧观测，但无法理解帧间的时间与因果依赖关系（见第1节）。近期，视频扩散模型在捕捉真实世界动态方面的成功，启发了研究者将其表征用于增强机器人策略学习（如VPP方法）。然而，现有方法（如VPP [22]、MIND [12]）通常直接使用VDM的原始特征，并未显式探索和利用VDM原始表征中固有的空间结构和运动动态信息（见第2.1节及第3.1节）。这些被忽略的结构和时序信息本可以为动作学习提供更高效、信息更丰富的先验。

为了探究VDM中编码的时空信息，作者在第3.1节进行了系统的定性分析。通过Grad-CAM可视化对比静态图像编码器（SigLIP、DINOv2）与VDM在机器人操作任务（静态第三人称视角和动态腕部相机视角）中的特征激活图，发现VDM的特征能够更稳定、一致地关注被操作的前景物体，即使在严重的自我运动干扰下，也表现出强大的空间结构感知和运动一致性（见图2）。这一观察表明，VDM特征中蕴含着对机器人策略学习极具价值的、鲁棒的时空表征，但现有工作未能有效提取和利用它们。因此，本研究的动机是：**如何显式地从VDM中提取并精炼这些空间和运动感知表征，并将其高效地整合到实时机器人策略学习中，以提升策略的鲁棒性和性能。**

#### **3. 核心贡献与创新点**
本文提出了三项核心贡献与创新点：

1.  **对VDM表征在机器人场景下的系统性分析**：论文首次通过定性的Grad-CAM可视化，系统地揭示了VDM在静态和动态机器人观测设置下，其潜在特征能够捕捉到稳定的、对机械臂和腕部相机动态变化鲁棒的结构和运动一致特征（见第3.1节，图2）。这一分析为后续利用VDM进行策略学习提供了实证依据，并指出了直接使用原始特征的不足。

2.  **显式的时空运动表征提取与整合方法**：这是本文最核心的概念性创新。不同于先前工作直接使用原始VDM特征，Video2Act设计了两种非学习的、通道级的操作符，从VDM特征中显式提取精炼的时空信息：
    *   **空间结构表征（Sobel算子）**：对高分辨率、短时序的VDM特征图（`F^H`）应用Sobel算子，计算每个通道的空间梯度幅值，以增强对前景物体结构边界的感知，同时过滤背景噪声和任务无关偏差（见第3.2.2节，公式(3)及后续推导）。
    *   **运动动态表征（快速傅里叶变换）**：对正常分辨率、长时序的VDM特征序列（`F^L`）沿时间轴进行一维离散傅里叶变换，并通过高通滤波器（频率掩码`B`）抑制低频背景成分，再经逆变换重构，以突出跨帧的连贯运动模式（见第3.2.2节，公式(3)及后续推导）。
    这两种精炼后的表征（`S_t`和`M_t`）通过交叉注意力作为额外条件注入到扩散变换器动作头中，使模型能够明确地推理“操作什么”和“如何移动”。

3.  **异步双系统策略架构**：针对VDM计算成本高、难以实时运行的问题，本文提出了一个工程与概念结合的创新架构。受认知科学中双系统理论的启发，该框架将计算昂贵的VDM设计为**慢速感知系统（系统2）**，负责低频更新，提供丰富的时空条件；将一个轻量级的扩散变换器动作头设计为**快速执行系统（系统1）**，负责高频（最高380 Hz）生成精确动作，并实时接收图像观测和系统2的低频条件（见第3.3节，图3）。实验表明（第4.2.2节，图5(b)），通过显式提供运动感知条件，系统1在接收不同频率的VDM更新时，能自适应地生成稳定动作，在系统更新频率比为1:8时取得效率与精度的最佳平衡。

#### **4. 方法概述**
Video2Act的整体框架如图3所示，其运作流程如下：

**A. 问题定义与模型组件**：
遵循VLA模仿学习范式，目标是学习一个条件策略 `π_θ`，在给定多模态观测 `o_t`（包含图像序列 `I_{t-T:t}` 和机器人状态 `s_t`）和语言指令 `l` 的情况下，生成未来 `H` 步的动作块 `a_{t+1:t+H}`。模型主要组件包括：
*   **图像与文本编码器**：使用SigLIP-ViT-L/14提取视觉词元，使用T5-XXL提取文本嵌入。
*   **系统2（VDM）**：采用Hunyuan视频扩散变换器模型，包含预训练的3D VAE和60个Transformer块。使用前25个块，并基于扩散反演技术（在去噪步 `t_diff=0` 时）提取特征，以保留原始表征并最小化去噪伪影（公式(1)）。
*   **系统1（动作头）**：采用一个10亿参数的扩散变换器。

**B. 时空运动表征提取流程**：
1.  **双流特征提取**：为了兼顾空间细节和长时运动，输入被分解为两个互补的视频流，分别由VDM处理（公式(2)）：
    *   **高分辨率短时序流**：输入 `I^H`（512x768，`T_s=2`帧），得到特征 `F^H`，用于空间分析。
    *   **常分辨率长时序流**：输入 `I^L`（256x256，`T_l=16`帧），得到特征 `F^L`，用于运动分析。
2.  **表征精炼**：
    *   **空间过滤**：对 `F^H` 的每个通道应用3x3 Sobel算子（水平核 `S_x` 和垂直核 `S_y`），通过卷积计算梯度 `G_x`, `G_y`，并合成梯度幅值 `S_t`，增强结构边界。
    *   **运动过滤**：对 `F^L` 的每个空间位置和通道，沿时间轴进行一维DFT得到频域表示 `\widehat{F}`，应用高通频率掩码 `B` 过滤低频背景，再通过逆DFT重构回时域，得到突出运动模式的表征 `M_t`。
3.  **特征压缩**：提取的 `S_t` 和 `M_t` 通过两个轻量级Q-Former进行压缩，以减少词元冗余并保持全局一致性。

**C. 异步双系统训练与推理**：
*   **训练**：系统1（DiT动作头）在条件扩散目标下进行训练（公式(4)）。在每一步扩散中，向动作序列添加高斯噪声 `ϵ`，系统1 `D_θ` 预测噪声，条件包括高频图像特征 `F_I`、低频VDM特征 `F_VDM = Cat(S_t, M_t)` 和文本特征 `F_l`。这些条件通过交叉注意力层注入到DiT块中。
*   **推理**：采用异步策略。系统2以低频率（如每8个控制步）运行一次，更新 `S_t` 和 `M_t`。系统1在每个控制步都运行，接收最新的图像观测，并以上一次系统2提供的 `S_t` 和 `M_t` 为条件，生成实时动作。这使得整个系统能以高频率（约380 Hz）进行闭环控制，同时享受VDM提供的丰富时空先验。

#### **5. 实验说明**
*   **评估指标**：任务成功率（Success Rate, S.R.），在仿真中报告平均值及方差。
*   **数据集**：
    *

---

