# arXiv论文监控报告 - 2026年01月29日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2026年01月29日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 7篇

---

## 1. MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption

### 基本信息
- **作者**: Chen Li, Zhantao Yang, Han Zhang, Fangyi Chen, Chenchen Zhu, Anudeepsekhar Bolimera, Marios Savvides
- **arXiv ID**: [oai:arXiv.org:2510.05580v2](https://arxiv.org/abs/2510.05580)
- **发布日期**: Wed, 28 Jan 2026 00:00:00 -0500
- **分类**: cs.AI, cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.05580)

            ### 原文摘要
            arXiv:2510.05580v2 Announce Type: replace  Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, incur high compute costs, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption》内容，生成一份结构清晰、内容详实的总结报告。

***

### **论文概要**

本文针对具身智能中视觉-语言-动作（VLA）模型在适应新任务时面临的高计算成本、泛化能力差以及需要为每个任务单独微调的问题，提出了MetaVLA框架。该框架是一种与主干网络无关的后训练（post-training）方法，通过引入**上下文感知元协同训练（Context-Aware Meta Co-Training）**，将多个目标任务与结构多样的辅助任务统一在一个训练阶段中。其核心是一个轻量级的**元动作推理器（Meta-Action-Reasoner, MAR）** 模块，该模块基于注意力神经过程（ANP），能够从多样化的上下文数据中快速提取和适应知识，而无需显著增加推理开销。实验表明，在LIBERO基准测试上，MetaVLA仅用单一模型和更少的训练步骤，即可超越基线方法，在长视野任务上提升高达8.0%，同时将GPU训练时间减少约76%。

### **研究动机**

当前VLA模型在适应下游具身任务时，普遍采用**针对每个任务进行独立监督微调（SFT）** 的策略（如OpenVLA的做法，见第1节及参考文献Kim et al. (2024)）。这种做法存在三个主要缺陷：1) **效率低下**：每个任务都需要单独的训练流程，导致总体训练成本高昂（例如，OpenVLA在LIBERO四个任务套件上总计需要240K步训练）；2) **知识隔离**：任务间缺乏知识共享，限制了模型从相关任务中学习通用技能的能力；3) **泛化脆弱**：在单个任务上长时间微调可能导致过拟合，且对新任务变体的适应速度慢。

一种直观的改进是**朴素的多任务协同训练（naive multi-task SFT）**，即用一个模型同时训练所有目标任务（论文中称为SFT-4LIBERO）。作者实验发现，这虽然能减少总训练时间并略微提升成功率，但存在优化瓶颈（第1节）。当作者尝试引入更多**领域多样化（domain-diverse）的辅助任务**以进一步提升模型时，发现朴素的多任务SFT性能会急剧下降（见表1中SFT-4LIBERO+auxiliary tasks的结果）。作者将此归因于特征空间（如相机视角）和动作空间（如自由度）的异质性分布导致的**优化不稳定**（第1节）。

因此，论文的研究动机在于填补一个关键空白：**如何在VLA的后训练阶段，设计一种能够高效、稳定地整合多样化辅助任务，从而显著提升模型在目标任务上的适应效率、泛化能力和收敛速度的统一框架**，同时避免独立微调的高成本和朴素多任务训练的优化崩溃问题。

### **核心贡献与创新点**

1.  **提出了“上下文感知元协同训练”的后训练范式**：这是论文最核心的概念创新。与现有工作要么进行独立任务微调，要么在预训练阶段进行协同训练不同（第2.2节），MetaVLA专注于**后训练阶段的高效任务共享适应**。它将所有目标任务（如LIBERO的四个套件）与精心挑选的辅助任务（来自GR00T数据集）统一在一个训练流程中，通过一个共享的**上下文记忆库（Context Bank）** 进行知识聚合与条件化（见第3.2.2节及图2）。这实现了“一个模型适应所有任务”，显著减少了模型数量和训练成本。

2.  **设计了轻量级、可插拔的元动作推理器（MAR）模块**：这是实现上述范式的关键技术创新。MAR基于注意力神经过程（ANP）构建（第3.2.1节），被集成到VLA主干（如Llama2解码器）中。其创新性在于：
    *   **结构化上下文利用**：MAR通过自注意力（self-attention）和交叉注意力（cross-attention）机制，显式地对上下文（辅助+部分目标）数据对 `(xC, yC)` 进行建模，生成全局先验和任务特定的隐表示 `(rT, z)`，而非简单地将所有数据混合（见公式(1)）。
    *   **变分训练目标**：MAR的训练通过最大化证据下界（ELBO）进行（公式(2)），包含一个重构项 `log p(yT |xT , rT , z)` 和一个KL散度正则项 `DKL(q(z|¯sT ) ∥q(z|¯sC))`。该正则项旨在防止目标任务的隐分布过度偏离上下文分布，从而稳定训练（第4.4.6节讨论了其作用与取舍）。
    *   **骨干无关性与低开销**：MAR是一个紧凑的附加模块，不改变主干网络结构，推理时仅增加约0.3毫秒/令牌的延迟（第4.5节），体现了其工程友好性。

3.  **系统性地验证了利用领域多样化辅助任务提升目标任务的可行性**：论文通过详实的实验（第4.4.3节及表1）证明，在MAR的框架下，引入与目标任务在视角（侧视图vs正视图）、执行器（双臂14-DoF vs单臂7-DoF）等方面存在差异的辅助任务（见图3），能够**稳定地提升**模型在所有目标任务上的性能。这与朴素多任务SFT加入辅助任务后性能崩溃的结果形成鲜明对比，验证了MAR在缓解异质分布优化不稳定方面的有效性。

### **方法概述**

MetaVLA的方法流程围绕MAR模块和上下文记忆库展开，具体运作如下：

1.  **数据准备与记忆库构建**（第3.2.2节）：
    *   **目标数据**：来自目标任务（如LIBERO的四个套件）。每个套件的数据被划分为**上下文集（Context Set）** 和**目标集（Target Set）**。
    *   **辅助数据**：从GR00T数据集中选取，这些任务在机器人形态、相机视角上与LIBERO不同，但同属操作任务，具有部分领域相关性（第3.3节）。
    *   **上下文记忆库**：一个统一的内存，**聚合**了所有目标任务的上下文集和选定的辅助任务数据。
    *   **目标数据银行**：包含所有目标任务的目标集。

2.  **训练流程**（第3.2.3节及图2）：
    *   在每`K=200`个训练步，从上下文记忆库的每个任务中随机采样 `bC=32` 个样本对 `(xC, yC)`，构成当前批次的上下文数据。
    *   对于每个来自目标数据银行的训练样本 `(xT, yT)`：
        a.  **上下文编码**：MAR通过自注意力处理上下文对 `(xC, yC)`，生成每个上下文的表示 `rCi` 和 `sCi`，并计算全局先验 `¯sC`（`sCi`的均值）。
        b.  **目标查询与隐变量采样**：目标观测 `xT` 作为查询，与上下文键 `xCi` 和值 `rCi` 进行交叉注意力，得到目标相关表示 `rT`。同时，从以 `¯sC` 为参数的后验分布 `q(z|¯sC)` 中采样隐变量 `z`（在非随机版本中可省略，见第4.4.6节）。
        c.  **动作解码**：将 `rT` 和 `z` 拼接，并与主干网络（如Llama2）的隐藏状态融合，最后通过语言模型头输出动作令牌的预测分布。
        d.  **损失计算**：根据公式(2)计算ELBO损失。其中，重构损失基于预测动作与真实动作 `yT`；KL损失需要计算目标自身的 `¯sT`（由 `(xT, yT)` 通过相同自注意力过程得到），并衡量 `q(z|¯sT)` 与 `q(z|¯sC)` 的差距。训练时，整个模型（包括主干和MAR）端到端地进行优化。

3.  **推理流程**：训练完成后，得到一个统一的MetaVLA模型。对于任何目标任务，只需输入当前观测 `xT` 和指令，模型利用其内部通过训练学习到的、由丰富上下文知识塑造的MAR参数，即可直接解码出动作 `yT`，无需任何任务特定的调整。

### **实验说明**

*   **评估指标**：任务成功率（Success Rate, SR），即在多个任务实例中成功完成指令的比例（第4.1节）。
*   **主要数据集**：
    *   **目标数据集**：**LIBERO**基准，包含四个任务套件：LIBERO-Goal（目标变化）、LIBERO-Spatial（布局变化）、LIBERO-Object（物体变化）、LIBERO-Long（长视野混合变化）（第4.1节）。
    *   **辅助数据集**：从**GR00T**仿真数据集中选取，包含单臂侧视图、双臂正视图等与LIBERO存在领域差异的操作任务（第3.3节）。
*   **对比基线方法**：
    1.

---

## 2. LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries

### 基本信息
- **作者**: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen
- **arXiv ID**: [oai:arXiv.org:2601.15197v4](https://arxiv.org/abs/2601.15197)
- **发布日期**: Wed, 28 Jan 2026 00:00:00 -0500
- **分类**: cs.AI, cs.CL, cs.CV, cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.15197)

            ### 原文摘要
            arXiv:2601.15197v4 Announce Type: replace  Abstract: Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose LangForce, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $\pi(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, LangForce significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.


            
### AI分析（基于论文正文）
好的，作为一名资深论文总结者，我将严格遵循您提供的结构和要求，对论文《LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries》进行详实、客观的分析与总结。

***

### **论文概要**

本文针对视觉-语言-动作（VLA）模型在机器人操作任务中泛化能力不足的问题，指出其根源在于目标驱动数据收集导致的“信息坍缩”现象。由于数据集中视觉观察与语言指令高度相关，模型倾向于学习仅依赖视觉的捷径策略，而忽略语言指令。为此，本文提出LangForce框架，通过引入可学习的**潜在动作查询**，构建双分支架构分别建模视觉先验策略和语言后验策略，并最大化动作与指令之间的条件点互信息，从而强制模型遵循指令。实验在SimplerEnv和RoboCasa基准上验证了该方法的有效性，尤其在分布外泛化上取得了显著提升。

### **研究动机**

当前VLA模型虽然在分布内任务上表现良好，但在面对新指令或复杂多任务场景，特别是分布外环境时，泛化能力严重不足（见第1节引言）。作者通过深入分析，将这一问题的根源追溯到当前机器人数据集收集的固有偏差上。

大多数机器人数据集是以目标驱动方式收集的，即操作员在固定场景中重复执行特定任务（见第1节）。这导致视觉场景 `v` 与语言指令 `ℓ` 之间存在近乎确定性的映射关系（例如，看到柜子就对应“打开柜子”）。从贝叶斯视角看，最优策略可分解为 `π(a | v, ℓ) = p(ℓ| a, v) p(a | v) / p(ℓ| v)`（公式1）。当 `p(ℓ| v)` 非常尖锐（即 `H(ℓ| v) ≈ 0`）时，模型仅凭视觉即可预测指令，使得似然项 `p(ℓ| a, v)` 坍缩至 `p(ℓ| v)`，最终导致后验策略退化为视觉先验：`π(a | v, ℓ) ≈ p(a | v)`（公式2）。作者将这一现象称为“信息坍缩”（见第2.4节），其结果是模型学会了“视觉捷径”，在任务模糊或环境变化时完全失效。

为证实这一假设，作者在第2节“动机：指令遵循的幻觉”中设计并报告了三个预备实验：
1.  **在分布内测试中的视觉捷径**：在RoboCasa数据上，仅使用视觉（屏蔽语言）训练的模型成功率（44.6%）与完整VLA基线（47.8%）非常接近，表明模型无需语言即可完成任务（见第2.1节）。
2.  **在模糊场景中的失败**：在LIBERO Goal子集上，由于同一视觉场景对应多个可能任务，仅视觉模型成功率暴跌至9.8%（基线为98.0%），证明模型在模糊性面前无法仅凭视觉做出正确决策（见第2.2节）。
3.  **在分布外泛化中的灾难性失败**：在BridgeDataV2上训练，在视觉风格迥异的SimplerEnv上评估时，仅视觉模型成功率接近0%，表明其在训练中过拟合于领域特定的视觉模式，而非学习可泛化的技能（见第2.3节）。

这些实验共同揭示了标准VLA训练范式的一个关键病理：模型并未真正学习语言条件策略，而是退化为了一个视觉策略。这构成了本文研究的核心动机——设计一种方法，即使在使用存在偏差的数据集时，也能强制模型恢复并遵循语言指令。

### **核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下三个方面，每一项均有具体的技术实现和理论依据支撑：

1.  **对“视觉捷径”病理的识别与实证验证**：本文不仅从理论上（第2.4节）分析了目标驱动数据集导致条件互信息 `I(ℓ; a | v)` 坍缩的机制，更通过第2节所述的三个系统性实验提供了坚实的经验证据。这些实验清晰地展示了标准VLA模型在不同场景下（分布内、模糊、分布外）如何以及为何会退化为视觉策略，为后续方法设计提供了明确的靶点。这与前人工作（如引言中引用的Xing et al., 2025; Xu et al., 2025）仅指出泛化问题不同，本文精准定位了问题的信息论根源。

2.  **提出LangForce框架：基于潜在动作查询和双分支贝叶斯目标的方法**：这是本文最核心的概念与方法创新。
    *   **贝叶斯分解与LLR目标**：基于公式1的分解，作者提出最大化动作 `a` 与指令 `ℓ` 之间的条件点互信息（PMI），这等价于最大化后验策略与视觉先验策略的对数似然比（LLR）：`LLLR = log π(a | v, ℓ) - log p(a | v) = log p(ℓ| a, v) - log p(ℓ| v)`（公式4，推导见附录B）。该目标直接惩罚视觉捷径，奖励那些能为指令提供超出视觉可预测范围之外信息的动作（见第3.1节）。
    *   **潜在动作查询的创新设计**：为实现上述分解，本文引入了**潜在动作查询**（`Q`，K=64个可学习token）。这一设计具有双重目的（见第3.2节）：
        *   **作为瓶颈接口**：与π0、GR00T等工作将全部输入token的隐状态馈送给动作专家不同，LangForce仅使用 `Q` 对应的隐状态 `H_Q` 来条件化后续的扩散Transformer动作头。这迫使VLM将任务相关信息压缩并汇总到这一小组潜在token中。
        *   **实现双分支信息控制**：利用仅解码器VLM的因果注意力掩码，通过改变 `Q` 在输入序列 `[v, Q, ℓ]` 或 `[v, ℓ, Q]` 中的位置，可以精确控制 `Q` 只能关注视觉（先验分支）或同时关注视觉和语言（后验分支），从而在同一套VLM参数下实例化出 `p(a | v)` 和 `π(a | v, ℓ)`（见第3.3节及图3）。

3.  **验证了方法在提升泛化能力及保留VLM通用能力方面的有效性**：实验部分（第4节）表明，LangForce在SimplerEnv和RoboCasa基准上均达到了最先进的性能，特别是在SimplerEnv的分布外泛化上相比基线QwenGR00T取得了8.8%的绝对提升（见表1）。此外，一个重要的**衍生贡献**是，作者发现并论证了LangForce能够有效保留骨干VLM的纯文本对话和推理能力（见图4，图5），而标准VLA基线则出现了灾难性遗忘。作者在第4.3节中分析认为，LLR目标强制模型保持对语言的强依赖，这起到了正则化作用，防止了共享参数的语言功能漂移。

### **方法概述**

LangForce方法的核心运作流程围绕**潜在动作查询**和**双分支训练框架**展开，旨在优化公式4定义的LLR目标。其整体架构如图3所示。

**1. 输入与模型组件**：
*   **输入**：视觉token序列 `v`，语言token序列 `ℓ`，以及可学习的潜在动作查询 `Q`。
*   **骨干网络**：基于Qwen3-VL等预训练VLM（仅解码器架构）。
*   **动作专家**：一个扩散Transformer，用于预测连续动作轨迹。

**2. 双分支训练流程**：
训练时，共享权重的VLM同时以两种不同的输入顺序运行，形成两个分支：
*   **先验分支**：输入序列为 `[v, Q, ℓ]`（公式5）。由于因果掩码，`Q` 只能关注到其前方的视觉token `v`，而无法看到后方的语言token `ℓ`。因此，该分支提取的隐状态 `H_Q^prior` 编码了纯视觉信息，用于学习数据集固有的视觉动作先验分布 `p(a | v)`。
*   **后验分支**：输入序列为 `[v, ℓ, Q]`（公式6）。此时 `Q` 可以关注到其前方的所有token（`v` 和 `ℓ`）。因此，`H_Q^post` 编码了完整的视觉-语言上下文，用于学习目标语言条件策略 `π(a | v, ℓ)`。

**3. 损失函数与优化**：
总损失函数 `L_total` 由三部分构成（公式9）：
*   **动作匹配损失**：使用修正流匹配目标（公式8）分别训练两个分支的动作预测。`L_FM(ψ; H_Q^post)` 是主损失，`L_FM(ψ; H_Q^prior)` 是辅助损失，由超参数 `λ` 平衡。
*   **LLR最大化损失**：这是实现核心创新的关键。作者利用VLM的语言建模损失作为 `log p(ℓ| ...)` 的代理。具体地，LLR损失定义为（公式7）：
    `L_LLR = log p(ℓ| v,

---

## 3. Time series forecasting with Hahn Kolmogorov-Arnold networks

### 基本信息
- **作者**: Md Zahidul Hasan, A. Ben Hamza, Nizar Bouguila
- **arXiv ID**: [oai:arXiv.org:2601.18837v1](https://arxiv.org/abs/2601.18837)
- **发布日期**: Wed, 28 Jan 2026 00:00:00 -0500
- **分类**: cs.LG, stat.ML
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.18837)

            ### 原文摘要
            arXiv:2601.18837v1 Announce Type: new  Abstract: Recent Transformer- and MLP-based models have demonstrated strong performance in long-term time series forecasting, yet Transformers remain limited by their quadratic complexity and permutation-equivariant attention, while MLPs exhibit spectral bias. We propose HaKAN, a versatile model based on Kolmogorov-Arnold Networks (KANs), leveraging Hahn polynomial-based learnable activation functions and providing a lightweight and interpretable alternative for multivariate time series forecasting. Our model integrates channel independence, patching, a stack of Hahn-KAN blocks with residual connections, and a bottleneck structure comprised of two fully connected layers. The Hahn-KAN block consists of inter- and intra-patch KAN layers to effectively capture both global and local temporal patterns. Extensive experiments on various forecasting benchmarks demonstrate that our model consistently outperforms recent state-of-the-art methods, with ablation studies validating the effectiveness of its core components.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Time series forecasting with Hahn Kolmogorov-Arnold networks》和所有约束条件，生成一份详实、结构清晰的论文总结。

***

### **论文总结：Time series forecasting with Hahn Kolmogorov-Arnold networks**

#### **1. 论文概要**
本文提出了一种名为HaKAN的新型多元长期时间序列预测框架。该框架基于Kolmogorov-Arnold Networks (KANs)，并利用Hahn多项式作为可学习激活函数的参数化基础。HaKAN整合了通道独立、分块、堆叠的Hahn-KAN块（包含块内和块间KAN层）以及瓶颈结构。其核心设计旨在高效地捕捉时间序列中的局部和全局时序模式，同时克服Transformer的二次复杂性和MLP的频谱偏差问题。在多个基准数据集上的实验表明，HaKAN在多种预测长度上均优于现有先进方法。

#### **2. 研究动机**
论文的研究动机源于解决现有主流时间序列预测模型存在的固有缺陷。作者在引言和相关工作部分（第1、2节）系统性地指出了这些不足：
*   **Transformer类模型的局限性**：尽管基于注意力机制的模型（如Informer, Autoformer, PatchTST）在捕捉长程依赖方面表现出色，但其自注意力机制存在两个关键问题（见第1节）。一是计算复杂度与序列长度的平方成正比（O(L²)），在处理长序列时计算开销巨大。二是其置换等变性（permutation-equivariant）与时间序列固有的因果顺序相矛盾，可能损害对时序信息的建模。
*   **MLP类模型的局限性**：MLP模型（如DLinear, TSMixer）虽然计算高效，但存在频谱偏差（spectral bias），即倾向于学习数据中的低频分量，难以有效建模高频成分（见第1节，引用Rahaman等人，2019）。此外，MLP主要依赖线性变换，在捕捉非线性时序动态方面能力有限，导致在非线性模式主导的数据集上表现欠佳。
*   **现有KAN模型的不足与改进空间**：近期出现的KAN模型（如TimeKAN, TsKAN）为解决上述问题提供了新思路。KAN通过可学习的激活函数（通常用样条参数化）提供了更灵活的函数逼近能力，并能缓解频谱偏差（见第1节，引用Wang等人，2025b）。然而，标准KAN使用B样条作为激活函数，需要网格离散化，其计算和参数复杂度与网格大小G相关，并非最优（见第3.2节“Why KAN with Hahn Polynomials?”）。因此，论文动机在于设计一种更高效、轻量级的KAN变体，专门针对时间序列预测任务进行优化，以综合超越Transformer和MLP模型的性能。

#### **3. 核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下三个方面，均超越了现有工作：
1.  **提出基于Hahn多项式的KAN层参数化方法**：这是本文最核心的概念性创新。与标准KAN使用B样条（B-splines）不同，HaKAN采用Hahn正交多项式作为KAN层中可学习激活函数ϕq,p的参数化基础（见公式(4)）。Hahn多项式定义在离散域上，具有闭式的递推关系（公式(5)-(7)），无需网格离散化。这一创新带来了显著优势（见第3.2节）：**（a）计算高效**：时间复杂度从标准KAN的O(dᵢₙdₒᵤₜ[9d(G+1.5d)+...])简化为O(dᵢₙdₒᵤₜd)（d为多项式阶数，通常为3），与MLP的O(dᵢₙdₒᵤₜ)相当。**（b）参数高效**：参数量从标准KAN的(dᵢₙdₒᵤₜ(G+d+3)+dₒᵤₜ)减少到(dᵢₙdₒᵤₜ(d+1))。**（c）全局逼近**：Hahn多项式作为正交基，提供了全局函数逼近能力，避免了样条方法的局部性限制。
2.  **设计新颖的层次化Hahn-KAN块架构**：论文提出了一个由**块内（intra-patch）KAN层**和**块间（inter-patch）KAN层**组成的核心模块（见公式(3)和图1）。该设计是对传统MLP-Mixer或注意力模块在KAN范式下的创新性重构。**块内KAN层**（特征混合）作用于每个时间块（patch）内部，专注于捕捉局部、细粒度的时序模式（如短时波动）。**块间KAN层**（块混合）作用于不同时间块之间，负责建模全局、长程的时序依赖（如跨越整个回顾窗口的趋势）。这种明确的层次化分工，使得模型能够协同地捕捉多尺度时序特征，超越了单一混合层（如MLP）或注意力机制的设计。
3.  **构建完整、高效的HaKAN预测框架**：论文将上述创新点整合到一个完整的端到端框架中。该框架系统地集成了多项已被证明有效的技术，包括**通道独立**（Channel Independence）以保持各变量的独特动态、**可逆实例归一化**（RevIN）以处理分布偏移、**分块**（Patching）以提升计算效率和局部信息保留、**位置编码**以注入时序顺序信息，以及一个**瓶颈结构**（两个全连接层）用于高效地将高维特征映射到预测长度。这种集成并非简单堆砌，而是以Hahn-KAN块为核心，形成了一个在表达力、效率和可解释性之间取得平衡的轻量级模型（见第3.2节及算法1）。

#### **4. 方法概述**
HaKAN方法的工作流程清晰，其核心运作机制如下（对应图1和算法1）：
*   **输入与预处理**：给定多元时间序列X₁:ʟ ∈ ℝᴸˣᴹ，模型对每个通道i独立处理（通道独立）。首先，对单通道序列x⁽ⁱ⁾应用可逆实例归一化（RevIN）进行标准化。接着，将标准化后的序列通过一个滑动窗口（窗口大小P，步长S）划分为N个重叠的时间块（Patching），得到Xₚ⁽ⁱ⁾ ∈ ℝᴺˣᴾ。
*   **嵌入表示**：每个时间块通过一个可学习的线性投影矩阵Wₚ ∈ ℝᴾˣᴰ映射到D维嵌入空间。同时，添加一个可学习的位置编码矩阵Wₚₒₛ ∈ ℝᴺˣᴰ以保留块的时序顺序，得到嵌入序列X_d⁽ⁱ⁾ = Xₚ⁽ⁱ⁾Wₚ + Wₚₒₛ ∈ ℝᴺˣᴰ（公式(2)）。
*   **核心特征提取：堆叠的Hahn-KAN块**：嵌入序列X_d⁽ⁱ⁾被送入一个由R个相同的Hahn-KAN块组成的堆栈中。每个块是模型的核心，其计算如公式(3)所示：`X_k⁽ⁱ⁾ = KAN(KAN(X_d⁽ⁱ⁾)ᵀ)ᵀ + X_d⁽ⁱ⁾`。这里包含两个关键操作：
    *   **第一个KAN(·)**：这是一个**块内KAN层**。它接收X_d⁽ⁱ⁾ ∈ ℝᴺˣᴰ，其中每行代表一个块的D维特征。该层在特征维度D上进行混合，即对每个块的D个特征应用KAN变换，学习块内的局部模式。输出维度仍为ℝᴺˣᴰ。
    *   **转置与第二个KAN(·)**：将上述输出转置后，送入**块间KAN层**。此时输入维度为ℝᴰˣᴺ，每行代表所有N个块在某一特征维度上的值。该层在块数量N维度上进行混合，学习不同时间块之间的全局依赖关系。输出转置回ℝᴺˣᴰ。
    *   **残差连接**：块间KAN层的输出与原始输入X_d⁽ⁱ⁾相加，形成残差连接，以稳定训练并促进梯度流动。
    *   **Hahn多项式参数化**：上述两个KAN层中的每一个可学习激活函数ϕq,p均由公式(4)定义，即Hahn多项式的线性组合。系数γq,p,r是可学习的参数。多项式的阶数d固定为3，参数(a, b, n)设置为(1, 1, 7)。
*   **输出预测**：经过R个块后，输出X_k⁽ⁱ⁾ ∈ ℝᴺˣᴰ被展平为向量x_f⁽ⁱ⁾ ∈ ℝᴺᴰ。该向量通过一个**瓶颈结构**映射到预测长度T：首先通过一个下投影层h⁽ⁱ⁾ = W_dₒwₙ x_f⁽

---

## 4. A Comprehensive Survey of Deep Learning for Multivariate Time Series Forecasting: A Channel Strategy Perspective

### 基本信息
- **作者**: Xiangfei Qiu, Hanyin Cheng, Xingjian Wu, Junkai Lu, Jilin Hu, Chenjuan Guo, Christian S. Jensen, Bin Yang
- **arXiv ID**: [oai:arXiv.org:2502.10721v3](https://arxiv.org/abs/2502.10721)
- **发布日期**: Wed, 28 Jan 2026 00:00:00 -0500
- **分类**: cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2502.10721)
- **源码地址**: [查看源码](https://github.com/decisionintelligence/cs4ts))

            ### 原文摘要
            arXiv:2502.10721v3 Announce Type: replace  Abstract: Multivariate Time Series Forecasting (MTSF) plays a crucial role across diverse fields, ranging from economic, energy, to traffic. In recent years, deep learning has demonstrated outstanding performance in MTSF tasks. In MTSF, modeling the correlations among different channels is critical, as leveraging information from other related channels can significantly improve the prediction accuracy of a specific channel. This study systematically reviews the channel modeling strategies for time series and proposes a taxonomy organized into three hierarchical levels: the strategy perspective, the mechanism perspective, and the characteristic perspective. On this basis, we provide a structured analysis of these methods and conduct an in-depth examination of the advantages and limitations of different channel strategies. Finally, we summarize and discuss some future research directions to provide useful research guidance. Moreover, we maintain an up-to-date Github repository (https://github.com/decisionintelligence/CS4TS) which includes all the papers discussed in the survey.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文节选内容，为您生成一份符合要求的、详实的论文总结。

***

### **论文总结**

**论文标题：** A Comprehensive Survey of Deep Learning for Multivariate Time Series Forecasting: A Channel Strategy Perspective
**作者：** Xiangfei Qiu, Hanyin Cheng, Xingjian Wu, Junkai Lu, Jilin Hu, Chenjuan Guo, Christian S. Jensen, Bin Yang
**arXiv ID：** oai:arXiv.org:2502.10721v3

---

#### **1. 论文概要**

本文是一篇关于深度学习在多变量时间序列预测（MTSF）领域的综述性论文。论文聚焦于“通道策略”这一核心设计维度，旨在系统性地梳理和总结现有方法如何建模不同通道（变量）之间的相关性。作者提出了一个包含三个层次（策略视角、机制视角、特征视角）的新颖分类法，对现有方法进行了结构化分析，并深入探讨了不同通道策略的优势与局限。最后，论文总结了未来的研究方向，并维护了一个包含所有讨论论文的GitHub仓库。

#### **2. 研究动机**

多变量时间序列预测在众多领域至关重要。现有研究主要从时间维度和通道维度对MTSF进行建模。尽管已有若干关于MTSF的综述（如Wen等人，2023；Wang等人，2024d），但这些综述往往缺乏对通道策略在多元设置中作用的全面讨论（见第1节）。作者指出，在MTSF中，建模不同通道之间的相关性是提升预测精度的关键，因为利用其他相关通道的信息可以显著改善特定通道的预测。然而，如何选择和处理这些通道间关系（即通道策略）是模型设计的关键考量，现有文献对此缺乏系统性的梳理和比较。

具体而言，论文观察到现有方法在处理通道关系时呈现出不同的哲学和实现方式，但缺乏一个统一的框架来理解和对比它们。这种缺失使得研究人员难以根据数据特性（如通道间相关性强弱）选择或设计最合适的模型。因此，本研究的核心动机是填补这一空白，首次从通道策略的视角，对基于深度学习的MTSF方法进行全面、系统的回顾，提供一个结构化的分类和分析框架，以帮助研究者更好地理解该领域的发展脉络和设计选择。

#### **3. 核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下三个方面：

1.  **首次提出以“通道策略”为核心的MTSF分类法**：这是本文最核心的概念性创新。作者摒弃了传统的按模型架构（如Transformer、GNN）分类的方式，转而根据模型如何处理通道间关系，提出了三种基础策略：通道独立（CI）、通道依赖（CD）和通道部分依赖（CP）（见第3.1节，图1）。这一分类法为理解纷繁复杂的MTSF模型提供了一个清晰、统一且本质的视角。

2.  **构建了一个三层次、结构化的分析框架**：在基础策略分类之上，论文进一步深化，构建了一个包含“策略-机制-特征”的三层次分类法（见表1）。**策略视角**定义了CI、CD、CP三种基本范式。**机制视角**（第3.2节）深入探讨了每种策略是如何通过具体的技术模块（如基于Transformer的注意力、基于MLP的混合、基于CNN的合并、基于GNN的图消息传递等）实现的，并细分为多种子类型（如朴素注意力、路由注意力、频率注意力、掩码注意力；简单图、时空图、超图、时序图等）。**特征视角**（第3.3节，图5）则总结了通道相关性可能具备的六种特性：非对称性、滞后性、极性、分组性、动态性和多尺度性，并分析了不同方法对这些特性的建模能力。这个框架不仅对现有工作进行了系统归类，还揭示了方法设计的内在逻辑和演进路径。

3.  **基于分类法的系统性比较与实证分析**：论文并非简单的文献罗列，而是基于提出的分类法进行了深入的比较分析。第4节通过设计实验，直观地展示了不同通道策略的性能与效率权衡。例如，图6的实验表明，在通道间相关性较弱的数据集上，CI策略因避免了噪声干扰而表现最佳；而在强相关性数据集上，CP和CD策略则能更好地利用通道间信息，其中CP策略因其灵活性展现出最强的泛化能力。图7则从参数量和推理时间角度，量化比较了不同策略的计算效率，证实了CI策略在效率上的显著优势。这种基于统一基准和分类维度的比较，为模型选择提供了实证依据。

#### **4. 方法概述**

本文作为一篇综述，其“方法”并非提出一个新算法，而是系统性地梳理和分析了现有MTSF方法中通道策略的实现机制。其方法概述体现在对现有技术方案的归纳与解构上：

**核心框架**：论文将所有MTSF模型抽象为一个通用流程（见图1）：输入经过嵌入层后，通过**时间依赖模块**（如CNN、MLP、Transformer）捕获单个通道的时间模式，同时通过**通道相关性模块**（即通道策略）建模通道间关系，最后进行预测。论文的核心是详细剖析“通道相关性模块”的不同实现。

**机制详解**：
*   **基于Transformer的机制**（第3.2节，图2）：这是当前主流。论文将其细分为四类：
    *   **朴素注意力**：将每个通道的时间序列或片段视为一个token，直接应用自注意力计算所有通道对之间的相关性，属于CD策略（如iTransformer）。
    *   **路由注意力**：为降低O(N²)复杂度，引入少量“路由器”token作为信息中转站，先聚合所有通道信息再分发，属于高效的CD策略（如Crossformer）。
    *   **频率注意力**：在频域而非时域应用注意力机制来建模通道关系，属于CD策略（如FECAM）。
    *   **掩码注意力**：在朴素注意力基础上引入一个可学习的掩码矩阵，强制让每个通道只与部分相关通道交互，从而实现了CP策略（如DUET）。
*   **基于MLP/CNN的机制**（第3.2节，图3）：
    *   **MLP混合**：使用全连接层在不同通道的特征之间进行混合，直接学习通道间的全局依赖，属于CD策略（如TSMixer）。
    *   **CNN合并**：在卷积操作中，不同通道的输入特征图通过卷积核的权重进行加权合并，从而在局部感受野内实现通道交互，属于CD策略（如Informer, TimesNet）。
*   **基于GNN的机制**（第3.2节，图4）：将每个通道视为图中的一个节点，通道间关系视为边。
    *   若构建的是**稠密图**（每个节点与几乎所有其他节点相连），则属于CD策略（如GTS）。
    *   若构建的是**稀疏图**（每个节点只与少数节点相连，如K-正则图或通过阈值过滤），则属于CP策略（如MTGNN）。论文进一步将图类型分为简单图、时空图、超图和时序图，分析了它们如何支持CD或CP策略。
*   **其他机制**（第3.2节）：还介绍了如SOFTS的星型聚合分发结构、LIFT的领先指标插件、C-LoRA的通道感知低秩适配器等独特设计，并明确了它们所属的策略类别。

**特征整合**：论文进一步指出，上述机制在实现通道策略时，可能会考虑或不考虑第3.3节所述的六种通道相关性特征（如非对称性、动态性等）。例如，基于注意力的方法天然支持非对称性；基于时序图的方法（如MTSF-DG）能够建模相关性的动态演化；基于超图的方法（如ReMo）擅长处理分组性。

#### **5. 实验说明**

本文作为综述，其“实验”部分主要用于验证和说明其提出的分类观点，而非提出新模型的性能评估。

*   **评估指标**：主要使用均方误差（MSE）作为预测性能的评估指标（见图6）。
*   **数据集**：实验数据来源于时间序列预测基准TFB。论文从中选取了9个具有不同通道间相关性强度的数据集进行展示，包括：ETTm2, ETTh2, Exchange, AQShunyi, AQWang, Solar, PEMS04, PEMS08, PEMS-BAY（见图6标注）。
*   **对比基线方法**：论文从CI、CD、CP三类策略中各选取了若干代表性模型进行对比。根据图6和图7，涉及的模型包括：
    *   **CI策略**：SparseTSF, DLinear, PatchTST。
    *   **CD策略**：TimesNet, iTransformer, Crossformer。
    *   **CP策略**：DUET, ModernTCN, TimeFilter。
*   **实验条件**：论文中未明确说明训练、微调、推理所使用的具体GPU型号、数量及配置。实验设置参数在图表说明中提及：对于性能比较（图6），使用回看窗口长度为512，预测步长为96；对于效率比较（图7），使用批次大小为1，回看窗口512，预测步长96，并在ETTm2（弱相关）和PEMS-BAY（强相关）两个代表性数据集上测量。

#### **6. 改进建议和未来研究方向**

论文在第

---

## 5. AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation

### 基本信息
- **作者**: Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu
- **arXiv ID**: [oai:arXiv.org:2601.19634v1](https://arxiv.org/abs/2601.19634)
- **发布日期**: Wed, 28 Jan 2026 00:00:00 -0500
- **分类**: cs.RO, cs.MM
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.19634)

            ### 原文摘要
            arXiv:2601.19634v1 Announce Type: new  Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.


            
### AI分析（基于论文正文）
好的，我将以一位资深论文总结者的身份，为您提供一份关于《AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation》的详细总结。

***

### **论文概要**

本文针对视觉-语言-动作模型在机器人闭环操控中因重复执行大型多模态骨干网络而导致的高延迟和高计算成本问题，提出了一种名为AC^2-VLA的动作上下文感知自适应计算框架。该框架的核心是引入一个轻量级的动作先验路由器，该路由器基于当前视觉观察、语言指令和先前的动作状态，动态协调三种互补的效率机制：跨时间步的认知缓存重用、空间维度的动作上下文感知令牌剪枝以及深度维度的条件层跳过。通过一种动作引导的自蒸馏方案训练路由器，AC^2-VLA在保持与密集基线模型相当任务成功率的同时，实现了高达1.79倍的加速，并将FLOPs降低至基线模型的29.4%。

### **研究动机**

VLA模型（如RT-2、OpenVLA、CogACT）在机器人操控任务中展现出强大的泛化能力，但其闭环部署面临严峻挑战：在每个控制时间步都需要重复执行计算昂贵的视觉-语言骨干网络，导致高延迟和高计算成本，降低了控制频率，并可能损害动态环境中的实时响应能力（见第1节）。

为应对此挑战，现有工作探索了多种VLA模型效率机制，主要包括四类（见第2.2节）：1）轻量级模型设计（如TinyVLA）；2）动态路由与条件执行（如MoLe-VLA）；3）通过剪枝和量化的压缩（如EfficientVLA）；4）通过缓存的时域重用（如VLA-Cache）。然而，作者指出这些方法存在一个关键局限：它们主要基于视觉线索或静态启发式方法进行计算分配决策，这对于具身任务而言可能是次优的。在机器人操控中，视觉复杂度与操控难度并不必然相关。例如，视觉简单的场景（如精确抓取）可能需要全容量的推理，而视觉复杂的过渡阶段（如移动）则可能允许更激进的剪枝（见第1节及图1说明）。

因此，论文的核心动机是解决现有效率方法**忽视动作上下文**的问题。作者认为，在具身任务中，动作状态（如机器人上一时刻的动作）是决定当前推理需求更直接、更有效的信号。基于此洞察，本文旨在开发一个统一框架，能够利用动作上下文来协调时域、空域和深度维度的自适应计算，从而实现更高效、更鲁棒的闭环机器人操控。

### **核心贡献与创新点**

本文的核心贡献与创新点可归纳为以下三个方面：

1.  **提出了动作上下文感知的自适应计算范式**：本文首次系统性地论证并实现了将**动作上下文**作为VLA模型动态计算分配的核心指导信号。与现有基于视觉复杂度或静态规则的方法（如VLA-Cache、MoLe-VLA）不同，AC^2-VLA明确地将前一时刻的动作状态 `a_{t-1}` 编码为路由条件向量 `c_t` 的主要组成部分（见公式(5)）。这一概念性创新源于对具身任务本质的洞察：下一时刻的动作分布强烈依赖于持续的运动状态。这使得计算资源的分配与任务的实际需求（即“要做什么”）而非感知的表象（即“看到了什么”）更紧密地结合。

2.  **设计了一个统一协调的动作先验路由器**：本文提出了一个轻量级的**动作先验路由器**，作为实现上述范式的技术核心（见图2）。该路由器的创新性在于其**统一性**：它接收融合了动作、视觉和语言信息的条件向量 `c_t`，并同时输出三组控制门（见公式(2)）：
    *   **缓存重用门** `p^cache_t`：一个标量，预测重用缓存特征的概率（公式(6)）。
    *   **令牌剪枝门** `p^topk_t`：一个向量，为每个视觉令牌预测保留分数，其计算融入了动作条件匹配（公式(7)）。
    *   **层跳过门** `p^layer_t`：一个向量，为每个Transformer层预测执行概率（公式(8)）。
    这种统一设计使得模型能够联合优化时域、空域和深度维度的计算稀疏性，避免了现有方法（如图1所示）孤立应用不同机制可能导致的次优决策。

3.  **引入了动作引导的自蒸馏训练方案**：为了在引入结构化稀疏化（缓存、剪枝、跳层）的同时，保持原始密集策略的行为鲁棒性，本文提出了一种**动作引导的自蒸馏**优化策略（见第3.4节）。其创新点在于蒸馏目标不仅包含常见的特征匹配损失 `D(z^stu_t, z^tea_t)`，还**显式地包含了动作预测的蒸馏损失** `||\hat{\epsilon}^{stu} - \hat{\epsilon}^{tea}||^2_2`（见公式(17)）。这确保了学生模型（执行稀疏计算）在最终的动作输出上与教师模型（执行密集计算）保持一致，从而直接保障了任务性能。此外，训练中还加入了正则化项 `L_reg` 和时间平滑项 `L_temp`，以控制稀疏预算并确保闭环控制稳定性。

### **方法概述**

AC^2-VLA方法的核心流程如图2和算法1所示，其运作流程如下：

**1. 构建动作先验条件向量 `c_t`：**
在每个时间步 `t`，模型从以下输入构建路由条件：
*   **动作上下文**：前一时刻的动作 `a_{t-1}`，通过投影函数 `ψ_a` 嵌入。
*   **视觉摘要**：对视觉编码器输出的令牌特征 `V_t` 进行均值-最大值混合池化得到 `s^v_t`（公式(3)），再经 `ψ_v` 投影。
*   **语言摘要**：对嵌入的指令令牌 `E_t` 进行池化得到 `s^u_t`（公式(4)），再经 `ψ_u` 投影。
*   **动作头步索引**：使用正弦编码器 `e(τ_t)` 编码扩散去噪步索引 `τ_t`，再经 `ψ_τ` 投影。
*   **缓存状态提示**（可选）：当启用缓存时，加入缓存状态提示 `s^c_t`，再经 `ψ_c` 投影。
所有投影后的特征通过一个多层感知机 `f_fuse` 融合，生成统一的条件向量 `c_t`（公式(5)）。**关键细节**：视觉令牌和摘要特征在进入路由器前被分离（detach），以防止梯度通过路由路径流入重型骨干网络。

**2. 路由器生成统一门控信号：**
条件向量 `c_t` 输入路由器 `R`，并行生成三组门控信号（公式(6)-(8)）。这些门控值在训练时为连续概率，在推理时被二值化。

**3. 门控信号转化为实际加速操作：**
*   **缓存重用**：当 `p^cache_t` 高于阈值（`REUSEREQ`）时，尝试查询**认知缓存**。缓存键 `k_t` 由动作增量代理 `Quant(||Δa_t||)` 和轻量级视觉哈希 `Hash(\bar{v}_t)` 构成（公式(10)），兼顾运动连续性和视觉一致性。若缓存命中 (`h_t=1`)，则直接复用缓存的骨干网络表征 `z_t`，跳过前向传播；否则，正常计算 `z_t`。**关键机制**：仅当路由器请求重用但缓存未命中时，才将新计算的 `z_t` 写回缓存，确保缓存内容与路由意图一致。
*   **令牌剪枝**：根据 `p^topk_t` 生成保留掩码 `m_t`，并通过 `Compact` 操作物理移除被剪枝的令牌，缩短Transformer序列长度（公式(11)）。为保持RoPE位置编码的一致性，使用映射 `π_t` 为保留的令牌分配原始补丁位置（公式(12)），并为文本令牌分配紧随原始视觉序列之后的位置（公式(13)）。训练时采用软松弛（公式(14)）以保持可微性。
*   **层跳过**：根据 `p^layer_t` 为每个Transformer层 `ℓ` 生成门控系数 `α_{t,ℓ}`。层的计算被重写为门控残差更新：`h^{(ℓ+1)} = h^{(ℓ)} + α_{t,ℓ}(F_ℓ(h^{(ℓ)}) - h^{(ℓ)})`（公式(15)）。推理时，`α_{t,ℓ}` 二值化，被跳过的层完全绕过。为实现高效执行，活跃样本被动态分组为一个子批次来执行 `F_ℓ(·)`。

**4. 动作生成：**
最终，处理后的（或缓存的）骨干网络表征 `z_t` 被送入动作头 `p_φ`（如扩散模型）生成动作块 `a_{t:t+H}`。

### **实验说明**

**评估指标与数据集：**
*   **主要指标**：任务成功率（Success Rate）。
*   **效率指标**：相对于密集

---

## 6. Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework

### 基本信息
- **作者**: Hao Chang, Zhihui Wang, Lingxiang Wu, Peijin Wang, Wenhui Diao, Jinqiao Wang
- **arXiv ID**: [oai:arXiv.org:2601.19640v1](https://arxiv.org/abs/2601.19640)
- **发布日期**: Wed, 28 Jan 2026 00:00:00 -0500
- **分类**: cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.19640)

            ### 原文摘要
            arXiv:2601.19640v1 Announce Type: new  Abstract: Low-altitude vision systems are becoming a critical infrastructure for smart city governance. However, existing object-centric perception paradigms and loosely coupled vision-language pipelines are still difficult to support management-oriented anomaly understanding required in real-world urban governance. To bridge this gap, we introduce GovLA-10K, the first management-oriented multi-modal benchmark for low-altitude intelligence, along with GovLA-Reasoner, a unified vision-language reasoning framework tailored for governance-aware aerial perception. Unlike existing studies that aim to exhaustively annotate all visible objects, GovLA-10K is deliberately designed around functionally salient targets that directly correspond to practical management needs, and further provides actionable management suggestions grounded in these observations. To effectively coordinate the fine-grained visual grounding with high-level contextual language reasoning, GovLA-Reasoner introduces an efficient feature adapter that implicitly coordinates discriminative representation sharing between the visual detector and the large language model (LLM). Extensive experiments show that our method significantly improves performance while avoiding the need of fine-tuning for any task-specific individual components. We believe our work offers a new perspective and foundation for future studies on management-aware low-altitude vision-language systems.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文内容，生成一份符合要求的详细总结报告。

***

### **论文总结报告**

**论文标题：** Towards Governance-Oriented Low-Altitude Intelligence: A Management-Centric Multi-Modal Benchmark With Implicitly Coordinated Vision-Language Reasoning Framework

**1. 论文概要**
本文旨在解决面向城市治理的低空智能系统中存在的关键瓶颈。作者指出，现有研究侧重于通用的、穷举式的目标检测，缺乏针对异常、风险等治理相关事件的、可提供行动建议的基准数据集。同时，现有视觉-语言模型（VLM）与视觉检测器之间松耦合的显式协调范式存在信息丢失和误差累积问题。为此，本文提出了一个协同的基准-框架方案：1）**GovLA-10K**，首个面向城市治理的低空多模态基准数据集，专注于功能显著的目标并提供基于视觉证据的管理建议；2）**GovLA-Reasoner**，一个通过隐式特征空间协调统一视觉感知与语言推理的框架，避免了显式提示构建的缺陷。实验表明，该框架在无需微调任何组件的情况下，显著提升了描述生成性能。

**2. 研究动机**
论文的研究动机源于现有低空视觉系统在满足真实世界城市治理需求时存在的两个根本性差距（见第1节“Limitation”）。

首先，**数据层面的缺口**。当前主流低空数据集（如VisDrone）采用通用的、检测导向的标注策略，旨在标注场景中所有可见的感兴趣物体，以支持全面的视觉理解。然而，真实城市治理的核心并非无差别地识别所有物体，而是**有选择性地识别异常、风险或违规情况**（例如，非法停车车辆、建筑安全隐患）。这种从“全面识别”到“选择性确认”的转变，需要更高层次的语义解释和情境感知，超出了传统以物体为中心的感知范围。因此，瓶颈不在于进一步提升检测精度（现有模型已能很好解决），而在于**缺乏一个能明确指导模型关注哪些物体及其治理重要性的、面向管理的基准数据集**。

其次，**模型架构层面的不足**。为了弥补VLM在特定低空场景中识别精度不足的问题，实践中常部署场景优化的视觉检测器。现有方法通常采用**松耦合的显式协调范式**：将检测器预测的边界框和图像转换为结构化的、细粒度的提示词，再输入VLM进行推理（见第3节图4左）。这种范式存在三个主要问题：1）视觉检测器的定位误差；2）将边界框转换为手工结构化提示时引入的信息丢失和歧义；3）VLM对这些提示进行二次编码时产生的误差。这些中间表示容易导致信息损失和误差累积，即使微小的视觉定位或上下文抽象不准确，也可能显著影响下游基于语言的理解和决策性能。因此，**需要一个更集成、更原则性的视觉感知与语言推理协调机制**。

**3. 核心贡献与创新点**
本文的核心贡献体现在一个协同的基准数据集和一个创新的推理框架上，具体如下：

1.  **面向治理的低空多模态基准数据集 GovLA-10K**：这是本文的首要贡献。与现有强调穷举式物体级标注的通用数据集不同，GovLA-10K从城市治理视角出发，**专注于与异常、风险和违规情况直接相关的功能显著目标**（见第2.1节）。它定义了9个治理相关类别（如非法停车车辆、建筑垃圾、地面垃圾）。更重要的是，它不仅提供选择性视觉标注，还通过一个两阶段半自动标注流程（见图1），为每个场景生成**对齐的、基于语言的场景描述和管理建议**，将监督目标从全场景识别转向语义驱动的事件理解。这为低空视觉系统提供了坚实的数据基础。

2.  **隐式协调的视觉-语言推理框架 GovLA-Reasoner**：这是本文的核心技术创新。针对松耦合显式协调范式的缺陷，GovLA-Reasoner提出了一种**隐式的特征空间协调机制**（见第3节及图4右）。其核心创新在于引入一个轻量级的特征适配器（Adapter），该适配器直接接收并处理来自视觉基础模型（Grounding Model）的异构判别性特征，并将其投影为大语言模型（LLM）兼容的表示，从而**绕过了显式的提示词构建和二次编码阶段**。这种方法实现了视觉与语言之间端到端的交互，有效缓解了信息损失和误差累积。

3.  **高效且部署友好的解决方案**：GovLA-Reasoner框架在设计上追求高效性。其适配器是唯一需要训练的部分，而视觉基础模型和LLM的参数在训练过程中保持冻结（见第3.2节“Adapter Optimization”及图4标注）。这意味着该方法**在带来显著性能提升的同时，无需对任何任务特定的独立组件进行微调**，降低了计算开销和部署复杂度，非常适合资源受限的低空智能应用。

**4. 方法概述**
GovLA-Reasoner框架分为两个阶段，其核心是第II阶段中的特征适配器设计。

**阶段 I：视觉基础（Vision Grounding）**（见第3.1节）。此阶段目标是为特定低空场景优化视觉检测器，提供稳定的视觉基础。作者没有重新设计检测器架构，而是基于GovLA-10K的数据特性，选择了**基于文本提示的基础模型（如MM-GroundingDINO）**。相比纯视觉检测器，基础模型具有两大优势：1）支持以文本为条件的开放集识别，与治理场景的选择性需求相符；2）其生成的判别性特征通过视觉-语言跨模态交互得到增强，为后续与LLM的隐式特征对齐和协同推理奠定了基础。

**阶段 II：描述推理（Caption Reasoning）**（见第3.2节）。此阶段是方法的核心，旨在通过特征适配器高效利用基础模型的特征生成描述。适配器的运作流程如下（见图5）：
1.  **输入**：适配器接收三组来自基础模型的异构特征作为输入：原始图像特征 `F_img`、文本引导的跨模态查询特征 `F_query`、解码后的判别性基础特征 `F_decoder`。它们分别编码了全局场景上下文、语言条件对齐和以物体为中心的判别信息。
2.  **令牌压缩**：由于 `F_query` 和 `F_decoder` 包含大量冗余和噪声（如背景激活），首先进行令牌级压缩以蒸馏判别性语义。初始化一组可学习的令牌 `T ∈ R^(N×C)`（N << L），通过全局注意力建模（公式1，2）获得压缩后的特征 `˜F_query` 和 `˜F_decoder`。这些可学习令牌通过端到端优化自适应地聚合信息语义并抑制无关激活。
3.  **令牌集成**：将压缩后的特征与 `F_img` 沿通道维度拼接（公式3），得到一个统一的令牌序列 `F_cat`，显式聚合了全局外观、语言条件响应和判别性基础信息。
4.  **特征融合与投影**：`F_cat` 被送入堆叠的Transformer块进行深度语义融合和上下文精炼（公式4）。最后，通过一个轻量级的线性投影层（公式5）将融合后的特征 `F_fuse` 投影到LLM的嵌入空间，得到适配后的令牌 `F_adapter`，作为视觉条件令牌直接注入LLM，支持高级描述推理。

整个框架仅优化适配器参数，使用生成的描述与真实描述之间的损失进行训练，视觉基础模型和LLM均保持冻结，实现了高效训练。

**5. 实验说明**
*   **评估指标**：
    *   **检测任务**：采用标准目标检测指标，包括mAP、mAP@50、mAP@75。
    *   **描述生成任务**：采用广泛使用的自然语言生成指标，包括BLEU-1/2/3/4、ROUGE-L、METEOR和CIDEr-D。
*   **数据集**：实验主要在本文提出的 **GovLA-10K** 数据集上进行。该数据集包含10,572张低空无人机图像，标注了9个治理相关类别，并配有场景描述和管理建议。按约7:3的比例划分为训练集（7,500张图）和测试集（3,072张图）。
*   **对比基线方法**：
    *   **检测方法**（表3）：Faster R-CNN, Cascade R-CNN, YOLO-X, Deformable DETR, DINO, MMGrounding-DINO。
    *   **描述生成方法**（表2）：一系列主流开源VLM，包括LLaVA-OneVision-1.5 (4B/8B), InternVL3 (8B), InternVL3.5 (4B/8B), Qwen2.5-VL (3B/7B), Qwen3-VL (4B/8B)。
*   **实验条件**：论文中未明确说明训练、微调、推理所使用的具体GPU型号、数量及配置。仅提及实验在PyTorch框架下进行，并使用NVIDIA H20 GPUs（见第4.1节“Implementation Details”）。描述生成任务中，GovLA-Reasoner采用Qwen3-4B作为其LLM组件，批量大小设置为16，使用AdamW优化器，初始学习率为1e-4。

**6. 改进建议和未来研究方向

---

## 7. Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning

### 基本信息
- **作者**: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou
- **arXiv ID**: [oai:arXiv.org:2510.11027v2](https://arxiv.org/abs/2510.11027)
- **发布日期**: Wed, 28 Jan 2026 00:00:00 -0500
- **分类**: cs.CV
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.11027)

            ### 原文摘要
            arXiv:2510.11027v2 Announce Type: replace  Abstract: While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning》内容，生成一份符合要求的详细总结。

***

### **论文概要**

本文提出了Vlaser，一个旨在弥合高级具身推理与低级机器人控制之间鸿沟的视觉-语言-动作基础模型。研究核心在于构建一个高质量的具身多模态数据集Vlaser-6M，并基于此训练一个具备强大具身推理能力的视觉-语言模型。通过系统性地分析不同数据流对下游视觉-语言-动作模型微调的影响，论文发现：尽管互联网规模的具身推理数据能提升上游基准测试性能，但与机器人交互数据同域的数据对于加速策略学习收敛和提升任务成功率更为关键。Vlaser在广泛的具身推理基准测试中取得了最先进的性能，并在WidowX和Google Robot等仿真机器人控制任务上验证了其有效性。

### **研究动机**

当前具身人工智能的发展面临一个关键挑战：上游的视觉-语言模型具备强大的感知与推理能力，而下游的视觉-语言-动作模型专注于端到端的机器人控制，但两者之间存在显著的“领域鸿沟”。论文指出，尽管已有大量工作致力于开发具身视觉-语言模型以增强智能体的推理能力（如RoboBrain2.0、Embodied-R1），或直接将先进的视觉-语言模型扩展为视觉-语言-动作模型用于机器人控制（如OpenVLA、π0），但鲜有研究直接探讨“何种上游的视觉-语言模型能力或数据流对下游的视觉-语言-动作策略学习最为关键”（见第1节引言部分）。

具体而言，现有视觉-语言-动作模型通常直接微调预训练的视觉-语言模型。然而，选择一个能够加速收敛、提升成功率的视觉-语言模型主干网络，仍然是一个未被充分探索的问题（见第1节：“the selection of an optimal VLM backbone ... remains an under-explored research problem”）。作者认为，理解并弥合互联网规模预训练数据与具身特定策略学习数据之间的领域偏移，对于构建更高效的具身基础模型至关重要。

因此，本文的研究动机是双重的：1）构建一个具备强大且均衡的具身推理能力（如空间推理、视觉定位、任务规划）的视觉-语言模型；2）基于此模型，系统地研究不同视觉-语言模型初始化对下游视觉-语言-动作模型微调的影响，从而回答“哪些多模态数据流/能力对提升下游视觉-语言-动作模型最为关键”这一核心问题，为未来具身模型的构建提供数据层面的实用洞见。

### **核心贡献与创新点**

本文的核心贡献与创新点主要体现在以下三个方面：

1.  **开源的高质量具身视觉-语言模型与数据集**：论文构建并开源了Vlaser-6M数据集及基于此训练的Vlaser模型。Vlaser-6M数据集规模达600万，系统地涵盖了具身推理的多个关键维度：**具身视觉定位**（180万条，包含边界框和中心点预测）、**通用与空间推理**（170万条，包含机器人视觉问答和空间智能）、**任务规划**（40万条）以及关键的**同域仿真数据**（200万条，直接来自机器人交互仿真平台）。该数据集通过精心策划、重组和标注公开数据集构建而成，旨在全面增强模型的具身感知与推理能力（见第2.2节及图1）。基于此数据集监督微调InternVL3得到的Vlaser模型，在2B和8B两个参数量级上均展现出卓越的具身推理性能。

2.  **对视觉-语言模型到视觉-语言-动作模型迁移中数据有效性的系统性分析**：这是本文最具洞察力的概念性创新。论文通过严谨的消融实验，揭示了上游视觉-语言模型能力与下游视觉-语言-动作模型性能之间一个反直觉的关系：**提升标准具身推理基准测试（域外数据）的性能，并不能直接或显著地转化为下游视觉-语言-动作模型在闭环控制任务上的提升**（见第3.2节及表2、3、4）。相反，直接基于机器人交互数据集（如Open X-Embodiment）生成的**同域数据**，虽然在标准基准测试上可能不占优，却能显著加速视觉-语言-动作模型微调的收敛并提高任务成功率。这一发现挑战了“更强的上游基准性能必然带来更好的下游控制”的直觉，为未来构建真正有利于机器人具身化的视觉-语言模型指明了方向：**亟需缩小当前具身感知推理基准与真实世界机器人任务之间的领域差距**（见第1节结论部分）。

3.  **在具身推理基准测试上的最先进性能**：Vlaser在涵盖12个基准测试的综合评估中取得了领先或极具竞争力的性能。如表1所示，Vlaser-8B在同类规模模型中取得了最高的平均分（51.3），尤其在具身视觉定位（如Pixmo-Points, VSI-Bench）和仿真评估（如EB-ALFRED）等任务上优势明显。即使是较小的Vlaser-2B模型，其综合性能也超越了包括RoboBrain2.0-7B和Embodied-R1-7B在内的许多更大规模的具身专用模型。这证明了Vlaser-6M数据集的高质量以及Vlaser模型在感知、推理、规划等多维度具身智能上的均衡与强大能力。

### **方法概述**

Vlaser采用两阶段训练方案，模型结构包含视觉-语言主干网络和动作专家模块。

**1. 模型结构（见第2.1节及图2）**：
*   **视觉-语言主干网络**：基于InternVL3构建，采用InternViT作为视觉编码器，分别搭配Qwen2.5-1.5B和Qwen2.5-7B作为大语言模型。此部分负责多模态感知与高级推理。
*   **动作专家模块**：为实现低层控制，在视觉-语言主干上附加了一个基于流匹配的动作预测模块。该模块与语言模型共享自注意力参数，但使用独立的权重处理机器人状态和动作令牌，类似于混合专家架构。它采用**非因果注意力**，以同时处理观测、指令和动作序列。

**2. 训练流程（见第2.3节）**：
*   **阶段一：视觉-语言预训练**：使用Vlaser-6M数据集（包含域外和同域数据）对InternVL3进行监督微调。训练目标为标准自回归语言建模损失（公式1），旨在注入具身推理能力。
*   **阶段二：视觉-语言-动作微调**：在预训练的Vlaser模型上，冻结视觉-语言主干的大部分参数，主要训练动作专家模块。使用机器人交互数据集（如SimplerEnv中的WidowX和Google Robot数据）进行训练。
    *   **动作表示**：将未来H步的动作块 \(A_t = [a_t, a_{t+1}, ..., a_{t+H-1}]\) 表示为动作令牌。
    *   **流匹配训练**：核心思想是学习一个去噪向量场。对真实动作块 \(A_t\) 添加噪声得到 \(A_t^\tau\)，训练网络 \(v_\theta\) 预测去噪方向 \(u(A_t^\tau | A_t) = \epsilon - A_t\)。损失函数为预测向量场与真实向量场之间的L2损失（公式2）。
    *   **推理**：从随机噪声 \(A_t^0 \sim \mathcal{N}(0, I)\) 开始，通过多次（如10步）积分学习到的向量场 \(v_\theta\)，逐步去噪生成最终的动作序列（公式3）。实验中默认预测长度H=4，积分步数δ⁻¹=10。

**3. 方法流程与创新点的结合**：
该方法的核心流程是：首先利用大规模、多任务的Vlaser-6M数据增强模型的**具身推理能力**；然后，在特定的机器人 embodiment（如WidowX）上，利用同域的交互数据微调**动作生成能力**。创新点（2）中关于数据有效性的分析，正是通过对比不同数据子集（如仅用域外数据的Vlaser-OOD、使用不同同域数据类型的Vlaser-QA/Spatial/Grounding）微调得到的模型在下游任务上的性能差异而得以验证的。这体现了方法设计（两阶段训练、模块化结构）直接服务于核心研究问题的探索。

### **实验说明**

**1. 评估指标与数据集**：
*   **具身推理能力评估（12个基准测试）**：
    *   **具身问答**：ERQA。
    *   **任务规划**：Ego-Plan2。
    *   **具身视觉定位**：Where2place, Pointarena, Paco-Lavis, Pixmo-Points。
    *   **空间智能**：VSI-Bench, RefSpatial-Bench, MMSI-Bench, VLABench。
    *   **闭环仿真评估**：EmbodiedBench (ALFRED, Habitat)。
    *   **主要指标**：各基准测试特定的准确率或成功率。
*   **下游闭环机器人任务评估**：
    *   **仿真平台与

---

