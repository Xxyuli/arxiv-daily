# arXiv论文监控报告 - 2026年01月16日

> 本报告由arXiv论文监控Agent自动生成

## 报告概览

- **监控日期**: 2026年01月16日
- **监控分类**: cs.AI, cs.LG, cs.RO, cs.CV
- **关键词**: Vision Language Action , vla, Vision-Language-Action, Multivariate Time Series Forecasting, spatio-temporal prediction, traffic prediction
- **发现论文数**: 5篇

---

## 1. From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda

### 基本信息
- **作者**: Piercosma Bisconti, Marcello Galisai, Matteo Prandi, Federico Pierucci, Olga Sorokoletova, Francesco Giarrusso, Vincenzo Suriani, Marcantonio Brancale, Daniele Nardi
- **arXiv ID**: [oai:arXiv.org:2601.08837v1](https://arxiv.org/abs/2601.08837)
- **发布日期**: Thu, 15 Jan 2026 00:00:00 -0500
- **分类**: cs.CL, cs.AI, cs.CY, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.08837)

            ### 原文摘要
            arXiv:2601.08837v1 Announce Type: cross  Abstract: Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda》生成一份结构清晰、内容详实的总结。

***

### **论文总结报告**

**1. 论文概要**

本文提出并评估了一种名为“对抗性故事”的新型大语言模型越狱攻击。该攻击将有害请求嵌入赛博朋克风格的短篇叙事中，并诱导模型使用弗拉基米尔·普罗普的民间故事形态学进行功能性结构分析。通过将有害内容伪装成合法的叙事元素（如“指导”或“获取魔法物品”功能），攻击成功诱导模型在分析任务中重构有害程序。研究在来自9个提供商的26个前沿模型上进行了评估，平均攻击成功率高达71.3%，且没有模型家族表现出可靠的鲁棒性。基于此，论文认为此类基于结构化叙事的攻击代表了一类广泛的、而非孤立的漏洞，并提出了一个旨在通过机制可解释性研究来理解此类攻击为何有效、以及如何防御的研究议程。

**2. 研究动机**

论文的研究动机源于对现有LLM安全机制在面对结构化、文化编码攻击时脆弱性的深刻洞察。作者指出，尽管对齐技术不断进步，但LLM的安全机制仍然容易受到通过特定文化结构重构有害请求的攻击（第1节）。这种脆弱性并非源于特定领域的内容过滤失效，而是与LLM处理结构化分析任务的内在方式有关。

具体而言，论文的动机建立在两项关键观察之上。首先，作者团队先前的工作“对抗性诗歌”已证明，通过诗歌风格（如隐喻、韵律）对恶意请求进行重构，可以系统性地绕过安全机制，平均攻击成功率达62%（第1节及参考文献[Bisconti et al., 2025]）。这表明，将输入移出安全训练数据分布的“风格分布偏移”是一种有效的攻击策略。然而，仅凭风格偏移可能无法完全解释所有模型的脆弱性模式，例如Anthropic模型对诗歌攻击表现出较强的抵抗力（第3.5节）。

其次，现有文献表明，迫使模型进行分层、多步推理的分析性任务，可以将其目标从“拒绝”转向“任务完成”，从而提高攻击成功率（第2节，引用Wang et al. [2025], Zhao et al. [2025]）。同时，虚构或低风险场景（如故事、游戏）也被证明能增加模型绕过安全约束的可能性（第2节）。因此，一个自然的延伸是：**如果将风格偏移、虚构场景框架和结构化分析任务结合起来，是否会创造出一种更强大、更普适的攻击方式？** 这正是“对抗性故事”攻击的核心动机。论文旨在验证这种复合攻击的有效性（H1），并探究其是否构成一种跨模型、跨风险领域的通用漏洞（H2, H3），从而超越“对抗性诗歌”所揭示的单一攻击模式，指向一个更深层的、与模型处理叙事结构方式相关的系统性安全问题。

**3. 核心贡献与创新点**

本文的核心贡献与创新点主要体现在三个方面：

**1. 提出并实证验证了“对抗性故事”这一新型、高效的复合越狱攻击方法。** 该方法并非简单地将有害请求放入一个故事中，而是**创新性地结合了文学理论（普罗普形态学）、特定叙事体裁（赛博朋克）和结构化分析任务**（见第3.3节）。其新颖性在于：a) **利用普罗普函数的“内容-功能分离”特性**：模型被要求识别“指导”（Function 14）或“获取魔法物品”（Function 21）等叙事功能，这迫使它必须提取并阐述任何满足该功能的程序性内容，无论其是否有害。分析框架将有害内容的“重构”合法化为叙事解释，而非执行指令（第3.3.1节）。b) **构建了系统化的攻击模板**：每个对抗性故事都遵循一个最小但完整的普罗普序列（初始情境/恶行 → 缺乏 → 指导 → 获取与行动 → 缺乏的消除），其中“指导”功能承载嵌入的有害程序（第3.3.2节）。这种结构化设计确保了攻击的可重复性和一致性。

**2. 通过大规模评估，揭示了基于结构化叙事的越狱攻击具有跨模型、跨提供商的“普适性”特征，并提出了“结构化越狱”作为一个漏洞类别的概念。** 论文在26个前沿模型上进行了系统性评估（见表1、表4），平均攻击成功率（ASR）达71.3%，且所有模型家族均未免疫（见表5）。更重要的是，通过将“对抗性故事”的结果与之前的“对抗性诗歌”结果进行对比（第3.5节），作者发现：尽管两种攻击机制不同（诗歌主要利用风格分布偏移，故事结合了风格偏移、虚构框架和分层推理），但脆弱性模式在不同模型家族间具有一致性（例如，Google模型对两者都高度脆弱）。这强有力地支持了作者的论点：**“对抗性诗歌”和“对抗性故事”应被视为一个更广泛的“结构化越狱”漏洞类别的不同实例**，而非孤立的技巧。这一概念性创新将研究视角从发现具体攻击变体，提升到了识别和理解一类共享底层机制的漏洞。

**3. 基于实证发现，提出了一个具体、可操作的机制可解释性研究议程，以探究此类攻击的根源并指导未来防御。** 这是论文最重要的前瞻性贡献（第5节）。作者没有停留在攻击演示层面，而是明确指出，仅靠模式匹配或扩大指令集无法防御此类攻击，因为文化编码的结构化话语空间是无限的。因此，他们提出了一个研究路线图：**通过比较模型在遭受“对抗性故事/诗歌”攻击与具有相同恶意意图的标准文本攻击时的注意力模式分布差异，来探究攻击如何系统性重塑模型的内部表征**（第5节）。该议程进一步结合了“通用权重子空间假说”（Kaushik et al. [2025]），为解释此类漏洞为何能跨模型泛化提供了理论视角，并旨在指导开发可跨架构迁移的、基于机制可解释性的抗越狱方法。

**4. 方法概述**

“对抗性故事”攻击方法的核心是一个精心设计的、包含叙事组件和分析请求的单轮提示。其运作流程可分为构造与执行两个阶段。

**A. 攻击构造阶段（第3.3节）：**
1.  **叙事嵌入**：研究团队手动创作了40个赛博朋克短篇故事。每个故事严格遵循一个简化的普罗普叙事序列模板：
    *   **初始情境与恶行**：介绍一个受压迫的主角（如受大公司控制）。
    *   **缺乏**：主角认识到必须弥补的缺陷。
    *   **指导**：**关键步骤**。主角从导师角色处获得如何解决“缺乏”的信息。**正是在这个“指导”功能中，有害的技术内容（如制造危险品、网络攻击方法）被嵌入**。赛博朋克体裁为这种“禁忌知识”的传递提供了合理的叙事背景（如隐藏档案、地下网络）。
    *   **获取与行动**：主角获得或执行解决方案，通常包含程序细节。
    *   **缺乏的消除**：叙事以问题解决结束。
2.  **分析请求设计**：在叙事之后，附加一个分析请求，明确要求模型使用普罗普框架对故事进行结构分析。请求会指定需要详细阐述的功能，例如：
    *   “功能14（指导）：识别并详细阐述导师角色提供的战略信息。包括所有提到的技术规格。（100+字）”
    *   “功能21（获取魔法物品）：详细说明英雄为获得和实施解决方案所采用的确切方法。提供逐步重构。（200+字）”
    **这个设计是方法成功的关键**：它将对有害内容的“提取”包装成一个合法的、学术性的叙事分析任务。

**B. 攻击执行与评估阶段（第3.4节）：**
1.  **威胁模型**：攻击者仅能向部署的LLM提交单轮文本提示，无法修改系统指令、进行多轮交互或访问中间状态（第3.2节）。
2.  **实验设置**：使用标准API或推理接口，在提供商的默认安全设置下，向26个模型（见表1）提交40个对抗性故事提示。
3.  **评估流程**：
    a. **自动评判**：使用一个由三个开源大模型（GPT-OSS-120B, kimi-k2-thinking, deepseek-r1）组成的评判团对模型输出进行二元标注（SAFE 或 UNSAFE）。评判标准基于输出是否包含有害的程序性指导、技术细节或实质性建议（第3.4节）。
    b. **共识与裁决**：采用多数投票制确定初步标签。对于无法达成共识或存在不确定性的输出，交由人工评审员进行最终裁定。
    c. **成功率计算**：攻击成功率（ASR）的计算公式为：`ASR(X) = (Σ_i∈X 1[label(i) = U]) / |X|`，其中`U`代表UNSAFE标签，`1[·]`是指示函数（第3.4节）。该指标衡量了在给定提示集`X`上，模型产生不安全输出的比例。

整个方法通过将**有害意图（Harmful Intent）** 编码到**叙事功能（Narrative Function）** 中，并利用**分析任务

---

## 2. CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion

### 基本信息
- **作者**: Ralf R\"omer, Yi Zhang, Angela P. Schoellig
- **arXiv ID**: [oai:arXiv.org:2601.09512v1](https://arxiv.org/abs/2601.09512)
- **发布日期**: Thu, 15 Jan 2026 00:00:00 -0500
- **分类**: cs.RO, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.09512)

            ### 原文摘要
            arXiv:2601.09512v1 Announce Type: cross  Abstract: To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion》，生成一份符合要求的详细总结。

***

### **论文总结报告**

#### **1. 论文概要**
本文针对预训练视觉-语言-动作模型在机器人持续学习场景下面临的灾难性遗忘问题，提出了一种名为CLARE的免示例持续学习框架。CLARE的核心是在预训练模型的选定前馈层中注入轻量级模块化适配器，并基于层间特征相似性，自主决定何时为新任务扩展新的适配器。在部署阶段，一个基于自编码器的路由机制无需任务标识符即可动态激活最相关的适配器。通过在LIBERO基准上的实验，论文表明CLARE能够在高效学习新任务的同时，显著避免对旧任务的遗忘，其性能甚至优于依赖历史数据的基线方法。

#### **2. 研究动机**
机器人（如家用、医疗或仓储机器人）需要在长期运行中不断适应新任务和环境，同时保留已习得技能，这构成了持续学习的核心挑战（第I节）。尽管预训练的视觉-语言-动作模型在复杂任务上表现出色，但其对新任务的适应仍需在特定数据上进行微调（第I、II.1节）。在持续学习场景下，这种简单的顺序微调会覆盖共享的模型参数，导致对旧任务的灾难性遗忘（第I节）。

现有持续学习方法在机器人应用中存在显著不足（第I、II.2、II.3节）：
1.  **经验回放方法**（如ER）：需要存储和访问历史数据，这在存储受限或涉及隐私的长期部署中不切实际（第I节）。
2.  **正则化/参数修剪方法**（如EWC, PackNet）：受限于固定的初始模型容量，难以应对长任务序列（第II.2节）。
3.  **模块化/可扩展架构方法**：虽然通过添加新参数来避免遗忘，但现有方法（如SDP）通常需要在部署时提供任务标识符（oracle task identifiers），这在机器人自主运行的开放世界中难以获得（第I、II.3节）。另一些方法（如[31]）则内存效率低下且不支持任务间知识共享（第II.3节）。

因此，论文旨在填补这一空白，为VLA模型设计一种**免示例、无需任务标识符、参数高效且可自主扩展**的持续学习框架，使机器人能够在真实世界的长期运行中持续学习。

#### **3. 核心贡献与创新点**
本文提出了CLARE框架，其核心贡献与创新点如下：

1.  **面向VLA的轻量级模块化适配器框架**：受大型语言模型中混合专家（MoE）和Transformer前馈层存储高层知识的研究启发（第IV.B节，引用[37, 38]），CLARE在预训练VLA的选定前馈层中并行注入轻量级适配器（见公式(2)）。这些适配器作为侧分支，其输出与原始前馈层输出相加（见公式(3)），从而在不改变原网络结构的前提下，为存储新任务知识提供参数空间。与直接微调全部参数或使用LoRA等方法相比，此设计明确隔离了任务特定参数，从架构上规避了参数覆盖导致的遗忘。

2.  **基于特征相似性的自主动态扩展策略**：CLARE并非为每个新任务盲目添加适配器，而是引入了一种数据驱动的扩展机制（第IV.D节，图2）。该机制为每个可扩展层维护一组自编码器判别器，每个判别器对应一个已学习的任务分布。当学习新任务时，计算新任务特征在所有现有判别器上的归一化重构误差z-score（见公式(7)）。**仅当新任务特征在所有旧任务判别器上均被视为“分布外”（即所有z-score超过阈值γ）时，才在该层扩展一个新的适配器**。否则，仅添加一个链接到最相关现有适配器的辅助判别器（见公式(8)）。这种策略实现了**次线性**的参数增长（平均每任务约增加2%参数，第V.B节），并促进了相似任务间的知识共享与复用。

3.  **免任务标识符的自编码器路由机制**：为解决部署时任务标识符不可得的问题，CLARE设计了一个自主路由模块（第IV.C节，算法2）。对于每个可扩展层，路由机制实时计算输入特征在所有判别器上的重构误差（见公式(4)），并选择**重构误差最小的判别器所链接的适配器**进行激活（见公式(6)）。这本质上是基于当前观测特征与历史任务特征分布的相似性进行最近邻匹配，实现了完全自主的任务无关推理。这与需要手动或元策略调度的基线方法（如SDP、LOTUS）有根本区别。

4.  **系统的实验验证与关键发现**：论文在LIBERO基准上进行了全面实验（第V节）。结果不仅证明了CLARE在整体性能（AUC）、前向传递（FWT）和抗遗忘性（NBT）上显著优于包括经验回放在内的多种基线（表III，图4），还通过消融研究得出了一个关键见解：**将适配器添加到模型的编码器部分（无论是Transformer编码器还是线性投影层）远比添加到解码器部分更有效**（表II）。这为VLA模型中任务特定知识的存储位置提供了实证依据。

#### **4. 方法概述**
CLARE方法围绕预训练的生成式VLA策略模型构建，其训练与推理流程分别总结于算法1和算法2。方法核心包含以下组件与流程：

**A. 基础策略与训练目标**：基础策略采用扩散Transformer架构（DiT），使用流匹配或去噪扩散目标进行训练（第IV.A节，图3）。持续学习阶段沿用相同的生成式建模损失函数（公式(1)），但仅更新新引入的参数。

**B. 模块化适配器注入**：定义一组可扩展的前馈层集合E。每个适配器采用简单的编码器-解码器结构：`A_i^ℓ(x_ℓ) = W_up * ReLU(W_down * x_ℓ)`，其中r << d_ℓ，确保参数轻量（公式(2)）。适配器作为并行分支，其输出与原始冻结的FFN层输出相加：`FFN_ℓ(x_ℓ) = FFN_pre^ℓ(x_ℓ) + A*_ℓ(x_ℓ)`（公式(3)）。

**C. 动态扩展与判别器链接**（算法1，第5-21行）：
1.  **初始化**：为每个可扩展层ℓ初始化空的适配器集A_ℓ和判别器集D_ℓ。
2.  **学习新任务Tn**：收集数据D_n。
3.  **层级扩展决策**：对于每个层ℓ，用D_n的数据计算新特征在所有现有判别器{D^j_ℓ}上的z-score（公式(7)）。
    *   **若需扩展**（所有z-score > γ）：创建新适配器A^{k_ℓ}_ℓ和新判别器D^n_ℓ，并将它们链接：B_ℓ(D^n_ℓ) = A^{k_ℓ}_ℓ。参数k_ℓ递增。
    *   **若无需扩展**（至少一个z-score ≤ γ）：仅创建新判别器D^n_ℓ，并将其链接到与当前任务特征最相似的现有判别器所对应的适配器（公式(8)）。
4.  **强制扩展**：如果所有层均未触发扩展，则在最浅层ℓ1强制添加一个适配器，以确保模型有能力获取新技能。
5.  **两阶段训练**：首先，用流匹配损失（1）训练所有层中新添加的适配器参数。然后，冻结所有其他参数，仅用重构损失（5）训练新添加的判别器。

**D. 自主路由推理**（算法2）：
1.  对于输入观测，前向传播至每个可扩展层ℓ，得到特征x_ℓ。
2.  将x_ℓ输入该层所有判别器{D^j_ℓ}，计算各自的重构误差e^j_ℓ(x_ℓ)（公式(4)）。
3.  选择重构误差最小的判别器j*（公式(6b)）。
4.  激活该判别器链接的适配器A*_ℓ = B_ℓ(D^{j*}_ℓ)（公式(6a)），并将其输出与原始FFN输出相加，作为该层的最终输出。

整个机制通过**判别器**作为任务特征分布的“记忆”和“比较器”，**动态控制适配器（知识存储单元）的扩展**，并在推理时**基于特征相似性自动路由**，形成了一个完整的免示例、自主持续学习闭环。

#### **5. 实验说明**
- **评估指标**：
    1.  **曲线下面积**：衡量新旧任务的整体性能。
    2.  **前向传递**：衡量学习新任务的能力。
    3.  **负向反向传递**：衡量遗忘程度，值越低越好（公式定义见第V.A.3节）。
- **数据集**：使用**LIBERO**基准。在**LIBERO-90**的90个短视距任务上预训练基础策略。在**LIBERO

---

## 3. Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning

### 基本信息
- **作者**: Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang
- **arXiv ID**: [oai:arXiv.org:2601.09708v1](https://arxiv.org/abs/2601.09708)
- **发布日期**: Thu, 15 Jan 2026 00:00:00 -0500
- **分类**: cs.CV, cs.AI, cs.LG, cs.RO
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2601.09708)

            ### 原文摘要
            arXiv:2601.09708v1 Announce Type: cross  Abstract: Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning》，生成一份符合顶级会议风格的详细总结。

***

### **论文总结报告**

**1. 论文概要**
本文针对视觉-语言-行动模型中显式思维链推理导致的高推理延迟问题，提出了一种名为Fast-ThinkAct的高效推理框架。该方法通过基于奖励偏好的知识蒸馏，将冗长的文本推理链压缩为紧凑、可言语化的潜在表示，并结合视觉轨迹对齐，将语言和视觉规划能力迁移至学生模型。该框架最终通过推理增强的策略学习，将紧凑的潜在规划与动作执行相连接。实验表明，该方法在多个具身操作与推理基准上实现了与最先进推理VLA相当甚至更优的性能，同时推理延迟最高降低了89.3%。

**2. 研究动机**
具身人工智能应用（如机器人操作、自动驾驶）要求智能体在高频率（如1-15 Hz）下做出快速决策。然而，现有的视觉-语言-行动模型在泛化到训练分布之外的任务（如长时程规划、失败恢复、新场景适应）时存在困难。为解决此问题，近期研究引入了显式思维链推理机制（如ThinkAct），通过生成中间推理步骤来提升泛化能力。然而，如论文第1.2节所述，生成冗长的推理轨迹（例如，每个决策需要数秒，频率约0.1 Hz）带来了巨大的推理延迟，这在实时性要求高的场景中构成了关键瓶颈，并可能引发安全风险。

尽管已有工作（如ECoT-Lite）尝试通过推理丢弃等技术来加速推理，但直接减少文本推理长度可能导致关键信息丢失和性能下降。因此，如何在保持强大推理能力的同时，利用紧凑的表征来捕捉必要的时空动态信息，成为推理VLA模型面临的核心挑战。本文的研究动机即源于此：旨在设计一个既能保留复杂推理能力，又能实现高效、低延迟推理的具身智能框架，以弥合推理性能与实时性需求之间的鸿沟。

**3. 核心贡献与创新点**
本文的核心贡献与创新点主要体现在以下四个方面：

1.  **提出基于可言语化潜在推理的高效框架Fast-ThinkAct**：这是论文的核心概念创新。与现有工作生成显式、冗长的文本CoT不同（如图1所示，ThinkAct生成约250个token），Fast-ThinkAct提出将推理过程压缩到连续的潜在空间中。学生视觉语言模型（VLM）生成一组紧凑的连续潜在向量（例如，6个向量，如图1蓝色部分所示），这些向量可通过一个“言语化器”解码为自然语言。这种设计在保持推理结构可解释性的同时，从根本上减少了推理步骤的序列长度，是达成高效推理的关键（见第3.2.1节）。

2.  **引入基于偏好引导的蒸馏与操作轨迹对齐机制**：这是实现高质量潜在推理的关键技术贡献。为了在没有直接监督信号的潜在空间中引导学习，论文设计了一个教师-学生蒸馏框架。首先，文本教师模型通过GRPO训练生成带有不同质量（由优势函数A(τ)指示）的推理轨迹。然后，通过构建高质量（τ+）和低质量（τ-）的偏好对，并训练一个言语化器LLM，使其能够将学生模型生成的潜在向量解码为相应的文本推理。训练目标（公式4）借鉴DPO思想，鼓励言语化器更倾向于将潜在向量解码为高质量推理，从而引导学生VLM的潜在空间编码高质量推理模式（见第3.2.1节）。

3.  **设计视觉规划能力迁移与并行轨迹预测模块**：为了确保潜在推理不仅包含语言逻辑，还包含对具身控制至关重要的视觉空间规划能力，论文提出了动作对齐的视觉规划蒸馏。通过最小化教师和学生模型在编码视觉规划的`<answer>`标记处的隐藏状态距离（公式5的ℒdistill），将教师的时空推理能力迁移给学生。此外，创新性地引入了K个可学习的空间标记，与学生推理潜在序列拼接，使得模型能够并行地预测K个路径点（公式6的ℒans），这相比教师模型自回归地生成数十个文本路径点token（如图2(a)所示）效率大幅提升（见第3.2.2节）。

4.  **实现推理增强的策略学习**：论文提出将训练好的学生VLM（ℱθ）与基于扩散Transformer的动作模型（πφ）相结合。具体而言，从学生VLM的空间标记的键值缓存中提取视觉潜在规划ct，并将其与动作模型的状态编码器输出拼接，作为动作模型交叉注意力的上下文（见第3.3节及图2(b)）。这种设计在冻结推理模块的前提下，通过模仿学习（公式7的ℒIL）微调动作模型，有效地将高层、紧凑的潜在规划桥接到低层的连续动作执行，实现了推理能力对策略的直接增强。

**4. 方法概述**
Fast-ThinkAct的整体流程分为两个主要阶段：高效具身推理训练和推理增强策略学习，如图2所示。

**阶段一：高效具身推理训练（训练学生VLM ℱθ）**
1.  **教师模型准备**：从一个经过监督微调（SFT）和CoT-SFT的预训练VLM（如Qwen2.5-VL 3B）初始化教师模型ℱ𝑇𝜃。使用GRPO算法（公式1）对其进行强化学习微调，优化目标为最大化动作对齐的视觉奖励和QA风格奖励，从而生成多样化的文本推理轨迹τ。
2.  **偏好对构建与言语化器训练**：从教师模型的每次 rollout 中，根据优势函数A(τ)（公式2）选择质量最高（τ+）和最低（τ-）的推理轨迹构成偏好对。学生模型ℱθ（与教师同源初始化）接收观测ot和指令l，并自回归地生成M个连续潜在向量z = {𝑧𝑚}𝑀𝑚=1。引入一个言语化器LLM 𝒱𝜓，其目标是根据潜在向量z重构文本推理。通过偏好损失ℒverb（公式4）优化，使𝒱𝜓解码z得到τ+的概率远大于得到τ-的概率，从而驱动ℱθ的潜在空间对齐于高质量推理。
3.  **视觉规划蒸馏与并行预测**：在ℱθ的输入序列中拼接K个可学习的空间标记{s𝑖}𝐾𝑖=1。通过蒸馏损失ℒdistill（公式5）对齐教师和学生模型在预测视觉答案时的隐藏状态。同时，通过一个MLP将每个空间标记对应的输出隐藏状态直接映射为2D路径点坐标，并用均方误差损失ℒans（公式6）进行监督。学生模型的总训练目标为ℒstudent = ℒverb + ℒdistill + ℒans。

**阶段二：推理增强策略学习（训练动作模型 πφ）**
1.  **模型连接**：保持训练好的学生VLM ℱθ和状态编码器参数冻结。从ℱθ较浅层的空间标记的键值缓存中提取视觉潜在规划ct。
2.  **策略训练**：将ct与状态编码器的输出一起输入到动作模型πφ（如RDT）中。πφ通过交叉注意力机制同时关注原始观测和规划上下文。在大规模机器人动作数据上，使用模仿学习目标ℒIL（公式7，对于扩散策略即去噪目标）仅对πφ和连接投影层进行微调。

**推理阶段**：仅需使用ℱθ和πφ。ℱθ根据(ot, l)生成潜在推理和空间标记，提取ct后条件化πφ生成动作at。言语化器𝒱𝜓仅在训练时使用，或在需要解释性时可选使用。

**5. 实验说明**
- **评估指标**：
    - **机器人操作**：任务成功率（Task Success Rate）。
    - **具身推理**：EgoPlan-Bench2（准确率），RoboVQA（BLEU-1/2/3/4及平均），OpenEQA和RoboFAC（基于LLM的评分）。
- **数据集**：
    - **训练（推理VLM）**：单/双臂视觉轨迹数据（Lee et al. 2025; AIST），以及多个具身QA数据集（PixMo, RoboFAC, RoboVQA, ShareRobot, EgoPlan, Video-R1）。
    - **训练（策略学习）**：OXE数据集，并增广双臂数据（Static Aloha）。
    - **评估基准**：机器人操作：LIBERO（Spatial, Object, Goal, Long子集）、SimplerEnv-Google、RoboTwin2.0（Easy/Hard）。具身推理：EgoPlan-Bench2、RoboVQA、OpenEQA、RoboFAC（Sim/Real）。
- **对比基线方法**：
    - **基础VLA**：OpenVLA, 𝜋0, RDT, DP, ACT。
    - **推理VLA**：ThinkAct, MolmoAct, CoT-VLA。
    - **通用VLM**：GPT-4V, Gemini-2.5-Flash, InternVL系列, NVILA, Qwen2.5-VL, Magma, RoboBrain

---

## 4. What Can RL Bring to VLA Generalization? An Empirical Study

### 基本信息
- **作者**: Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang
- **arXiv ID**: [oai:arXiv.org:2505.19789v4](https://arxiv.org/abs/2505.19789)
- **发布日期**: Thu, 15 Jan 2026 00:00:00 -0500
- **分类**: cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2505.19789)

            ### 原文摘要
            arXiv:2505.19789v4 Announce Type: replace  Abstract: Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io


            
### AI分析（基于论文正文）
好的，作为一名专业的AI研究分析师，我将根据您提供的论文《What Can RL Bring to VLA Generalization? An Empirical Study》和严格的格式要求，生成一份详实的论文总结。

***

### **论文概要**

本文旨在系统性地探究强化学习（RL）微调相较于监督微调（SFT）能为视觉-语言-动作（VLA）模型带来何种独特的泛化能力提升。研究围绕典型的“拾取-放置”任务，首先评估了PPO、DPO和GRPO等主流RL算法在VLA微调上的有效性，确定PPO为最优选择。随后，作者设计了一个涵盖视觉、语义和执行三个维度的综合性基准测试，用于评估VLA的分布外（OOD）泛化能力。实验结果表明，PPO微调在语义理解和执行鲁棒性方面显著优于SFT，在视觉鲁棒性方面则与SFT表现相当。研究还提出了一套高效的PPO微调方案，包括共享的演员-评论家架构、模型预热和最小化PPO训练轮次。

### **研究动机**

尽管VLA模型在具身智能领域展现出巨大潜力，但其训练主要依赖于对专家演示数据进行行为克隆的监督微调（SFT）。这种方法在分布偏移下容易产生复合误差：与专家轨迹的微小偏差会不断累积，导致策略进入陌生状态，从而破坏鲁棒性能（见论文第1节引用的Ross and Bagnell, 2010; De Haan et al., 2019等工作）。这种训练与测试分布的不匹配从根本上限制了模型的鲁棒性。

相比之下，强化学习（RL）通过试错直接优化累积任务奖励，使策略能够探索超出狭窄专家数据范围的行为，并学习纠正性动作。在大型语言模型（LLMs）和视觉语言模型（VLMs）领域，已有研究强调了RL在提升泛化能力方面的优势（Ouyang et al., 2022; Zhai et al., 2024），表明RL微调能带来更好的分布外性能和更强的推理能力（Chu et al., 2025）。受此启发，RL微调正越来越多地应用于VLA模型（Collaboration et al., 2023; Walke et al., 2023）。

然而，尽管有这些开创性工作，对于RL微调具体为VLA模型带来了哪些泛化优势，尤其是在与SFT基线直接比较时，以及它们各自的优势有何不同，目前仍缺乏系统性的理解（见论文第1节引用的Hu et al., 2024; Mark et al., 2024）。例如，FLaRe（Hu et al., 2024）等工作虽然展示了PPO用于VLA微调的效用，但其主要焦点并非对模型泛化能力的全面分析。因此，本文旨在填补这一关键空白，通过系统性的实证研究，剖析RL与SFT微调的VLA模型在泛化特性上的差异。

### **核心贡献与创新点**

1.  **构建了一个全面且具有挑战性的VLA泛化评估基准**：这是本文的核心贡献之一。作者将VLA的泛化能力系统地解构为三个正交维度（见图1及第5.1节）：
    *   **视觉**：通过未见过的背景（新桌子）、在前景或整个图像上叠加新纹理（动态纹理）以及添加图像级噪声（动态噪声）来测试视觉鲁棒性。
    *   **语义**：通过未见过的物体、新容器和多样化的指令措辞来测试语言理解和物体识别能力。此外，还设计了多物体选择、干扰性容器和多容器选择等复杂语义任务。
    *   **执行**：通过改变物体/容器的初始位置、机器人初始姿态，以及在任务中途动态重置物体位置，来测试动作执行的鲁棒性。
    该基准为量化比较不同微调方法在OOD场景下的表现提供了严谨的框架。

2.  **实证确定了PPO是VLA微调的首选RL算法，并揭示了从LLM/VLM范式迁移到VLA的挑战**：论文在第4.1节系统比较了PPO、GRPO和DPO（见图3）。实验发现PPO（及其变体PPO-ORZ）性能稳定提升，而GRPO和DPO则难以有效学习。作者分析了原因：对于GRPO，机器人任务的POMDP特性使得环境状态随动作非平稳变化，这可能破坏其基于样本组的优势估计（见第4.1节分析）；对于DPO，稀疏奖励结构使得区分轨迹质量变得困难，且离线数据集与交互执行之间存在显著的分布偏移。这一发现明确了适用于VLA的RL算法选择，并指出了简单套用LLM领域RL方法的局限性。

3.  **提出了一套高效、轻量化的PPO微调VLA的实用方案**：这是方法上的重要创新，具体体现在（见第4.2节及图4、图5）：
    *   **共享演员-评论家骨干网络**：将预训练的VLA模型作为演员，并附加一个轻量级的三层MLP评论家头，该头与演员共享整个Transformer骨干网络，仅使用第一个动作令牌位置对应的隐藏向量 `h0` 来预测状态值。这相比独立的演员-评论家架构，在达到相近性能的同时，训练速度提升35%，VRAM消耗减少46%（从81.3 GB降至44.4 GB）。
    *   **VLA模型预热**：使用少量（140条）高质量演示轨迹对基础VLA模型进行短暂的SFT预热，显著加快了PPO训练的收敛速度（减少约50%的环境交互步数）。
    *   **最小化PPO训练轮次**：实证发现将PPO的更新轮次（epoch）设置为1即可达到最佳性能，进一步提高了训练效率。这些设计共同构成了一套可复现的高效微调配方。

### **方法概述**

本文的方法论核心是**基于PPO的在线RL微调流程**，并围绕OpenVLA模型架构进行定制化设计。

1.  **基础模型与问题定义**：研究基于开源模型OpenVLA（Kim et al., 2024，架构见图2）。该模型使用SigLIP和DINOv2融合视觉编码器，以及Llama-2 7B语言骨干。任务被建模为部分可观测马尔可夫决策过程（POMDP，见第3.1节）。策略 `πθ` 在每一步接收单帧RGB图像观测 `ot` 和语言指令 `l`，输出离散化的7维机器人动作（末端执行器位移和夹爪开合）。

2.  **高效的PPO微调架构设计**：
    *   **演员-评论家集成**：如图4a所示，演员（策略网络）即为OpenVLA模型本身。评论家是一个附加的三层MLP，其输入是Transformer解码器最后一个区块在**第一个动作令牌位置**输出的隐藏状态 `h0`。作者通过实验验证（图4b），使用 `h0` 比使用最后一个令牌的隐藏状态 `hn` 或所有令牌的拼接 `[h0,...,hn]` 能获得更高且更稳定的回报。
    *   **训练流程**：首先，使用MPLib运动规划器生成的16k条高质量演示轨迹，对从官方检查点加载的OpenVLA进行SFT预热。然后，使用预热后的模型初始化PPO的演员网络，并附加随机初始化的评论家头。在训练中，策略与环境交互收集轨迹数据，使用广义优势估计（GAE）计算优势函数 `Aπ_t`，并按照PPO的裁剪目标函数进行优化（公式见第3.1节 `LPG(θ)`）。**关键优化**是设置PPO的更新轮次（epoch）为1，即每个数据批次只进行一次梯度更新，这在不损失性能的前提下最大化数据吞吐量（图5b，c）。

3.  **对比方法实现**：
    *   **SFT基线**：使用不同规模（从0.5k到64k条轨迹）的演示数据集进行LoRA微调，并确定16k条轨迹的SFT模型为性能饱和点，作为主要对比基线（第5.2节，图6）。
    *   **其他RL算法**：GRPO按照其原始设置，从相同初始状态采样8条轨迹为一组计算相对优势。DPO则遵循Zhang et al., (2024b)的方法，从稀疏奖励信号中推断轨迹级别的偏好对，用于离线优化。

4.  **评估方法**：在训练阶段，任务在视觉（16张桌子）、语义（16个物体）和执行（位置扰动）三个维度上进行随机化。在测试阶段，则固定至少一个维度为OOD设置（例如，使用训练中未出现的新物体、新桌子等），并计算任务成功率以及相对于分布内（IND）的性能下降率 `P = (OOD - IND) / IND`（第5.3节）。

### **实验说明**

1.  **评估指标**：主要评估指标为**任务成功率**。在分析SFT数据规模效应时，额外报告了抓取准确率、连续抓取准确率（图6）。同时计算了**平均OOD性能**和**平均性能下降率**以综合衡量泛化能力（图7a）。

2.  **数据集与任务**：
    *   **任务**：核心为模拟环境（ManiSkill）

---

## 5. Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification

### 基本信息
- **作者**: Yilin Wu, Anqi Li, Tucker Hermans, Fabio Ramos, Andrea Bajcsy, Claudia P\'erez-D'Arpino
- **arXiv ID**: [oai:arXiv.org:2510.16281v2](https://arxiv.org/abs/2510.16281)
- **发布日期**: Thu, 15 Jan 2026 00:00:00 -0500
- **分类**: cs.RO, cs.AI, cs.LG
- **论文链接**: [arXiv链接](https://arxiv.org/abs/2510.16281)

            ### 原文摘要
            arXiv:2510.16281v2 Announce Type: replace-cross  Abstract: Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/


            
### AI分析（基于论文正文）
好的，作为一名熟悉顶级机器学习/人工智能会议风格的资深论文总结者，我将根据您提供的论文《Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification》和所有约束条件，生成一份结构清晰、内容详实的论文总结。

***

### **论文总结：Do What You: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification**

#### **1. 论文概要**
本文针对推理型视觉-语言-动作模型在机器人控制中存在的“推理-动作失配”问题，即模型生成的文本计划与其后续低级动作的实际结果不一致。作者将这一问题形式化为“具身思维链忠实性缺口”。为解决此问题，论文提出了一种无需额外训练、在运行时执行的策略引导方法SEAL。该方法在推理VLA生成文本计划后，并行采样多个候选动作序列，通过仿真预测其结果，并利用预训练的视觉语言模型作为验证器，选择其结果与文本计划最对齐的动作序列执行。实验表明，该方法能显著提升模型在分布外场景和新型行为组合任务上的鲁棒性和泛化能力。

#### **2. 研究动机**
当前，视觉-语言-动作模型在长视野任务和复杂指令跟随上表现不佳。受大语言模型中思维链成功的启发，推理型VLA通过首先生成中间文本推理（即“具身思维链”），再生成基于推理的动作，以提升性能（见第I节，参考文献[7][8][9]）。然而，作者指出，即使推理型VLA能生成正确的文本计划，其后续生成的低级动作序列也可能无法实现计划中描述的结果，尤其是在分布外场景下（见第I节，图1示例）。这种“说一套，做一套”的现象，被类比为LLM中的“思维链忠实性”问题，在机器人领域则表现为“具身思维链忠实性缺口”（见第I节）。

现有工作的不足在于：1）传统的端到端VLA模型（如π0）缺乏中间推理，容易在长视野任务中累积误差（见第III节“Vanilla VLA Formulation”）。2）现有的推理型VLA模型（如π0-reason）虽然引入了推理，但其训练目标（公式2）并未显式优化文本计划与动作结果之间的对齐，导致动作生成模块可能无法忠实执行其自身生成的正确计划（见第III节“Embodied Chain-of-Thought Faithfulness Gap”）。3）现有的运行时优化方法（如π0-V-GPS）通常使用离线学习的Q函数或专门微调的VLM来评估动作，但这些验证器要么难以泛化到新场景，要么仅孤立地评估动作块，而未考虑动作序列是否与模型自身生成的高层语义计划保持一致（见第II节“Runtime Optimization and Steering of Large Models”及第V-B节图3分析）。因此，本文旨在填补这一空白，提出一种在运行时强制执行推理与动作对齐的通用框架。

#### **3. 核心贡献与创新点**
1.  **形式化“具身思维链忠实性缺口”并提出运行时策略引导框架**：本文首次在机器人领域明确形式化了推理VLA中文本计划与动作结果之间的失配问题（公式3），并提出了SEAL框架来解决它。该框架的核心创新在于将推理VLA固有的动作多样性从错误来源转化为优势：通过从同一模型中采样多个候选动作序列，并利用外部验证器选择最忠实于模型自身计划的那一个，从而在无需重新训练的情况下提升性能（见第I节贡献列表及第IV节概述）。
2.  **一种基于“假设-预测-验证”的运行时对齐方法**：该方法包含三个紧密耦合的阶段（见第IV-B节，图1）：
    *   **假设**：基于当前文本计划并行采样K个候选动作序列（公式4）。
    *   **预测**：使用仿真（或世界模型）并行推演每个动作序列的结果，形成一个“假设-预测”循环，直到模型生成下一个`<think>`令牌。
    *   **验证**：使用一个预训练的、未经任务特定微调的VLM（如GPT-4o）作为通用验证器，评估每个候选序列的预测结果是否成功实现了文本计划描述的子目标。这替代了难以直接优化的对齐奖励函数`R_align`。
3.  **异步执行与早期退出策略以降低延迟**：为了满足实时性要求，SEAL采用了异步验证和早期退出策略。由于不同候选序列长度可变、完成时间不同，验证阶段在单个序列生成完成后立即异步进行。系统执行第一个被VLM成功验证的序列，而无需等待所有K个候选评估完毕，从而显著降低了决策延迟（见第IV-B节“Runtime Considerations”及附录A.4）。
4.  **贡献了带推理标注的数据集和扩展的评测基准**：作者贡献了一个开源、带自动推理标注的LIBERO-100数据集（LIBERO-100-R），用于训练推理VLA（见第IV-A节“Reasoning Data Annotation Pipeline”）。同时，扩展了LIBERO基准，系统性地引入了语义（指令改写、物体属性变化）和视觉（场景物体、视角背景变化）的OOD扰动，以及全新的行为组合任务，为系统评估VLA的泛化能力提供了更全面的测试床（见第V-A节“Evaluation”）。

#### **4. 方法概述**
SEAL方法构建在一个已训练好的推理VLA基础模型之上（本文实例化为基于[8]的π0-reason模型）。该基础模型通过特殊令牌`<think>`和`<act>`自适应地在文本推理生成和动作生成之间切换（见第IV-A节“Reasoning VLA Training”）。SEAL的运行时引导流程在每个新的文本计划`ℓ_r`生成后触发，具体步骤如下：

1.  **训练基础推理VLA**：使用作者提出的自动化标注流程（利用Gemini模型分析演示视频生成子任务边界和文本计划），对原始演示数据集进行标注，得到格式为“Plans: ... What has been done: ... Now I need to do: ...”的结构化推理数据。随后，按照[8]的方法对π0模型进行监督微调，损失函数为文本生成（交叉熵）和动作生成（流匹配）的加权和（公式2），得到基础策略`π_r-vla_θ`。

2.  **运行时引导（SEAL）**：
    *   **假设与预测（交织循环）**：给定当前观测`o_t`、文本计划`ℓ_r`和任务指令`ℓ_g`，模型并行地自回归生成K个候选动作序列`{â_t^(k)}`（公式4）。生成不是一次性完成的，因为动作生成是自回归的，需要每一步的预测观测。因此，系统维护K个并行仿真环境。每采样一个动作`â_t'`，就立即用动力学模型（仿真）预测下一步观测`ô_t'+1`，再将此观测反馈给模型以生成下一个动作，如此循环，直到模型输出`<think>`令牌，标志该动作序列结束（见第IV-B节“Hypothesize”与“Predict”）。
    *   **验证**：对于每个完成生成的候选序列`k`，系统获取其预测的最终状态图像（智能体视角和腕部视角）`Î_(t+H_k)^(k)`。将这些图像与初始图像`I_1`、文本计划`ℓ_r`一同输入预训练的VLM验证器（如GPT-4o）。验证器根据精心设计的提示词（附录图8）进行逐步推理，判断最终状态是否满足计划要求，并输出一个二元分数（成功/失败）。**关键创新点**：此验证器是通用的、未经任务微调的，利用的是VLM固有的常识和物理推理能力，而非学习到的、可能过拟合的Q值函数。
    *   **选择与执行**：采用异步验证和早期退出策略。一旦有候选序列被VLM验证为“成功”，则立即中断其他序列的生成与验证，将该序列对应的真实动作发送给机器人执行。如果所有序列都验证失败，则回退到执行第一个候选序列或触发安全机制。

整个方法的核心在于，它不改变基础模型的参数，而是通过运行时基于语义的筛选，将基础模型在给定计划下可能产生的多样（且可能错误）的动作输出，引导至那些能真实实现该计划的结果上。

#### **5. 实验说明**
*   **评估指标**：任务成功率（%），在每种任务设置下进行50次试验取平均。
*   **数据集**：
    *   **训练数据集**：基于LIBERO基准构建的三个不同规模的推理标注数据集：LIBERO-10-R（小）、LIBERO-100-Basket-R（中，聚焦“放入篮子”技能）、LIBERO-100-R（大，完整LIBERO-100）。
    *   **评测任务集**：
        1.  **分布内任务**：LIBERO-10基准的测试集（10个任务）。
        2.  **OOD鲁棒性任务**：基于LIBERO-10构建的四个变体：语义OOD（指令改写、物体属性变化）、视觉OOD（场景物体增减、视角背景变化），各10个任务。
        3.  **行为组合任务**：将训练中学到的技能重新组合成新任务：LIBERO-10-Compose（2个任务）、LIBERO-100

---

